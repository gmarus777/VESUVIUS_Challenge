{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "166d387b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting path:/home/gregory_maruss/VESUVIUS_Challenge/jupyter notebooks\n",
      "Current path:/home/gregory_maruss/VESUVIUS_Challenge\n"
     ]
    }
   ],
   "source": [
    "# Here we take care of paths.\n",
    "# Make sure root project directory is named 'VESUVIUS_Challenge' for this to work\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "print('Starting path:' + os.getcwd())\n",
    "if os.getcwd()[-18:] == 'VESUVIUS_Challenge':\n",
    "    pass\n",
    "else:\n",
    "    PATH = Path().resolve().parents[0]\n",
    "    os.chdir(PATH)\n",
    "\n",
    "# make sure you are in the root folder of the project\n",
    "print('Current path:' + os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6e6af9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-15 19:09:41,008 - Created a temporary directory at /tmp/tmp2peb1e8k\n",
      "2023-05-15 19:09:41,009 - Writing /tmp/tmp2peb1e8k/_remote_module_non_scriptable.py\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import monai\n",
    "from monai.visualize import matshow3d\n",
    "import segmentation_models_pytorch as smp\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Tuple, List\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from Data_Modules.Vesuvius_Dataset import Vesuvius_Tile_Datamodule\n",
    "from lit_models.Vesuvius_Lit_Model import Lit_Model\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import torch.nn as nn\n",
    "from Models.PVT_model import PyramidVisionTransformerV2\n",
    "import torch.nn as nn\n",
    "from functools import partial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b0e6ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATCH_SIZE = 256\n",
    "Z_DIM = 16\n",
    "COMPETITION_DATA_DIR_str =  \"kaggle/input/vesuvius-challenge-ink-detection/\"\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# change to the line below if not using Apple's M1 or chips\n",
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74e688c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEncoder(torch.nn.Module, smp.encoders._base.EncoderMixin ):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = PyramidVisionTransformerV2(img_size=PATCH_SIZE,\n",
    "                                  patch_size=4,\n",
    "                                  in_chans=Z_DIM,\n",
    "                                  num_classes=1,\n",
    "                                  embed_dims=[64, 128, 256, 512],\n",
    "                                num_heads=[1, 2, 4, 8],\n",
    "                                  mlp_ratios=[4, 4, 4, 4],\n",
    "                                  qkv_bias=True,\n",
    "                                  qk_scale=None,\n",
    "                                  drop_rate=0.,\n",
    "                                attn_drop_rate=0.,\n",
    "                                  drop_path_rate=0.1,\n",
    "                                  norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
    "                                  depths=[3, 4, 6, 3],\n",
    "                                  sr_ratios=[8, 4, 2, 1]\n",
    "                                 )\n",
    "\n",
    "\n",
    "        # A number of channels for each encoder feature tensor, list of integers\n",
    "        self._out_channels: List[int] = [16, 0 , 64, 128, 256, 512]\n",
    "\n",
    "        # A number of stages in decoder (in other words number of downsampling operations), integer\n",
    "        # use in in forward pass to reduce number of returning features\n",
    "        self._depth: int = 5\n",
    "\n",
    "        # Default number of input channels in first Conv2d layer for encoder (usually 3)\n",
    "        self._in_channels: int = 16\n",
    "\n",
    "        # Define encoder modules below\n",
    "        ...\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> List[torch.Tensor]:\n",
    "        \"\"\"Produce list of features of different spatial resolutions, each feature is a 4D torch.tensor of\n",
    "        shape NCHW (features should be sorted in descending order according to spatial resolution, starting\n",
    "        with resolution same as input `x` tensor).\n",
    "\n",
    "        Input: `x` with shape (1, 3, 64, 64)\n",
    "        Output: [f0, f1, f2, f3, f4, f5] - features with corresponding shapes\n",
    "                [(1, 3, 64, 64), (1, 64, 32, 32), (1, 128, 16, 16), (1, 256, 8, 8),\n",
    "                (1, 512, 4, 4), (1, 1024, 2, 2)] (C - dim may differ)\n",
    "\n",
    "        also should support number of features according to specified depth, e.g. if depth = 5,\n",
    "        number of feature tensors = 6 (one with same resolution as input and 5 downsampled),\n",
    "        depth = 3 -> number of feature tensors = 4 (one with same resolution as input and 3 downsampled).\n",
    "        \"\"\"\n",
    "        B, C, H, W = x.shape\n",
    "        dummy = torch.empty([B, 0, H // 2, W // 2], dtype=x.dtype, device=x.device)\n",
    "        \n",
    "        out = self.model(x)\n",
    "\n",
    "        return [x, dummy] + out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef78e256",
   "metadata": {},
   "outputs": [],
   "source": [
    "smp.encoders.encoders['PVT'] = {\n",
    "    \"encoder\": MyEncoder, # encoder class here\n",
    "    \"pretrained_settings\": {\n",
    "        \n",
    "    },\n",
    "    \"params\": {\n",
    "        # init params for encoder if any\n",
    "    },\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a23edeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CFG:\n",
    "    \n",
    "    device = DEVICE\n",
    "    \n",
    "    THRESHOLD = 0.4\n",
    "    use_wandb = True\n",
    "    \n",
    "    ######### Dataset #########\n",
    "    \n",
    "    # stage: 'train' or 'test'\n",
    "    stage = 'train' \n",
    "    \n",
    "    # location of competition Data\n",
    "    competition_data_dir = COMPETITION_DATA_DIR_str\n",
    "    \n",
    "    # Number of slices in z-dim: 1<z_dim<65\n",
    "    z_dim = Z_DIM\n",
    "    \n",
    "    # fragments to use for training avalaible [1,2,3]\n",
    "    train_fragment_id=[2,3]\n",
    "    \n",
    "    # fragments to use for validation\n",
    "    val_fragment_id=[1]\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    batch_size = 16\n",
    "    \n",
    "    # Size of the patch and stride for feeding the model\n",
    "    patch_size = PATCH_SIZE\n",
    "    stride = patch_size // 2\n",
    "    \n",
    "    \n",
    "    num_workers = 8\n",
    "    on_gpu = True\n",
    "    \n",
    "    \n",
    "    ######## Model and Lightning Model paramters ############\n",
    "    \n",
    "    # MODEL\n",
    "    model =smp.PSPNet(encoder_name='PVT',\n",
    "                   encoder_weights=None, \n",
    "                   encoder_depth=5, \n",
    "                   psp_out_channels=512,\n",
    "                   psp_use_batchnorm=True,\n",
    "                   psp_dropout=0.2,\n",
    "                   in_channels=3, \n",
    "                   classes=1, \n",
    "                   activation=None,\n",
    "                   upsampling=32,\n",
    "                   aux_params=None)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    checkpoint = None\n",
    "    save_directory = None\n",
    "    \n",
    "    \n",
    "    accumulate_grad_batches = 128 // batch_size  # experiments showed batch_size * accumulate_grad = 192 is optimal\n",
    "    learning_rate = 0.0001\n",
    "    eta_min = 1e-8\n",
    "    t_max = 80\n",
    "    max_epochs = 120\n",
    "    weight_decay =  0.0001\n",
    "    precision =16\n",
    "    \n",
    "    # checkpointing\n",
    "    save_top_k=5\n",
    "    \n",
    "    monitor=\"FBETA\"\n",
    "    mode=\"max\"\n",
    "    \n",
    "    \n",
    "    ####### Augemtnations ###############\n",
    "    \n",
    "    # Training Aug\n",
    "    train_transforms = [\n",
    "        # A.RandomResizedCrop(\n",
    "        #     size, size, scale=(0.85, 1.0)),\n",
    "        A.Resize(patch_size, patch_size),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.75),\n",
    "        A.ShiftScaleRotate(p=0.75),\n",
    "        A.OneOf([\n",
    "                A.GaussNoise(var_limit=[10, 50]),\n",
    "                A.GaussianBlur(),\n",
    "                A.MotionBlur(),\n",
    "                ], p=0.4),\n",
    "        \n",
    "       \n",
    "        A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.5),\n",
    "        A.CoarseDropout(max_holes=1, max_width=int(patch_size * 0.3), max_height=int(patch_size * 0.3), \n",
    "                        mask_fill_value=0, p=0.5),\n",
    "        # A.Cutout(max_h_size=int(size * 0.6),\n",
    "        #          max_w_size=int(size * 0.6), num_holes=1, p=1.0),\n",
    "        A.Normalize(\n",
    "            mean= [0] * z_dim,\n",
    "            std= [1] * z_dim\n",
    "        ),\n",
    "        ToTensorV2(transpose_mask=True),\n",
    "    ]\n",
    "    \n",
    "\n",
    "    \n",
    "    # Validaiton Aug\n",
    "    val_transforms = [\n",
    "        A.Resize(patch_size, patch_size),\n",
    "        A.Normalize(\n",
    "            mean= [0] * z_dim,\n",
    "            std= [1] * z_dim\n",
    "        ),\n",
    "        ToTensorV2(transpose_mask=True),\n",
    "    ]\n",
    "    \n",
    "    # Test Aug\n",
    "    test_transforms = [\n",
    "        A.Resize(patch_size, patch_size),\n",
    "        A.Normalize(\n",
    "            mean=[0] * z_dim,\n",
    "            std=[1] * z_dim\n",
    "        ),\n",
    "\n",
    "        ToTensorV2(transpose_mask=True),\n",
    "    ]\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3001d827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc68e417562449ed84c3fb6722f9740c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a79d327ee3dc4330bacb37aef8dca17d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b375ebaaf7f3488798396dcdd488044f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = Vesuvius_Tile_Datamodule(cfg=CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "263a0a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgmarus\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/gregory_maruss/VESUVIUS_Challenge/wandb/run-20230515_191011-i2t97a76</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gmarus/VESUVIUS_Challenge/runs/i2t97a76' target=\"_blank\">quiet-sun-536</a></strong> to <a href='https://wandb.ai/gmarus/VESUVIUS_Challenge' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gmarus/VESUVIUS_Challenge' target=\"_blank\">https://wandb.ai/gmarus/VESUVIUS_Challenge</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gmarus/VESUVIUS_Challenge/runs/i2t97a76' target=\"_blank\">https://wandb.ai/gmarus/VESUVIUS_Challenge/runs/i2t97a76</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lit_model = Lit_Model(cfg=CFG,)\n",
    "\n",
    "Checkpoint = False\n",
    "if Checkpoint:\n",
    "    lit_model = lit_model.load_from_checkpoint('logs/gcp_checkpoints/MoUB4_Bce015_Tver_alpha085epoch_64.ckpt',\n",
    "                                               #learning_rate =7e-6 ,\n",
    "                                                #t_max = 70,\n",
    "                                               #eta_min = 1e-8,\n",
    "                                               #weight_decay =  0.0001,\n",
    "                                              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e001178",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-15 19:10:16,883 - Using 16bit None Automatic Mixed Precision (AMP)\n",
      "2023-05-15 19:10:16,946 - GPU available: True (cuda), used: True\n",
      "2023-05-15 19:10:16,947 - TPU available: False, using: 0 TPU cores\n",
      "2023-05-15 19:10:16,948 - IPU available: False, using: 0 IPUs\n",
      "2023-05-15 19:10:16,949 - HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gregory_maruss/anaconda3/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/gregory_maruss/VESUVIUS_Challenge/logs/PVT_256 exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-15 19:10:18,182 - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "2023-05-15 19:10:18,195 - \n",
      "  | Name                  | Type                  | Params\n",
      "----------------------------------------------------------------\n",
      "0 | metrics               | ModuleDict            | 0     \n",
      "1 | model                 | PSPNet                | 21.1 M\n",
      "2 | loss_dice             | DiceLoss              | 0     \n",
      "3 | loss_tversky          | TverskyLoss           | 0     \n",
      "4 | loss_focal            | FocalLoss             | 0     \n",
      "5 | loss_bce              | SoftBCEWithLogitsLoss | 0     \n",
      "6 | loss_monai_focal_dice | DiceFocalLoss         | 0     \n",
      "----------------------------------------------------------------\n",
      "21.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.1 M    Total params\n",
      "42.201    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4391f2ca5d7483aa0c6aae709543a5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SAVE_DIR = 'logs/PVT_256'\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    save_top_k=5,\n",
    "    monitor=\"FBETA\",\n",
    "    mode=\"max\",\n",
    "    dirpath=SAVE_DIR,\n",
    "    filename=\"PVT_256{epoch:02d}{FBETA:.2f}{val_loss:.2f}{fbeta_4:.2f}{recall:.2f}{precision:.2f}\",\n",
    "    save_last =True,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "        accelerator='gpu',\n",
    "        #benchmark=True,\n",
    "        max_epochs=CFG.max_epochs,\n",
    "        check_val_every_n_epoch= 1,\n",
    "        devices=1,\n",
    "        #fast_dev_run=fast_dev_run,\n",
    "        logger=pl.loggers.CSVLogger(save_dir=SAVE_DIR),\n",
    "        log_every_n_steps=1,\n",
    "        default_root_dir = SAVE_DIR,\n",
    "        #overfit_batches=1,\n",
    "        precision=CFG.precision,\n",
    "        accumulate_grad_batches=CFG.accumulate_grad_batches, \n",
    "        callbacks=[checkpoint_callback],\n",
    "        #resume_from_checkpoint ='logs/gcp_checkpoints/MoUB4_Bce015_Tver_alpha085epoch_64.ckpt'\n",
    "        \n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trainer.fit(lit_model, datamodule=dataset,\n",
    "            #ckpt_path='logs/gcp_checkpoints/MoUB4_Bce015_Tver_alpha085epoch_64.ckpt'\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8262c5b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
