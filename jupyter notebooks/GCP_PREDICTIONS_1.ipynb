{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c777b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting path:/home/gregory_maruss/VESUVIUS_Challenge\n",
      "Current path:/home/gregory_maruss/VESUVIUS_Challenge\n"
     ]
    }
   ],
   "source": [
    "# Here we take care of paths.\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "print('Starting path:' + os.getcwd())\n",
    "if os.getcwd()[-18:] == 'VESUVIUS_Challenge':\n",
    "    pass\n",
    "else:\n",
    "    PATH = Path().resolve().parents[0]\n",
    "    os.chdir(PATH)\n",
    "\n",
    "# make sure you are in Paragraph_to_Tex folder\n",
    "print('Current path:' + os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffe9ceff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import monai\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "#from Data_Modules.MONAI_Dict_Dataset_Module import MONAI_CSV_Scrolls_Dataset\n",
    "import matplotlib.patches as patches\n",
    "#from lit_models.UNET_monai_lit import UNET_lit\n",
    "#from monai.visualize import matshow3d\n",
    "from Models.pvtv2 import PyramidVisionTransformerV2\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e5fe992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-14 18:55:50,441 - Loading pretrained weights from url (https://github.com/whai362/PVT/releases/download/v2/pvt_v2_b1.pth)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/whai362/PVT/releases/download/v2/pvt_v2_b1.pth\" to /home/gregory_maruss/.cache/torch/hub/checkpoints/pvt_v2_b1.pth\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d7a2c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyramidVisionTransformerV2(\n",
       "  (patch_embed): OverlapPatchEmbed(\n",
       "    (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n",
       "    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (stages): ModuleList(\n",
       "    (0): PyramidVisionTransformerStage(\n",
       "      (blocks): ModuleList(\n",
       "        (0-1): 2 x Block(\n",
       "          (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (kv): Linear(in_features=64, out_features=128, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))\n",
       "            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MlpWithDepthwiseConv(\n",
       "            (fc1): Linear(in_features=64, out_features=512, bias=True)\n",
       "            (relu): Identity()\n",
       "            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=512, out_features=64, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (1): PyramidVisionTransformerStage(\n",
       "      (downsample): OverlapPatchEmbed(\n",
       "        (proj): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0-1): 2 x Block(\n",
       "          (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (kv): Linear(in_features=128, out_features=256, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MlpWithDepthwiseConv(\n",
       "            (fc1): Linear(in_features=128, out_features=1024, bias=True)\n",
       "            (relu): Identity()\n",
       "            (dwconv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (2): PyramidVisionTransformerStage(\n",
       "      (downsample): OverlapPatchEmbed(\n",
       "        (proj): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0-1): 2 x Block(\n",
       "          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MlpWithDepthwiseConv(\n",
       "            (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "            (relu): Identity()\n",
       "            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (3): PyramidVisionTransformerStage(\n",
       "      (downsample): OverlapPatchEmbed(\n",
       "        (proj): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0-1): 2 x Block(\n",
       "          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (kv): Linear(in_features=512, out_features=1024, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MlpWithDepthwiseConv(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (relu): Identity()\n",
       "            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (head): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b051d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "KAGGLE_DIR = PATH / \"kaggle\"\n",
    "\n",
    "INPUT_DIR = KAGGLE_DIR / \"input\"\n",
    "\n",
    "COMPETITION_DATA_DIR = INPUT_DIR / \"vesuvius-challenge-ink-detection\"\n",
    "\n",
    "TRAIN_DATA_CSV_PATH = COMPETITION_DATA_DIR / \"data_train_0.5.csv\"\n",
    "TEST_DATA_CSV_PATH = COMPETITION_DATA_DIR / \"data_test_1.csv\"\n",
    "TEST_DOWNSAMPLED_DATA_CSV_PATH = COMPETITION_DATA_DIR /\"data_test_0.5.csv\"\n",
    "TEST_DOWNSAMPLED_HALf_DATA_CSV_PATH = COMPETITION_DATA_DIR /\"data_test_0.75.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2063b359",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Tuple, List\n",
    "\n",
    "\n",
    "import monai\n",
    "from monai.inferers import sliding_window_inference\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import torch\n",
    "\n",
    "from torchmetrics import Dice, FBetaScore\n",
    "from torchmetrics import MetricCollection\n",
    "from tqdm.auto import tqdm\n",
    "try:\n",
    "    import wandb\n",
    "except ModuleNotFoundError:\n",
    "    pass\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "class UNET_lit(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        use_wandb = True,\n",
    "        z_dim= 16,\n",
    "        patch_size = (512,512),\n",
    "        sw_batch_size=16 ,\n",
    "        eta_min = 1e-6,\n",
    "        t_max = 200,\n",
    "        max_epochs = 700,\n",
    "        weight_decay: float = 0.00005,\n",
    "        learning_rate: float = 0.0003,\n",
    "        gamma: float = 0.85,\n",
    "        milestones: List[int] = [  100, 150, 200, 250, 300, 350, 400, 450, 500],\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        if use_wandb:\n",
    "            wandb.init()\n",
    "        self.z_dim = z_dim\n",
    "        self.metrics = self._init_metrics()\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.weight_decay = weight_decay\n",
    "        self.milestones = milestones\n",
    "\n",
    "        self.model = self._init_model()\n",
    "        self.loss = self._init_loss()\n",
    "\n",
    "\n",
    "\n",
    "    def _init_model(self):\n",
    "        return monai.networks.nets.UNet(\n",
    "                            spatial_dims=2,\n",
    "                            in_channels= self.z_dim,\n",
    "                            out_channels=1,\n",
    "                            channels=( 64, 128, 256, 512, 1024,2048),\n",
    "                            strides=(2, 2, 2, 2,2 ),\n",
    "                            num_res_units=6,\n",
    "                            dropout=0,\n",
    "                        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images = batch[\"volume_npy\"].as_tensor().to(DEVICE)\n",
    "        labels = batch[\"label_npy\"].long().to(DEVICE)\n",
    "        masks = batch[\"mask_npy\"].to(DEVICE)\n",
    "        outputs = self.model(images)\n",
    "\n",
    "        loss = self.loss(outputs, labels, masks)\n",
    "\n",
    "        self.log(\"train/loss\", loss.as_tensor(), on_step=True,on_epoch=True, prog_bar=True)\n",
    "        self.metrics[\"train_metrics\"](outputs, labels)\n",
    "        wandb.log({\"train/loss\": loss.as_tensor()})\n",
    "\n",
    "        outputs = {\"loss\": loss}\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images = batch[\"volume_npy\"].as_tensor().to(DEVICE)\n",
    "        labels = batch[\"label_npy\"].long().to(DEVICE)\n",
    "        masks = batch[\"mask_npy\"].to(DEVICE)\n",
    "        outputs = self.model(images)\n",
    "\n",
    "        loss = self.loss(outputs, labels, masks)\n",
    "        preds = torch.sigmoid(outputs.detach()).gt(.4).int()\n",
    "\n",
    "        accuracy = (preds == labels).sum().float().div(labels.size(0) * labels.size(2) ** 2)\n",
    "        fbeta_score = FBetaScore(task=\"binary\", beta=.5, threshold=.4).to(DEVICE)\n",
    "        fbeta = fbeta_score(torch.sigmoid(outputs), labels)\n",
    "        #fbeta_score_vesuvio = self.fbeta_score_vesuvio(torch.sigmoid(outputs).to(dtype=torch.long, device=DEVICE),labels, 0.4 )\n",
    "\n",
    "\n",
    "        self.log(\"val/loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"accuracy\", accuracy, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"fbeta\", fbeta, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        #self.log(\"fbeta_vesuvio\", fbeta_score_vesuvio, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.metrics[\"val_metrics\"](outputs, labels)\n",
    "\n",
    "        wandb.log({\"val/loss\": loss.as_tensor()})\n",
    "        wandb.log({\"accuracy\": accuracy.as_tensor()})\n",
    "        wandb.log({\"fbeta\": fbeta.as_tensor()})\n",
    "        #wandb.log({\"fbeta_vesuvio\": fbeta_score_vesuvio.as_tensor()})\n",
    "\n",
    "\n",
    "        outputs = {\"loss\": loss}\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        images = batch[\"volume_npy\"].as_tensor()\n",
    "        masks = batch[\"mask_npy\"].as_tensor()\n",
    "        h ,w = images.shape[2], images.shape[3]\n",
    "        h_mod = h % 1024\n",
    "        w_mod = w % 1024\n",
    "        h -= h_mod\n",
    "        w -= w_mod\n",
    "        outputs = sliding_window_inference(\n",
    "            inputs=images,\n",
    "            roi_size= (h, w),# ( 1024,2048),# (h, w), #self.hparams.patch_size,\n",
    "            sw_batch_size=self.hparams.sw_batch_size,\n",
    "            predictor=self,\n",
    "            overlap=0.05,\n",
    "            mode='gaussian',\n",
    "           \n",
    "            \n",
    "        )\n",
    "        return outputs.sigmoid().squeeze()\n",
    "\n",
    "\n",
    "\n",
    "    def _init_loss(self):\n",
    "\n",
    "        loss = monai.losses.DiceLoss(sigmoid=True)\n",
    "        return monai.losses.MaskedLoss(loss)\n",
    "\n",
    "    def _init_metrics(self):\n",
    "        metric_collection = MetricCollection(\n",
    "            {\n",
    "                \"dice\": Dice(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return torch.nn.ModuleDict(\n",
    "            {\n",
    "                \"train_metrics\": metric_collection.clone(prefix=\"train_\"),\n",
    "                \"val_metrics\": metric_collection.clone(prefix=\"val_\"),\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        #scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=self.milestones, gamma=self.gamma)\n",
    "        scheduler =  torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=self.hparams.t_max,  eta_min=self.hparams.eta_min, )\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    #torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=self.hparams.max_epochs,  eta_min=self.hparams.eta_min, )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069a6396",
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_model = UNET_lit( \n",
    "        use_wandb = False,\n",
    "        z_dim = 32,\n",
    "        patch_size = (512,512),\n",
    "        sw_batch_size=16 ,\n",
    "        eta_min = 1e-6,\n",
    "        t_max = 75,\n",
    "        max_epochs = 700,\n",
    "        weight_decay = 0.00005,\n",
    "        learning_rate = 0.0003,\n",
    "        gamma = 0.85,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa476fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "        accelerator='cpu',\n",
    "        #benchmark=True,\n",
    "        max_epochs=700,\n",
    "        check_val_every_n_epoch= 1,\n",
    "        devices=1,\n",
    "        #fast_dev_run=fast_dev_run,\n",
    "        logger=pl.loggers.CSVLogger(save_dir='logs/up1024/'),\n",
    "        log_every_n_steps=1,\n",
    "        \n",
    "        overfit_batches=0,\n",
    "        #precision=16,\n",
    "         accumulate_grad_batches=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d024ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_model = lit_model.load_from_checkpoint('logs/monai_32_2048_epoch550.ckpt', \n",
    "                                           patch_size = (512,512),\n",
    "                                           use_wandb=False,\n",
    "                                          sw_batch_size = 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823dc947",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ffdc91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604f01aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = MONAI_CSV_Scrolls_Dataset(\n",
    "                z_dim=32,\n",
    "                batch_size=1,\n",
    "                data_csv_path= TEST_DOWNSAMPLED_DATA_CSV_PATH, #TEST_DOWNSAMPLED_DATA_CSV_PATH,#TEST_DATA_CSV_PATH,\n",
    "                num_workers=16,\n",
    "                num_samples=4,\n",
    "                patch_size=(512,512),\n",
    "                val_fragment_id=3,\n",
    "                stage='predict',\n",
    "                on_gpu =False,\n",
    "                    )\n",
    "\n",
    "data_module.setup(stage='predict')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c38ecdb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lit_model.eval()\n",
    "predictions = trainer.predict( lit_model, data_module,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7ae089",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(predictions[0])\n",
    "print(predictions[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53afeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(predictions[0])\n",
    "print(predictions[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f8bb5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1572b870",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions[1].shape)\n",
    "plt.imshow(predictions[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49829674",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions[1].shape)\n",
    "plt.imshow(predictions[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ca7d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions[1].shape)\n",
    "plt.imshow(predictions[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2aca39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdb4126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(image, title):\n",
    "    fig = plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.imshow(image, cmap=\"gray\")\n",
    "    \n",
    "# ref.: https://www.kaggle.com/stainsby/fast-tested-rle\n",
    "def rle(img):\n",
    "    '''\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels = img.flatten()\n",
    "    # pixels = (pixels >= thr).astype(int)\n",
    "    \n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5993e6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import PIL.Image as Image\n",
    "import numpy as np\n",
    "import lovely_numpy as ln\n",
    "\n",
    "submission_df = pd.read_csv(\"kaggle/input/vesuvius-challenge-ink-detection/sample_submission.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762eb12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"kaggle/input/vesuvius-challenge-ink-detection/data_test_1.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11530b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_rle = []\n",
    "predictions_old = []\n",
    "THRESHOLD = .95\n",
    "for mask_png_path, prediction in zip(test_df[\"mask_png\"].values, predictions):\n",
    "    prediction = prediction.numpy()\n",
    "    mask = Image.open(mask_png_path)\n",
    "\n",
    "    prediction_resized = np.array(Image.fromarray(prediction).resize(mask.size))\n",
    "    \n",
    "    prediction_masked = prediction_resized * mask\n",
    "    \n",
    "    prediction_thresholded = np.where(prediction_masked > THRESHOLD, 1, 0).astype(np.uint8)\n",
    "    \n",
    "    prediction_rle = rle(prediction_thresholded)\n",
    "\n",
    "    predictions_rle.append(prediction_rle)\n",
    "    \n",
    "    \n",
    "    predictions_old.append(prediction_thresholded)\n",
    "\n",
    "    if 1:\n",
    "        #plot_image(prediction_resized, f\"prediction resized {ln.lovely(prediction_resized)}\")\n",
    "        #plot_image(prediction_masked, f\"prediction masked {ln.lovely(prediction_masked)}\")\n",
    "        plot_image(\n",
    "            prediction_thresholded,\n",
    "            f\"prediction thresholded {ln.lovely(prediction_thresholded)}\",\n",
    "        )\n",
    "    \n",
    "submission_df[\"Predicted\"] = predictions_rle\n",
    "    \n",
    "submission_df.to_csv(\"logs/submission.csv\", index=False)\n",
    "\n",
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386a2cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_rle = []\n",
    "predictions_old = []\n",
    "THRESHOLD = .983\n",
    "for mask_png_path, prediction in zip(test_df[\"mask_png\"].values, predictions):\n",
    "    prediction = prediction.numpy()\n",
    "    mask = Image.open(mask_png_path)\n",
    "\n",
    "    prediction_resized = np.array(Image.fromarray(prediction).resize(mask.size))\n",
    "    \n",
    "    prediction_masked = prediction_resized * mask\n",
    "    \n",
    "    prediction_thresholded = np.where(prediction_masked > THRESHOLD, 1, 0).astype(np.uint8)\n",
    "    \n",
    "    prediction_rle = rle(prediction_thresholded)\n",
    "\n",
    "    predictions_rle.append(prediction_rle)\n",
    "    \n",
    "    \n",
    "    predictions_old.append(prediction_thresholded)\n",
    "\n",
    "    if 1:\n",
    "        #plot_image(prediction_resized, f\"prediction resized {ln.lovely(prediction_resized)}\")\n",
    "        #plot_image(prediction_masked, f\"prediction masked {ln.lovely(prediction_masked)}\")\n",
    "        plot_image(\n",
    "            prediction_thresholded,\n",
    "            f\"prediction thresholded {ln.lovely(prediction_thresholded)}\",\n",
    "        )\n",
    "    \n",
    "submission_df[\"Predicted\"] = predictions_rle\n",
    "    \n",
    "submission_df.to_csv(\"logs/submission.csv\", index=False)\n",
    "\n",
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ca7063",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29d52d62",
   "metadata": {},
   "source": [
    "## Other type submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4d6ec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79526731",
   "metadata": {},
   "source": [
    "from skimage.transform import resize as resize_ski\n",
    "import PIL.Image as Image\n",
    "\n",
    "DATA_DIR = \"/kaggle/input/vesuvius-challenge-ink-detection\"\n",
    "mask_a = Image.open(COMPETITION_DATA_DIR / \"test/a/mask.png\")\n",
    "mask_b = Image.open(COMPETITION_DATA_DIR /\"test/b/mask.png\")\n",
    "original_size_a = mask_a.size\n",
    "original_size_b = mask_b.size\n",
    "predictions_map_a = resize_ski(predictions[0], (original_size_a[1], original_size_a[0])).squeeze()\n",
    "predictions_map_b = resize_ski(predictions[1], (original_size_b[1], original_size_b[0])).squeeze()\n",
    "predictions_map_a = predictions_map_a *mask_a\n",
    "predictions_map_b =  predictions_map_b *mask_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220a74ff",
   "metadata": {},
   "source": [
    "print(predictions_map_b.shape)\n",
    "plt.imshow(predictions_map_b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1a5352",
   "metadata": {},
   "source": [
    "def rle_new(predictions_map, threshold):\n",
    "    flat_img = predictions_map.flatten()\n",
    "    flat_img = np.where(flat_img > threshold, 1, 0).astype(np.uint8)\n",
    "\n",
    "    starts = np.array((flat_img[:-1] == 0) & (flat_img[1:] == 1))\n",
    "    ends = np.array((flat_img[:-1] == 1) & (flat_img[1:] == 0))\n",
    "    starts_ix = np.where(starts)[0] + 2\n",
    "    ends_ix = np.where(ends)[0] + 2\n",
    "    lengths = ends_ix - starts_ix\n",
    "    return \" \".join(map(str, sum(zip(starts_ix, lengths), ())))\n",
    "\n",
    "\n",
    "\n",
    "threshold = 0.9\n",
    "\n",
    "rle_a = rle_new(predictions_map_a, threshold=threshold)\n",
    "rle_b = rle_new(predictions_map_b, threshold=threshold)\n",
    "print(\"Id,Predicted\\na,\" + rle_a + \"\\nb,\" + rle_b, file=open('submission.csv', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c83ea82",
   "metadata": {},
   "source": [
    "COMARE SUBMISSIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b803fe37",
   "metadata": {},
   "source": [
    "submission_old = pd.read_csv('logs/submission.csv')\n",
    "submission_new = pd.read_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e784c64f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2c2293",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce63bc55",
   "metadata": {},
   "source": [
    "plot_image(\n",
    "            prediction_thresholded,\n",
    "            f\"prediction thresholded {ln.lovely(prediction_thresholded)}\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d72d9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c97c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module.df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be27043",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
