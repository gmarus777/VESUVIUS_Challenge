{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91098bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting path:/Users/gregory/PROJECT_ML/VESUVIUS_Challenge/jupyter notebooks\n",
      "Current path:/Users/gregory/PROJECT_ML/VESUVIUS_Challenge\n"
     ]
    }
   ],
   "source": [
    "# Here we take care of paths.\n",
    "# Make sure root project directory is named 'VESUVIUS_Challenge' for this to work\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "print('Starting path:' + os.getcwd())\n",
    "if os.getcwd()[-18:] == 'VESUVIUS_Challenge':\n",
    "    pass\n",
    "else:\n",
    "    PATH = Path().resolve().parents[0]\n",
    "    os.chdir(PATH)\n",
    "\n",
    "# make sure you are in the root folder of the project\n",
    "print('Current path:' + os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e2e38fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-25 10:33:42,163 - Created a temporary directory at /var/folders/wc/60y8v25x3ns_jgsx6clbdb180000gn/T/tmpzjlfj_5z\n",
      "2023-05-25 10:33:42,163 - Writing /var/folders/wc/60y8v25x3ns_jgsx6clbdb180000gn/T/tmpzjlfj_5z/_remote_module_non_scriptable.py\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import monai\n",
    "#from monai.visualize import matshow3d\n",
    "import segmentation_models_pytorch as smp\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Tuple, List\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from Data_Modules.Vesuvius_Dataset import Vesuvius_Tile_Datamodule\n",
    "from lit_models.Vesuvius_Lit_Model import Lit_Model\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import torch.nn as nn\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from lit_models.scratch_models import FPNDecoder\n",
    "from Models.PreBackbone_3D import PreBackbone_3D\n",
    "from Models.PreBackbone_3d_Zdim import PreBackbone_3D_ZDIM\n",
    "from Models.Segformer import SegFormer\n",
    "from einops import rearrange\n",
    "from Models.Model_3d import Encoder_3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d33d78a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATCH_SIZE = 256\n",
    "Z_DIM = 12\n",
    "COMPETITION_DATA_DIR_str =  \"kaggle/input/vesuvius-challenge-ink-detection/\"\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "\n",
    "# change to the line below if not using Apple's M1 or chips\n",
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56b3658c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder =PreBackbone_3D_ZDIM (out_channels = 6, z_dim= 12,att_dim=64,)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d31f3c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 12, 1024]) torch.Size([5, 12, 64])\n",
      "torch.Size([5, 6, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "dummy = torch.randn(5,12,256,256)\n",
    "out = encoder(dummy)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db2d75ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pool' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpool\u001b[49m(out)\u001b[38;5;241m.\u001b[39mshape2\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pool' is not defined"
     ]
    }
   ],
   "source": [
    "pool(out).shape2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b17824",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.randn(5,128,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e66c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "outs= torch.nn.functional.avg_pool3d(out, kernel_size=(out.shape[2], 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fa2c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "outs.shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa932439",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pre_model3d = PreBackbone_3D_ZDIM(z_dim=16, out_channels = 8)\n",
    "segformer = SegFormer(in_channels=3,\n",
    "    widths=[64, 128, 256, 512],\n",
    "    depths=[3, 4, 6, 3],\n",
    "    all_num_heads=[1, 2, 4, 8],\n",
    "    patch_sizes=[7, 3, 3, 3],\n",
    "    overlap_sizes=[4, 2, 2, 2],\n",
    "    reduction_ratios=[8, 4, 2, 1],\n",
    "    mlp_expansions=[4, 4, 4, 4],\n",
    "    decoder_channels=256,\n",
    "    scale_factors=[8, 4, 2, 1],\n",
    "    num_classes=1,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0b7cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = torch.randn(16, 16,256,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c07717",
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = Pre_model3d(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9215089",
   "metadata": {},
   "outputs": [],
   "source": [
    "outs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62cad31",
   "metadata": {},
   "outputs": [],
   "source": [
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8913e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pre_model3d = PreBackbone_3D_ZDIM(z_dim=24)\n",
    "dummy = torch.randn(16, 24,256,256)\n",
    "outs = Pre_model3d(dummy)\n",
    "print(outs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb3ad52",
   "metadata": {},
   "outputs": [],
   "source": [
    "segformer = SegFormer(\n",
    "    in_channels=3,\n",
    "    widths=[64, 128, 256, 512],\n",
    "    depths=[3, 4, 6, 3],\n",
    "    all_num_heads=[1, 2, 4, 8],\n",
    "    patch_sizes=[7, 3, 3, 3],\n",
    "    overlap_sizes=[4, 2, 2, 2],\n",
    "    reduction_ratios=[8, 4, 2, 1],\n",
    "    mlp_expansions=[4, 4, 4, 4],\n",
    "    decoder_channels=256,\n",
    "    scale_factors=[8, 4, 2, 1],\n",
    "    num_classes=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49117a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_final  = segformer(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63c3b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772c1863",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = nn.functional.interpolate(\n",
    "            out_final, \n",
    "            size=(256,256), \n",
    "            mode=\"bilinear\", \n",
    "            align_corners=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9881a956",
   "metadata": {},
   "outputs": [],
   "source": [
    "final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a52bdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "u"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2937539",
   "metadata": {},
   "source": [
    "# 3d Convolutions\n",
    "\n",
    "#### to decrease z_dim/channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f08d5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# TODO: implement attnetion between slices. currently only local info\n",
    "\n",
    "\n",
    "# TODO: try using Depth-wise convolution https://medium.com/@zurister/depth-wise-convolution-and-depth-wise-separable-convolution-37346565d4ec\n",
    "# TODO:\n",
    "class PreBackbone_3D(nn.Module):\n",
    "    def __init__(self, filter_sizes = [6,12,24,48], batch_norm=False):\n",
    "\n",
    "\n",
    "        super(PreBackbone_3D, self).__init__()\n",
    "\n",
    "        self.leaky_relu = nn.LeakyReLU(inplace=True)\n",
    "        self.batch_norm = batch_norm\n",
    "        self.pool = nn.AvgPool3d(kernel_size=(2, 1, 1), stride=(2, 1, 1))\n",
    "        self.global_pool = nn.AdaptiveAvgPool3d((1, None, None))\n",
    "        self.batch_norm = torch.nn.BatchNorm3d(num_features=1, momentum=0.9)\n",
    "\n",
    "        # layer 1\n",
    "        self.conv1 = nn.Conv3d(in_channels=1,\n",
    "                               out_channels=filter_sizes[0],\n",
    "                               kernel_size=(3, 1, 1),\n",
    "                               stride=(2, 1, 1),\n",
    "                               padding=(1, 0, 0)\n",
    "                               )\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(self.conv1.weight)\n",
    "        torch.nn.init.zeros_(self.conv1.bias)\n",
    "\n",
    "        # layer 2\n",
    "        self.conv2 = nn.Conv3d(in_channels=filter_sizes[0]//2,\n",
    "                               out_channels=filter_sizes[1],\n",
    "                               kernel_size=(3, 1, 1),\n",
    "                               stride=(2, 1, 1),\n",
    "                               padding=(1, 0, 0)\n",
    "                               )\n",
    "        torch.nn.init.xavier_uniform_(self.conv2.weight)\n",
    "        torch.nn.init.zeros_(self.conv2.bias)\n",
    "\n",
    "        # layer 3\n",
    "        self.conv3 = nn.Conv3d(in_channels=filter_sizes[1]//2,\n",
    "                               out_channels=filter_sizes[2],\n",
    "                               kernel_size=(3, 1, 1),\n",
    "                               stride=(2, 1, 1),\n",
    "                               padding=(1, 0, 0)\n",
    "                               )\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(self.conv3.weight)\n",
    "        torch.nn.init.zeros_(self.conv3.bias)\n",
    "\n",
    "        # layer 4\n",
    "        self.conv4 = nn.Conv3d(in_channels=filter_sizes[2]//2,\n",
    "                               out_channels=filter_sizes[3],\n",
    "                               kernel_size=(3, 1, 1),\n",
    "                               stride=(2, 1, 1),\n",
    "                               padding=(1, 0, 0)\n",
    "                               )\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(self.conv3.weight)\n",
    "        torch.nn.init.zeros_(self.conv3.bias)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # (B,C,H,W) -> (B, 1,  C, H, W)\n",
    "\n",
    "        # Layer 1\n",
    "        y = self.conv1(x)   # (B, 1,  C, H, W) -> (B, 4,  C/2, H, W)\n",
    "\n",
    "        y = y.permute(0, 2, 1, 3, 4)\n",
    "        y = self.pool(y)                # (B, 4,  C/2, H, W) ->  (B, 2,  C/2, H, W)\n",
    "        y = y.permute(0, 2, 1, 3, 4)\n",
    "        y = self.leaky_relu(y)\n",
    "        # if self.batch_norm:\n",
    "        #   y = self.batch_norm(y)\n",
    "\n",
    "        # Layer 2\n",
    "        y = self.conv2(y)\n",
    "        y = y.permute(0, 2, 1, 3, 4)\n",
    "        y = self.pool(y)\n",
    "        y = y.permute(0, 2, 1, 3, 4)\n",
    "        y = self.leaky_relu(y)\n",
    "\n",
    "        # Layer 3\n",
    "        y = self.conv3(y)\n",
    "        y = y.permute(0, 2, 1, 3, 4)\n",
    "        y = self.pool(y)\n",
    "        y = y.permute(0, 2, 1, 3, 4)\n",
    "        y = self.leaky_relu(y)\n",
    "\n",
    "        # Layer 4\n",
    "        y = self.conv4(y)  #(B, 48,  3, H, W)  where filter_sizes[-1]=48\n",
    "\n",
    "       # Final convolution\n",
    "        y = y.permute(0, 2, 1, 3, 4)\n",
    "        y = self.global_pool(y)\n",
    "        y = y.permute(0, 2, 1, 3, 4)\n",
    "\n",
    "        y = self.leaky_relu(y)\n",
    "\n",
    "\n",
    "        y = self.batch_norm(y)\n",
    "        return y.squeeze(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b645c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = torch.randn(5,64,256,256)\n",
    "pre_model = PreBackbone_3D()\n",
    "pre_out = pre_model(dummy)\n",
    "print(pre_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962a820c",
   "metadata": {},
   "source": [
    "# IDEA:\n",
    "\n",
    "- take 48 slices and embed them and apply attention\n",
    "- use residual connection at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec27d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm2d(nn.LayerNorm):\n",
    "    def forward(self, x):\n",
    "        x = rearrange(x, \"b c d h w -> b d h w c\")\n",
    "        x = super().forward(x)\n",
    "        x = rearrange(x, \"b d h w c -> b c d h w\")\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class Embed(nn.Module):\n",
    "    def __init__(self, in_channels=1, emdedding_dims = emdedding_dims,):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv_3d =  nn.Conv3d(in_channels=1,\n",
    "                               out_channels=emdedding_dims[0],\n",
    "                               kernel_size=(3, 1, 1),\n",
    "                               stride=(2, 1, 1),\n",
    "                               padding=(1, 0, 0)\n",
    "                               )\n",
    "    \n",
    "\n",
    "        self.conv_3d_embed =  nn.Conv3d(in_channels=emdedding_dims[0],\n",
    "                               out_channels=1,\n",
    "                               kernel_size=(1, 4, 4),\n",
    "                               stride=(1, 4, 4),\n",
    "                               padding=(0, 1, 1)\n",
    "                               )\n",
    "    \n",
    "        self.norm =  LayerNorm2d(emdedding_dims[0])\n",
    "        self.norm_embed =LayerNorm2d(1)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_3d(x)\n",
    "        x = self.norm(x)\n",
    "        x_embed = self.conv_3d_embed(x)\n",
    "        x_embed = self.norm_embed(x_embed)\n",
    "        return x, x_embed \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class EfficientMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, channels= Z_DIM//2, reduction_ratio: int = 1, num_heads: int = 8):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.reducer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels= channels,\n",
    "                               out_channels= channels,\n",
    "                               kernel_size=( 4, 4),\n",
    "                               stride=( 4, 4),\n",
    "                               padding=( 0, 0)),\n",
    "            \n",
    "                            LayerNorm_att(channels), )\n",
    "        \n",
    "        \n",
    "        self.att = nn.MultiheadAttention(PATCH_SIZE, num_heads=8, batch_first=True)\n",
    "        \n",
    "      \n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x,):\n",
    "        x = x.squeeze(1) #  (b z h w)\n",
    "        \n",
    "       \n",
    "        reduced_x = self.reducer(x)\n",
    "        \n",
    "        _, c, h, w = reduced_x.shape \n",
    "        \n",
    "        # attention needs tensor of shape (batch, sequence_length, channels)\n",
    "        reduced_x = rearrange(reduced_x, \"b c h w -> b  c ( h w )\")\n",
    "        x = rearrange(x, \"b c  h w -> b  c ( h w)\")\n",
    "        out = self.att(reduced_x, reduced_x, reduced_x)[0]\n",
    "        # reshape it back to (batch, channels, height, width)\n",
    "        out = rearrange(out, \"b  c (h w) -> b c h w\", h=h, w=w,)\n",
    "        out = nn.functional.interpolate( out, size=(256,256),  mode=\"bilinear\", align_corners=False )\n",
    "        out = out.unsqueeze(1)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "    \n",
    "class LayerNorm_att(nn.LayerNorm):\n",
    "    def forward(self, x):\n",
    "        x = rearrange(x, \"b c h w -> b  h w c\")\n",
    "        x = super().forward(x)\n",
    "        x = rearrange(x, \"b  h w c -> b c  h w\")\n",
    "        return x   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb72fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# TODO: implement attnetion between slices. currently only local info\n",
    "\n",
    "\n",
    "# TODO: try using Depth-wise convolution https://medium.com/@zurister/depth-wise-convolution-and-depth-wise-separable-convolution-37346565d4ec\n",
    "# TODO:\n",
    "\n",
    "\n",
    "class PreBackbone_3D(nn.Module):\n",
    "    def __init__(self, emdedding_dims = [4], filter_sizes=[16,32,48], batch_norm=False):\n",
    "\n",
    "\n",
    "        super(PreBackbone_3D, self).__init__()\n",
    "        \n",
    "        self.embed_layer = Embed(emdedding_dims = emdedding_dims)\n",
    "        self.attention = EfficientMultiHeadAttention()\n",
    "        \n",
    "        \n",
    "        self.pool = nn.AvgPool3d(kernel_size=(2, 1, 1), stride=(2, 1, 1))\n",
    "        self.global_pool = nn.AdaptiveAvgPool3d((1, None, None))\n",
    "        self.batch_norm = torch.nn.BatchNorm3d(num_features=1, momentum=0.1)\n",
    "        self.leaky_relu = nn.LeakyReLU(inplace=True)\n",
    "        \n",
    "        # layer 1\n",
    "        self.conv1 = nn.Conv3d(in_channels=5,\n",
    "                               out_channels=filter_sizes[0],\n",
    "                               kernel_size=(3, 1, 1),\n",
    "                               stride=(2, 1, 1),\n",
    "                               padding=(1, 0, 0)\n",
    "                               )\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(self.conv1.weight)\n",
    "        torch.nn.init.zeros_(self.conv1.bias)\n",
    "\n",
    "        # layer 2\n",
    "        self.conv2 = nn.Conv3d(in_channels=filter_sizes[0]//2,\n",
    "                               out_channels=filter_sizes[1],\n",
    "                               kernel_size=(3, 1, 1),\n",
    "                               stride=(2, 1, 1),\n",
    "                               padding=(1, 0, 0)\n",
    "                               )\n",
    "        torch.nn.init.xavier_uniform_(self.conv2.weight)\n",
    "        torch.nn.init.zeros_(self.conv2.bias)\n",
    "\n",
    "        # layer 3\n",
    "        self.conv3 = nn.Conv3d(in_channels=filter_sizes[1]//2,\n",
    "                               out_channels=filter_sizes[2],\n",
    "                               kernel_size=(3, 1, 1),\n",
    "                               stride=(2, 1, 1),\n",
    "                               padding=(1, 0, 0)\n",
    "                               )\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(self.conv3.weight)\n",
    "        torch.nn.init.zeros_(self.conv3.bias)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Stage 1 Embedding and Z_Dim attention\n",
    "        \n",
    "        # embed layer produces tensors:\n",
    "        # x_orig = (B, emdedding_dims[0], C/2, H, W) for residual connection\n",
    "        # x_att = (B, 1, C/2, H/4, W/4)\n",
    "        x_orig, x_att = self.embed_layer(x)\n",
    "        \n",
    "        # attention layer for z_dim\n",
    "        x_after_att = self.attention(x_att)\n",
    "        \n",
    "        x = torch.cat((x_orig,x_after_att ), dim=1)\n",
    "        \n",
    "        # Stage 2 Convolutions -- 3 layers\n",
    "        \n",
    "        # Layer 1\n",
    "        y = self.conv1(x)   # (B, 1,  C, H, W) -> (B, 4,  C/2, H, W)\n",
    "\n",
    "        y = y.permute(0, 2, 1, 3, 4)\n",
    "        y = self.pool(y)                # (B, 4,  C/2, H, W) ->  (B, 2,  C/2, H, W)\n",
    "    \n",
    "        y = y.permute(0, 2, 1, 3, 4)\n",
    "        y = self.leaky_relu(y)\n",
    "        # if self.batch_norm:\n",
    "        #   y = self.batch_norm(y)\n",
    "\n",
    "        # Layer 2\n",
    "        y = self.conv2(y)\n",
    "        y = y.permute(0, 2, 1, 3, 4)\n",
    "        y = self.pool(y)\n",
    "        y = y.permute(0, 2, 1, 3, 4)\n",
    "        y = self.leaky_relu(y)\n",
    "\n",
    "        # Layer 3\n",
    "        y = self.conv3(y)\n",
    "        \n",
    "        # Final convolution\n",
    "        y = y.permute(0, 2, 1, 3, 4)\n",
    "        y = self.global_pool(y)\n",
    "        y = y.permute(0, 2, 1, 3, 4)\n",
    "\n",
    "        y = self.leaky_relu(y)\n",
    "\n",
    "\n",
    "        y = self.batch_norm(y)\n",
    "        return y.squeeze(1)\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc865e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = torch.randn(5,1,48,256,256)\n",
    "pre_model = PreBackbone_3D()\n",
    "out = pre_model(dummy)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8749c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d59f3f5",
   "metadata": {},
   "source": [
    "# WORK TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b80584c",
   "metadata": {},
   "source": [
    "#### STEP 1\n",
    "outputs 2 tensors (for res connection and smaller for attention)\n",
    "- `Embedding (B,1, C, H, W) -> (B, emdedding_dims[0], C/2, H, W)`\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86d0bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed the input by convoluting in z-dim: z_dim/2\n",
    "\n",
    "dummy = torch.randn(5,1,48,256,256)\n",
    "\n",
    "embed_layer = Embed()\n",
    "dumm_or, dummy_embed = embed_layer(dummy)\n",
    "print(dumm_or.shape, dummy_embed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93000f43",
   "metadata": {},
   "source": [
    "### STEP 2\n",
    " takes `dummy_embed` and applies attention\n",
    "- Attention\n",
    "- channles must be last\n",
    "- reduce further and then add back the original tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92549c1a",
   "metadata": {},
   "source": [
    "print('initial shape:', dummy_embed.shape)\n",
    "att = EfficientMultiHeadAttention()\n",
    "\n",
    "\n",
    "\n",
    "dummy_reduced = att.reducer(dummy_embed.squeeze())\n",
    "print('reduced shape:',dummy_reduced.shape)\n",
    "\n",
    "#dummy_att = att.att(dummy_embed,dummy_reduced, dummy_reduced )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97637b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FULL MODULE\n",
    "att = EfficientMultiHeadAttention()\n",
    "dummy_after_att = att(dummy_embed)\n",
    "print(dummy_after_att.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bb36fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cfdd82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384820b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7cb72f5d",
   "metadata": {},
   "source": [
    "# Attention\n",
    "\n",
    "- channles must be last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c7aef0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f46d17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_out = att(conv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdc263a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dfdc32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58948fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463e120b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PVT_w_FPN(nn.Module):\n",
    "    def __init__(self, in_channels,  embed_dims=[  64, 128, 256, 512], n_classes=1, ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed_dims = embed_dims\n",
    "        \n",
    "       \n",
    "        self.pvt = PyramidVisionTransformerV2(img_size = PATCH_SIZE,\n",
    "                                  patch_size = 4,\n",
    "                                  in_chans = Z_DIM,\n",
    "                                  num_classes = 1,\n",
    "                                  embed_dims = embed_dims,\n",
    "                                num_heads=[1, 2, 4, 8],\n",
    "                                  mlp_ratios=[4, 4, 4, 4],\n",
    "                                  qkv_bias=True,\n",
    "                                  qk_scale=None,\n",
    "                                  drop_rate=0.,\n",
    "                                attn_drop_rate=0.,\n",
    "                                  drop_path_rate=0.1,\n",
    "                                  norm_layer=partial(nn.LayerNorm, eps=1e-3),\n",
    "                                #norm_layer=nn.LayerNorm,          \n",
    "                                  depths=[2, 2, 2,2],\n",
    "                                  sr_ratios=[1, 1, 1, 1]\n",
    "                                 ).to(DEVICE) \n",
    "        \n",
    "        self.FPN = FPNDecoder(\n",
    "                            in_channels = Z_DIM,\n",
    "                            encoder_channels = embed_dims ,\n",
    "                            encoder_depth=5,\n",
    "                            pyramid_channels=256,\n",
    "                            segmentation_channels=128,\n",
    "                            dropout=0.2,\n",
    "                            merge_policy=\"cat\",).to(DEVICE) \n",
    "        \n",
    "       \n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = x.unsqueeze(1)\n",
    "        #x = self.pre_model3d(x)\n",
    "        #x = x.squeeze(1)\n",
    "        \n",
    "        pvt_outs = self.pvt(x)\n",
    "        \n",
    "        logits = self.FPN(*pvt_outs)\n",
    "        \n",
    "       \n",
    "       \n",
    "            \n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c08954",
   "metadata": {},
   "source": [
    "dummy = torch.randn(5,8,256,256).to(DEVICE) \n",
    "model = PVT_w_FPN(in_channels =8 ,  embed_dims=[ 64, 128, 256, 512])\n",
    "out = model(dummy)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a0b58e",
   "metadata": {},
   "source": [
    "# BACKBONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3f1d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PVT_w_FPN(in_channels =8 ,  embed_dims=[ 64, 128, 256, 512])\n",
    "dummy = torch.randn(5,8,256,256).to(DEVICE) \n",
    "pvt_outs = model.pvt(dummy)\n",
    "print('pvt outputs')\n",
    "for t in pvt_outs:\n",
    "    print(t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0233b62",
   "metadata": {},
   "source": [
    "# NECK FPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027ce48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpn_outs = model.FPN(*pvt_outs)\n",
    "print(fpn_outs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb26d077",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lit_models.Vesuvius_Lit_Model import dice_coef_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f6597a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_y = torch.ones(5,1,256,256)\n",
    "loss = dice_coef_torch(fpn_outs, dummy_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b01ad2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454d3bec",
   "metadata": {},
   "source": [
    "# MLP HEAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddad23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Linear Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=2048, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(input_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class SegFormerHead(nn.Module):\n",
    "    \"\"\"\n",
    "    SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim, in_channels, embedding_dim, dropout= 0, feature_strides=None, **kwargs):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.feature_strides = feature_strides\n",
    "        self.num_classes = 1\n",
    "        self.dropout = dropout\n",
    "\n",
    "\n",
    "        #decoder_params = kwargs['decoder_params']\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.linear_c4 = MLP(input_dim=self.in_channels[-1], embed_dim=self.embedding_dim)\n",
    "        self.linear_c3 = MLP(input_dim=self.in_channels[-2], embed_dim=self.embedding_dim)\n",
    "        self.linear_c2 = MLP(input_dim=self.in_channels[-3], embed_dim=self.embedding_dim)\n",
    "        self.linear_c1 = MLP(input_dim=self.in_channels[-4], embed_dim=self.embedding_dim)\n",
    "        self.linear_c0 = MLP(input_dim=z_dim, embed_dim=self.embedding_dim)\n",
    "\n",
    "        self.conv_fuse = nn.Sequential(\n",
    "                    nn.ConvTranspose2d(\n",
    "                        embedding_dim*5, embedding_dim, kernel_size=1, stride=1),\n",
    "                    torch.nn.SyncBatchNorm(embedding_dim, eps=1e-04, momentum=0.1),\n",
    "                    #nn.GroupNorm(32, segmentation_channels, eps=1e-03),\n",
    "                    nn.GELU(),\n",
    "                    nn.ConvTranspose2d(\n",
    "                        embedding_dim, embedding_dim, kernel_size=1, stride=1),\n",
    "                ).to(DEVICE)\n",
    "\n",
    "        self.linear_pred = nn.Conv2d(embedding_dim, self.num_classes, kernel_size=1)\n",
    "        self.dropout = nn.Dropout2d(p=self.dropout, inplace=True)\n",
    "\n",
    "    def forward(self, *features):\n",
    "        #x = self._transform_inputs(inputs)  # len=4, 1/4,1/8,1/16,1/32\n",
    "        c0, c1, c2, c3, c4,  = features\n",
    "        \n",
    "        print(c0.shape,c1.shape, c2.shape, c3.shape, c4.shape)\n",
    "\n",
    "        ############## MLP decoder on C1-C4 ###########\n",
    "        n, _, h, w = c4.shape\n",
    "\n",
    "        _c4 = self.linear_c4(c4).permute(0,2,1).reshape(n, -1, c4.shape[2], c4.shape[3])\n",
    "        #_c4 = resize(_c4, size=c1.size()[2:],mode='bilinear',align_corners=False)\n",
    "        _c4 =  F.interpolate(_c4, size=c0.size()[2:], scale_factor=None, mode='bilinear',align_corners=False)\n",
    "\n",
    "        _c3 = self.linear_c3(c3).permute(0,2,1).reshape(n, -1, c3.shape[2], c3.shape[3])\n",
    "        #_c3 = resize(_c3, size=c1.size()[2:],mode='bilinear',align_corners=False)\n",
    "        _c3  =  F.interpolate(_c3, size=c0.size()[2:], scale_factor=None, mode='bilinear',align_corners=False)\n",
    "\n",
    "        _c2 = self.linear_c2(c2).permute(0,2,1).reshape(n, -1, c2.shape[2], c2.shape[3])\n",
    "        #_c2 = resize(_c2, size=c1.size()[2:],mode='bilinear',align_corners=False)\n",
    "        _c2 = F.interpolate(_c2, size=c0.size()[2:], scale_factor=None, mode='bilinear',align_corners=False)\n",
    "\n",
    "        _c1 = self.linear_c1(c1).permute(0,2,1).reshape(n, -1, c1.shape[2], c1.shape[3])\n",
    "        _c1 = F.interpolate(_c1 , size=c0.size()[2:], scale_factor=None, mode='bilinear',align_corners=False)\n",
    "        \n",
    "        _c0 =  self.linear_c0(c0).permute(0,2,1).reshape(n, -1, c0.shape[2], c0.shape[3])\n",
    "        \n",
    "        print('one', _c0.shape, _c1.shape, _c2.shape, _c3.shape, _c4.shape)\n",
    "        \n",
    "        cc =  torch.cat([_c4, _c3, _c2, _c1, _c0], dim=1)\n",
    "        print(cc.shape)\n",
    "\n",
    "        _c = self.conv_fuse(cc)\n",
    "\n",
    "        x = self.dropout(_c)\n",
    "        x = self.linear_pred(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ddb829",
   "metadata": {},
   "outputs": [],
   "source": [
    "S_Head = SegFormerHead( z_dim = 8, in_channels =[  64, 128, 256, 512] , embedding_dim=128 ).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe1e4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_out = S_Head(*pvt_outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a61b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mlp_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e752391",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0c1fe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0ae0d7d",
   "metadata": {},
   "source": [
    "# SEGFORMER TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca8585d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_model = SegFormer( in_channels=8,\n",
    "                        widths=[64, 128, 256, 512],\n",
    "                        depths=[3, 4, 6, 3],\n",
    "                        all_num_heads=[1, 2, 4, 8],\n",
    "                        patch_sizes=[7, 3, 3, 3],\n",
    "                        overlap_sizes=[4, 2, 2, 2],\n",
    "                        reduction_ratios=[8, 4, 2, 1],\n",
    "                        mlp_expansions=[4, 4, 4, 4],\n",
    "                        decoder_channels=256,\n",
    "                        scale_factors=[8, 4, 2, 1],\n",
    "                        num_classes=1, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df38dc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = torch.randn(5,8,256,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dedd57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_out = seg_model(dummy)\n",
    "final_out = nn.functional.interpolate(\n",
    "            seg_out, \n",
    "            size=(256,256), \n",
    "            mode=\"bilinear\", \n",
    "            align_corners=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd77c445",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077f997f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreBackbone_3D_old(nn.Module):\n",
    "    def __init__(self, batch_norm = True ):\n",
    "        \n",
    "        \n",
    "        \n",
    "        super(PreBackbone_3D, self).__init__()\n",
    "        \n",
    "        self.leaky_relu = nn.LeakyReLU( inplace=True)\n",
    "        self.batch_norm = batch_norm\n",
    "        \n",
    "        self.conv = nn.Conv3d(in_channels=1,\n",
    "                            out_channels=1,\n",
    "                             kernel_size = (3, 1, 1),\n",
    "                             stride=(1, 1, 1),\n",
    "                             padding= (1, 0, 0)\n",
    "                             )\n",
    "        \n",
    "        torch.nn.init.xavier_uniform_(self.conv.weight)\n",
    "        torch.nn.init.zeros_(self.conv.bias)\n",
    "         \n",
    "        self.pool = nn.AvgPool3d(kernel_size = (2,1,1), stride=(2,1,1))\n",
    "        self.batch_norm = torch.nn.BatchNorm3d( num_features=1, momentum=0.9)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1) # (B,C,H,W) -> (B, 1 C, H, W)\n",
    "    \n",
    "        \n",
    "        y = self.conv(x)\n",
    "        y = self.pool(y)\n",
    "        y = self.leaky_relu(y)\n",
    "        if self.batch_norm:\n",
    "            y = self.batch_norm(y)  # (B, 1 C, H, W) -> (B, 1 C/2, H, W)\n",
    "            \n",
    "       \n",
    "        y = self.conv(y)\n",
    "        y = self.pool(y)\n",
    "        y = self.leaky_relu(y)\n",
    "        if self.batch_norm:\n",
    "            y = self.batch_norm(y)  # (B, 1 C, H, W) -> (B, 1 C/2, H, W)\n",
    "            \n",
    "            \n",
    "        y = self.conv(y)\n",
    "        y = self.pool(y)\n",
    "        y = self.leaky_relu(y)\n",
    "        if self.batch_norm:\n",
    "            y = self.batch_norm(y)  # (B, 1 C, H, W) -> (B, 1 C/2, H, W)\n",
    "            \n",
    "        y = self.conv(y)\n",
    "        y = self.pool(y)\n",
    "        y = self.leaky_relu(y)\n",
    "        if self.batch_norm:\n",
    "            y = self.batch_norm(y)  # (B, 1 C, H, W) -> (B, 1 C/2, H, W)\n",
    "            \n",
    "        return y\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d01344d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
