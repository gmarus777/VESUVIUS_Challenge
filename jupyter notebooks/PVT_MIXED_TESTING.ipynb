{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91098bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting path:/Users/gregory/PROJECT_ML/VESUVIUS_Challenge/jupyter notebooks\n",
      "Current path:/Users/gregory/PROJECT_ML/VESUVIUS_Challenge\n"
     ]
    }
   ],
   "source": [
    "# Here we take care of paths.\n",
    "# Make sure root project directory is named 'VESUVIUS_Challenge' for this to work\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "print('Starting path:' + os.getcwd())\n",
    "if os.getcwd()[-18:] == 'VESUVIUS_Challenge':\n",
    "    pass\n",
    "else:\n",
    "    PATH = Path().resolve().parents[0]\n",
    "    os.chdir(PATH)\n",
    "\n",
    "# make sure you are in the root folder of the project\n",
    "print('Current path:' + os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "3e2e38fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import monai\n",
    "#from monai.visualize import matshow3d\n",
    "import segmentation_models_pytorch as smp\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Tuple, List\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from Data_Modules.Vesuvius_Dataset import Vesuvius_Tile_Datamodule\n",
    "from lit_models.Vesuvius_Lit_Model import Lit_Model\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import torch.nn as nn\n",
    "from Models.PVT2 import PyramidVisionTransformerV2, Up, OutConv\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from Models.Swin import SwinTransformer, SwinTransformerBlockV2, PatchMergingV2\n",
    "from lit_models.scratch_models import FPNDecoder\n",
    "from Models.PreBackbone_3D import PreBackbone_3D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d33d78a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATCH_SIZE = 256\n",
    "Z_DIM = 8\n",
    "COMPETITION_DATA_DIR_str =  \"kaggle/input/vesuvius-challenge-ink-detection/\"\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "\n",
    "# change to the line below if not using Apple's M1 or chips\n",
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b505f309",
   "metadata": {},
   "source": [
    "# 3d Convolutions\n",
    "\n",
    "#### to decrease z_dim/channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00d29aa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "226ac3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreBackbone_3D(nn.Module):\n",
    "    def __init__(self, batch_norm = True ):\n",
    "        \n",
    "        \n",
    "        \n",
    "        super(PreBackbone_3D, self).__init__()\n",
    "        \n",
    "        self.leaky_relu = nn.LeakyReLU( inplace=True)\n",
    "        self.batch_norm = batch_norm\n",
    "        \n",
    "        self.conv = nn.Conv3d(in_channels=1,\n",
    "                            out_channels=1,\n",
    "                             kernel_size = (3, 1, 1),\n",
    "                             stride=(1, 1, 1),\n",
    "                             padding= (1, 0, 0)\n",
    "                             )\n",
    "        \n",
    "        torch.nn.init.xavier_uniform_(self.conv.weight)\n",
    "        torch.nn.init.zeros_(self.conv.bias)\n",
    "         \n",
    "        self.pool = nn.AvgPool3d(kernel_size = (2,1,1), stride=(2,1,1))\n",
    "        self.batch_norm = torch.nn.BatchNorm3d( num_features=1, momentum=0.9)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1) # (B,C,H,W) -> (B, 1 C, H, W)\n",
    "    \n",
    "        \n",
    "        y = self.conv(x)\n",
    "        y = self.pool(y)\n",
    "        y = self.leaky_relu(y)\n",
    "        if self.batch_norm:\n",
    "            y = self.batch_norm(y)  # (B, 1 C, H, W) -> (B, 1 C/2, H, W)\n",
    "            \n",
    "       \n",
    "        y = self.conv(y)\n",
    "        y = self.pool(y)\n",
    "        y = self.leaky_relu(y)\n",
    "        if self.batch_norm:\n",
    "            y = self.batch_norm(y)  # (B, 1 C, H, W) -> (B, 1 C/2, H, W)\n",
    "            \n",
    "            \n",
    "        y = self.conv(y)\n",
    "        y = self.pool(y)\n",
    "        y = self.leaky_relu(y)\n",
    "        if self.batch_norm:\n",
    "            y = self.batch_norm(y)  # (B, 1 C, H, W) -> (B, 1 C/2, H, W)\n",
    "            \n",
    "        y = self.conv(y)\n",
    "        y = self.pool(y)\n",
    "        y = self.leaky_relu(y)\n",
    "        if self.batch_norm:\n",
    "            y = self.batch_norm(y)  # (B, 1 C, H, W) -> (B, 1 C/2, H, W)\n",
    "            \n",
    "        return y\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5588fbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 4, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "dummy = torch.randn(5,64,256,256)\n",
    "pre_model = PreBackbone_3D()\n",
    "pre_out = pre_model(dummy)\n",
    "print(pre_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "463e120b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PVT_w_FPN(nn.Module):\n",
    "    def __init__(self, in_channels,  embed_dims=[  64, 128, 256, 512], n_classes=1, ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed_dims = embed_dims\n",
    "        \n",
    "       \n",
    "        self.pvt = PyramidVisionTransformerV2(img_size = PATCH_SIZE,\n",
    "                                  patch_size = 4,\n",
    "                                  in_chans = Z_DIM,\n",
    "                                  num_classes = 1,\n",
    "                                  embed_dims = embed_dims,\n",
    "                                num_heads=[1, 2, 4, 8],\n",
    "                                  mlp_ratios=[4, 4, 4, 4],\n",
    "                                  qkv_bias=True,\n",
    "                                  qk_scale=None,\n",
    "                                  drop_rate=0.,\n",
    "                                attn_drop_rate=0.,\n",
    "                                  drop_path_rate=0.1,\n",
    "                                  norm_layer=partial(nn.LayerNorm, eps=1e-3),\n",
    "                                #norm_layer=nn.LayerNorm,          \n",
    "                                  depths=[2, 2, 2,2],\n",
    "                                  sr_ratios=[1, 1, 1, 1]\n",
    "                                 ).to(DEVICE) \n",
    "        \n",
    "        self.FPN = FPNDecoder(\n",
    "                            in_channels = Z_DIM,\n",
    "                            encoder_channels = embed_dims ,\n",
    "                            encoder_depth=5,\n",
    "                            pyramid_channels=256,\n",
    "                            segmentation_channels=128,\n",
    "                            dropout=0.2,\n",
    "                            merge_policy=\"cat\",).to(DEVICE) \n",
    "        \n",
    "       \n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = x.unsqueeze(1)\n",
    "        #x = self.pre_model3d(x)\n",
    "        #x = x.squeeze(1)\n",
    "        \n",
    "        pvt_outs = self.pvt(x)\n",
    "        \n",
    "        logits = self.FPN(*pvt_outs)\n",
    "        \n",
    "       \n",
    "       \n",
    "            \n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c08954",
   "metadata": {},
   "source": [
    "dummy = torch.randn(5,8,256,256).to(DEVICE) \n",
    "model = PVT_w_FPN(in_channels =8 ,  embed_dims=[ 64, 128, 256, 512])\n",
    "out = model(dummy)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a0b58e",
   "metadata": {},
   "source": [
    "# BACKBONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf3f1d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pvt outputs\n",
      "torch.Size([5, 8, 256, 256])\n",
      "torch.Size([5, 64, 64, 64])\n",
      "torch.Size([5, 128, 32, 32])\n",
      "torch.Size([5, 256, 16, 16])\n",
      "torch.Size([5, 512, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "model = PVT_w_FPN(in_channels =8 ,  embed_dims=[ 64, 128, 256, 512])\n",
    "dummy = torch.randn(5,8,256,256).to(DEVICE) \n",
    "pvt_outs = model.pvt(dummy)\n",
    "print('pvt outputs')\n",
    "for t in pvt_outs:\n",
    "    print(t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0233b62",
   "metadata": {},
   "source": [
    "# NECK FPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "027ce48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "fpn_outs = model.FPN(*pvt_outs)\n",
    "print(fpn_outs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb26d077",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lit_models.Vesuvius_Lit_Model import dice_coef_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47f6597a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_y = torch.ones(5,1,256,256)\n",
    "loss = dice_coef_torch(fpn_outs, dummy_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b01ad2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1686, device='mps:0', grad_fn=<RsubBackward1>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454d3bec",
   "metadata": {},
   "source": [
    "# MLP HEAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ddad23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Linear Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=2048, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(input_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class SegFormerHead(nn.Module):\n",
    "    \"\"\"\n",
    "    SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim, in_channels, embedding_dim, dropout= 0, feature_strides=None, **kwargs):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.feature_strides = feature_strides\n",
    "        self.num_classes = 1\n",
    "        self.dropout = dropout\n",
    "\n",
    "\n",
    "        #decoder_params = kwargs['decoder_params']\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.linear_c4 = MLP(input_dim=self.in_channels[-1], embed_dim=self.embedding_dim)\n",
    "        self.linear_c3 = MLP(input_dim=self.in_channels[-2], embed_dim=self.embedding_dim)\n",
    "        self.linear_c2 = MLP(input_dim=self.in_channels[-3], embed_dim=self.embedding_dim)\n",
    "        self.linear_c1 = MLP(input_dim=self.in_channels[-4], embed_dim=self.embedding_dim)\n",
    "        self.linear_c0 = MLP(input_dim=z_dim, embed_dim=self.embedding_dim)\n",
    "\n",
    "        self.conv_fuse = nn.Sequential(\n",
    "                    nn.ConvTranspose2d(\n",
    "                        embedding_dim*5, embedding_dim, kernel_size=1, stride=1),\n",
    "                    torch.nn.SyncBatchNorm(embedding_dim, eps=1e-04, momentum=0.1),\n",
    "                    #nn.GroupNorm(32, segmentation_channels, eps=1e-03),\n",
    "                    nn.GELU(),\n",
    "                    nn.ConvTranspose2d(\n",
    "                        embedding_dim, embedding_dim, kernel_size=1, stride=1),\n",
    "                ).to(DEVICE)\n",
    "\n",
    "        self.linear_pred = nn.Conv2d(embedding_dim, self.num_classes, kernel_size=1)\n",
    "        self.dropout = nn.Dropout2d(p=self.dropout, inplace=True)\n",
    "\n",
    "    def forward(self, *features):\n",
    "        #x = self._transform_inputs(inputs)  # len=4, 1/4,1/8,1/16,1/32\n",
    "        c0, c1, c2, c3, c4,  = features\n",
    "        \n",
    "        print(c0.shape,c1.shape, c2.shape, c3.shape, c4.shape)\n",
    "\n",
    "        ############## MLP decoder on C1-C4 ###########\n",
    "        n, _, h, w = c4.shape\n",
    "\n",
    "        _c4 = self.linear_c4(c4).permute(0,2,1).reshape(n, -1, c4.shape[2], c4.shape[3])\n",
    "        #_c4 = resize(_c4, size=c1.size()[2:],mode='bilinear',align_corners=False)\n",
    "        _c4 =  F.interpolate(_c4, size=c0.size()[2:], scale_factor=None, mode='bilinear',align_corners=False)\n",
    "\n",
    "        _c3 = self.linear_c3(c3).permute(0,2,1).reshape(n, -1, c3.shape[2], c3.shape[3])\n",
    "        #_c3 = resize(_c3, size=c1.size()[2:],mode='bilinear',align_corners=False)\n",
    "        _c3  =  F.interpolate(_c3, size=c0.size()[2:], scale_factor=None, mode='bilinear',align_corners=False)\n",
    "\n",
    "        _c2 = self.linear_c2(c2).permute(0,2,1).reshape(n, -1, c2.shape[2], c2.shape[3])\n",
    "        #_c2 = resize(_c2, size=c1.size()[2:],mode='bilinear',align_corners=False)\n",
    "        _c2 = F.interpolate(_c2, size=c0.size()[2:], scale_factor=None, mode='bilinear',align_corners=False)\n",
    "\n",
    "        _c1 = self.linear_c1(c1).permute(0,2,1).reshape(n, -1, c1.shape[2], c1.shape[3])\n",
    "        _c1 = F.interpolate(_c1 , size=c0.size()[2:], scale_factor=None, mode='bilinear',align_corners=False)\n",
    "        \n",
    "        _c0 =  self.linear_c0(c0).permute(0,2,1).reshape(n, -1, c0.shape[2], c0.shape[3])\n",
    "        \n",
    "        print('one', _c0.shape, _c1.shape, _c2.shape, _c3.shape, _c4.shape)\n",
    "        \n",
    "        cc =  torch.cat([_c4, _c3, _c2, _c1, _c0], dim=1)\n",
    "        print(cc.shape)\n",
    "\n",
    "        _c = self.conv_fuse(cc)\n",
    "\n",
    "        x = self.dropout(_c)\n",
    "        x = self.linear_pred(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34ddb829",
   "metadata": {},
   "outputs": [],
   "source": [
    "S_Head = SegFormerHead( z_dim = 8, in_channels =[  64, 128, 256, 512] , embedding_dim=128 ).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fe1e4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 8, 256, 256]) torch.Size([5, 64, 64, 64]) torch.Size([5, 128, 32, 32]) torch.Size([5, 256, 16, 16]) torch.Size([5, 512, 8, 8])\n",
      "one torch.Size([5, 128, 256, 256]) torch.Size([5, 128, 256, 256]) torch.Size([5, 128, 256, 256]) torch.Size([5, 128, 256, 256]) torch.Size([5, 128, 256, 256])\n",
      "torch.Size([5, 640, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "mlp_out = S_Head(*pvt_outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9a61b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "print(mlp_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e752391",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
