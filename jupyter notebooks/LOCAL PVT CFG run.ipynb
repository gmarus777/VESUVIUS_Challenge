{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01e4d0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting path:/Users/gregory/PROJECT_ML/VESUVIUS_Challenge/jupyter notebooks\n",
      "Current path:/Users/gregory/PROJECT_ML/VESUVIUS_Challenge\n"
     ]
    }
   ],
   "source": [
    "# Here we take care of paths.\n",
    "# Make sure root project directory is named 'VESUVIUS_Challenge' for this to work\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "print('Starting path:' + os.getcwd())\n",
    "if os.getcwd()[-18:] == 'VESUVIUS_Challenge':\n",
    "    pass\n",
    "else:\n",
    "    PATH = Path().resolve().parents[0]\n",
    "    os.chdir(PATH)\n",
    "\n",
    "# make sure you are in the root folder of the project\n",
    "print('Current path:' + os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6634f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-18 15:11:01,981 - Created a temporary directory at /var/folders/wc/60y8v25x3ns_jgsx6clbdb180000gn/T/tmp0n3x0wla\n",
      "2023-05-18 15:11:01,982 - Writing /var/folders/wc/60y8v25x3ns_jgsx6clbdb180000gn/T/tmp0n3x0wla/_remote_module_non_scriptable.py\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import monai\n",
    "from monai.visualize import matshow3d\n",
    "import segmentation_models_pytorch as smp\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Tuple, List\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from Data_Modules.Vesuvius_Dataset import Vesuvius_Tile_Datamodule\n",
    "from lit_models.Vesuvius_Lit_Model import Lit_Model\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import torch.nn as nn\n",
    "from Models.PVT2 import PyramidVisionTransformerV2, Up, OutConv\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from Models.Swin import SwinTransformer, SwinTransformerBlockV2, PatchMergingV2\n",
    "from lit_models.scratch_models import FPNDecoder, Feature2Pyramid\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a53d5f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATCH_SIZE = 256\n",
    "Z_DIM = 8\n",
    "COMPETITION_DATA_DIR_str =  \"kaggle/input/vesuvius-challenge-ink-detection/\"\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "\n",
    "# change to the line below if not using Apple's M1 or chips\n",
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdebfe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PVT_w_FPN(nn.Module):\n",
    "    def __init__(self, in_channels,  embed_dims=[  64, 128, 256, 512], n_classes=1, ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed_dims = embed_dims\n",
    "        \n",
    "       \n",
    "        self.pvt = PyramidVisionTransformerV2(img_size = PATCH_SIZE,\n",
    "                                  patch_size = 4,\n",
    "                                  in_chans = Z_DIM,\n",
    "                                  num_classes = 1,\n",
    "                                  embed_dims = embed_dims,\n",
    "                                num_heads=[1, 2, 4, 8],\n",
    "                                  mlp_ratios=[4, 4, 4, 4],\n",
    "                                  qkv_bias=True,\n",
    "                                  qk_scale=None,\n",
    "                                  drop_rate=0.,\n",
    "                                attn_drop_rate=0.,\n",
    "                                  drop_path_rate=0.1,\n",
    "                                  norm_layer=partial(nn.LayerNorm, eps=1e-3),\n",
    "                                #norm_layer=nn.LayerNorm,          \n",
    "                                  depths=[2, 2, 2,2],\n",
    "                                  sr_ratios=[1, 1, 1, 1]\n",
    "                                 ).to(DEVICE) \n",
    "        \n",
    "        self.FPN = FPNDecoder(\n",
    "                            in_channels = Z_DIM,\n",
    "                            encoder_channels = embed_dims ,\n",
    "                            encoder_depth=5,\n",
    "                            pyramid_channels=256,\n",
    "                            segmentation_channels=128,\n",
    "                            dropout=0.2,\n",
    "                            merge_policy=\"cat\",).to(DEVICE) \n",
    "        \n",
    "       \n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = x.unsqueeze(1)\n",
    "        #x = self.pre_model3d(x)\n",
    "        #x = x.squeeze(1)\n",
    "        \n",
    "        pvt_outs = self.pvt(x)\n",
    "        \n",
    "        logits = self.FPN(*pvt_outs)\n",
    "        \n",
    "       \n",
    "       \n",
    "            \n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a302ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CFG:\n",
    "    \n",
    "    device = DEVICE\n",
    "    \n",
    "    THRESHOLD = 0.4\n",
    "    use_wandb = False\n",
    "    \n",
    "    ######### Dataset #########\n",
    "    \n",
    "    # stage: 'train' or 'test'\n",
    "    stage = 'train' \n",
    "    \n",
    "    # location of competition Data\n",
    "    competition_data_dir = COMPETITION_DATA_DIR_str\n",
    "    \n",
    "    # Number of slices in z-dim: 1<z_dim<65\n",
    "    z_dim = Z_DIM\n",
    "    \n",
    "    # fragments to use for training avalaible [1,2,3]\n",
    "    train_fragment_id=[2,3]\n",
    "    \n",
    "    # fragments to use for validation\n",
    "    val_fragment_id=[1]\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    batch_size = 8\n",
    "    \n",
    "    # Size of the patch and stride for feeding the model\n",
    "    patch_size = PATCH_SIZE\n",
    "    stride = patch_size // 2\n",
    "    \n",
    "    \n",
    "    num_workers = 0\n",
    "    on_gpu = True\n",
    "    \n",
    "    \n",
    "    ######## Model and Lightning Model paramters ############\n",
    "    \n",
    "    # MODEL\n",
    "    model = PVT_w_FPN(in_channels = z_dim,  embed_dims=[ 64, 128, 256, 512])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    checkpoint = None\n",
    "    save_directory = None\n",
    "    \n",
    "    \n",
    "    accumulate_grad_batches = 128 // batch_size  # experiments showed batch_size * accumulate_grad = 192 is optimal\n",
    "    learning_rate = 0.00007\n",
    "    eta_min = 1e-8\n",
    "    t_max = 80\n",
    "    max_epochs = 120\n",
    "    weight_decay =  0.00001\n",
    "    precision =16\n",
    "    \n",
    "    # checkpointing\n",
    "    save_top_k=5\n",
    "    \n",
    "    monitor=\"FBETA\"\n",
    "    mode=\"max\"\n",
    "    \n",
    "    \n",
    "    ####### Augemtnations ###############\n",
    "    \n",
    "    # Training Aug\n",
    "    train_transforms = [\n",
    "        # A.RandomResizedCrop(\n",
    "        #     size, size, scale=(0.85, 1.0)),\n",
    "        A.Resize(patch_size, patch_size),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.75),\n",
    "        A.ShiftScaleRotate(p=0.75),\n",
    "        A.OneOf([\n",
    "                A.GaussNoise(var_limit=[10, 50]),\n",
    "                A.GaussianBlur(),\n",
    "                A.MotionBlur(),\n",
    "                ], p=0.4),\n",
    "        \n",
    "       \n",
    "        A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.5),\n",
    "        A.CoarseDropout(max_holes=1, max_width=int(patch_size * 0.3), max_height=int(patch_size * 0.3), \n",
    "                        mask_fill_value=0, p=0.5),\n",
    "        # A.Cutout(max_h_size=int(size * 0.6),\n",
    "        #          max_w_size=int(size * 0.6), num_holes=1, p=1.0),\n",
    "        A.Normalize(\n",
    "            mean= [0] * z_dim,\n",
    "            std= [1] * z_dim\n",
    "        ),\n",
    "        ToTensorV2(transpose_mask=True),\n",
    "    ]\n",
    "    \n",
    "\n",
    "    \n",
    "    # Validaiton Aug\n",
    "    val_transforms = [\n",
    "        A.Resize(patch_size, patch_size),\n",
    "        A.Normalize(\n",
    "            mean= [0] * z_dim,\n",
    "            std= [1] * z_dim\n",
    "        ),\n",
    "        ToTensorV2(transpose_mask=True),\n",
    "    ]\n",
    "    \n",
    "    # Test Aug\n",
    "    test_transforms = [\n",
    "        A.Resize(patch_size, patch_size),\n",
    "        A.Normalize(\n",
    "            mean=[0] * z_dim,\n",
    "            std=[1] * z_dim\n",
    "        ),\n",
    "\n",
    "        ToTensorV2(transpose_mask=True),\n",
    "    ]\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f5e0fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b334bebaeb74cc3b771c1d25b7d0af2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = Vesuvius_Tile_Datamodule(cfg=CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf372cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_model = Lit_Model(cfg=CFG,)\n",
    "\n",
    "Checkpoint = False\n",
    "if Checkpoint:\n",
    "    lit_model = lit_model.load_from_checkpoint('logs/gcp_checkpoints/MoUB4_Bce015_Tver_alpha085epoch_64.ckpt',\n",
    "                                               #learning_rate =7e-6 ,\n",
    "                                                #t_max = 70,\n",
    "                                               #eta_min = 1e-8,\n",
    "                                               #weight_decay =  0.0001,\n",
    "                                              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f743ac5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "SAVE_DIR = 'logs/PVT_Unet'\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    save_top_k=5,\n",
    "    monitor=\"FBETA\",\n",
    "    mode=\"max\",\n",
    "    dirpath=SAVE_DIR,\n",
    "    filename=\"PVT_Unet{epoch:02d}{FBETA:.2f}{val_loss:.2f}{fbeta_4:.2f}{recall:.2f}{precision:.2f}\",\n",
    "    save_last =True,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "        accelerator='mps',\n",
    "        #benchmark=True,\n",
    "        max_epochs=CFG.max_epochs,\n",
    "        check_val_every_n_epoch= 1,\n",
    "        devices=1,\n",
    "        #fast_dev_run=False,\n",
    "        logger=pl.loggers.CSVLogger(save_dir=SAVE_DIR),\n",
    "        log_every_n_steps=1,\n",
    "        default_root_dir = SAVE_DIR,\n",
    "        #overfit_batches=1,\n",
    "        #precision=CFG.precision,\n",
    "        accumulate_grad_batches=CFG.accumulate_grad_batches, \n",
    "        callbacks=[checkpoint_callback],\n",
    "        gradient_clip_val=1,\n",
    "        #resume_from_checkpoint ='logs/gcp_checkpoints/MoUB4_Bce015_Tver_alpha085epoch_64.ckpt'\n",
    "        detect_anomaly=True,\n",
    "        \n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trainer.fit(lit_model, datamodule=dataset,\n",
    "            #ckpt_path='logs/gcp_checkpoints/MoUB4_Bce015_Tver_alpha085epoch_64.ckpt'\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf76c23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6a2b54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87c3d0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8ea7101",
   "metadata": {},
   "source": [
    "class PVT_w_UNet(nn.Module):\n",
    "    def __init__(self, in_channels,  embed_dims=[ 64, 128, 256, 512], n_classes=1, ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed_dims = embed_dims\n",
    "        \n",
    "        self.pvt = PyramidVisionTransformerV2(img_size = PATCH_SIZE,\n",
    "                                  patch_size = 4,\n",
    "                                  in_chans = Z_DIM,\n",
    "                                  num_classes = 1,\n",
    "                                  embed_dims = embed_dims,\n",
    "                                num_heads=[1, 2, 4, 8],\n",
    "                                  mlp_ratios=[8, 8, 4, 4],\n",
    "                                  qkv_bias=True,\n",
    "                                  qk_scale=None,\n",
    "                                  drop_rate=0.,\n",
    "                                attn_drop_rate=0.,\n",
    "                                  drop_path_rate=0.1,\n",
    "                                  norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
    "                                  depths=[2, 2, 2, 2],\n",
    "                                  sr_ratios=[8, 4, 2, 1]\n",
    "                                 ) \n",
    "        \n",
    "        self.up1 = Up(self.embed_dims[-1], self.embed_dims[-2])\n",
    "        self.up2 = Up(self.embed_dims[-2], self.embed_dims[-3])\n",
    "        self.up3 = Up(self.embed_dims[-3], self.embed_dims[-4])\n",
    "        self.up4 = Up(self.embed_dims[-4], in_channels, last_layer = True)\n",
    "        \n",
    "        self.out_conv = OutConv(in_channels,n_classes)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x1, x2, x3, x4, x5 = self.pvt(x)\n",
    "        \n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        \n",
    "        logits = self.out_conv(x)\n",
    "        \n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
