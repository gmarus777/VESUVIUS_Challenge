{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dfdc84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting path:/Users/gregory/PROJECT_ML/VESUVIUS_Challenge/jupyter notebooks\n",
      "Current path:/Users/gregory/PROJECT_ML/VESUVIUS_Challenge\n"
     ]
    }
   ],
   "source": [
    "# Here we take care of paths.\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "print('Starting path:' + os.getcwd())\n",
    "if os.getcwd()[-18:] == 'VESUVIUS_Challenge':\n",
    "    pass\n",
    "else:\n",
    "    PATH = Path().resolve().parents[0]\n",
    "    os.chdir(PATH)\n",
    "\n",
    "# make sure you are in Paragraph_to_Tex folder\n",
    "print('Current path:' + os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5636399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-02 12:50:32,395 - Created a temporary directory at /var/folders/wc/60y8v25x3ns_jgsx6clbdb180000gn/T/tmpw27xohz1\n",
      "2023-05-02 12:50:32,396 - Writing /var/folders/wc/60y8v25x3ns_jgsx6clbdb180000gn/T/tmpw27xohz1/_remote_module_non_scriptable.py\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import monai\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from Data_Modules.Vesuvius_Tile_Dataset import Vesuvius_Tile_Datamodule\n",
    "import matplotlib.patches as patches\n",
    "from lit_models.UNET_TILE import UNET_TILE_lit\n",
    "from monai.visualize import matshow3d\n",
    "import einops\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bd419f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    \n",
    "    train_fragment_id=[2,3]\n",
    "    val_fragment_id=[1]\n",
    "    batch_size = 32\n",
    "    patch_size = 224\n",
    "    z_dim = 16\n",
    "    stride = patch_size // 2\n",
    "    #comp_dataset_path = COMPETITION_DATA_DIR\n",
    "    num_workers = 0\n",
    "    on_gpu = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "554acfe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee36645f275449a1ac00b60162ac8e36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dccbbf6b77d45d1abbbc6d195946296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e31040a8e5694d13987994adafd893c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = Vesuvius_Tile_Datamodule(cfg=CFG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad61167",
   "metadata": {},
   "source": [
    "dataloader = iter(dataset.train_dataloader())\n",
    "for i in range(1):\n",
    "    # Get image and label from train data -- change number for different ones\n",
    "    subvolumes, inklabels = next(dataloader)\n",
    "    print('subvolume shape:',subvolumes.shape)\n",
    "    print('inklabel shape:',inklabels.shape)\n",
    "    for subvolume,  inklabel in zip(subvolumes, inklabels):\n",
    "                fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "                for idx, image in enumerate((subvolume,  inklabel)):\n",
    "                    if idx==0:\n",
    "                        axes[idx].imshow(image[0])\n",
    "                    else:\n",
    "                         axes[idx].imshow(image.squeeze(0))\n",
    "\n",
    "                plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706db3ce",
   "metadata": {},
   "source": [
    "plot_dataset = dataset.train_dataloader()\n",
    "plot_count = 0\n",
    "for i in range(1000):\n",
    "    image, mask = plot_dataset[i]\n",
    "    #data = transform(image=image, mask=mask)\n",
    "    aug_image = image#.squeeze(0) # data['image']\n",
    "    aug_mask = mask#.squeeze(0) #data['mask']\n",
    "    print(image.shape, mask.shape)\n",
    "\n",
    "    if mask.sum() == 0:\n",
    "        continue\n",
    "\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(15, 8))\n",
    "    axes[0].imshow(image[..., 0], cmap=\"gray\")\n",
    "    axes[1].imshow(mask, cmap=\"gray\")\n",
    "    #axes[2].imshow(aug_image[..., 0], cmap=\"gray\")\n",
    "    #axes[3].imshow(aug_mask, cmap=\"gray\")\n",
    "    \n",
    "    #plt.savefig(CFG.figures_dir + f'aug_fold_{CFG.valid_id}_{plot_count}.png')\n",
    "\n",
    "    plot_count += 1\n",
    "    if plot_count == 5:\n",
    "        break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68e8bba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_model = UNET_TILE_lit(\n",
    "        use_wandb = False,\n",
    "        z_dim = 16,\n",
    "        patch_size = (224,224),\n",
    "        sw_batch_size=8 ,\n",
    "        eta_min = 1e-8,\n",
    "        t_max = 250,\n",
    "        max_epochs = 1000,\n",
    "        weight_decay =  0.001,\n",
    "        learning_rate = 0.0001,\n",
    "        gamma = 0.85,)\n",
    "\n",
    "#lit_model = lit_model.load_from_checkpoint('logs/FocalDICE_512_monai_unet_16_cont/lightning_logs/version_0/checkpoints/epoch=171-step=344.ckpt', \n",
    "#                                          #patch_size = (512,512),\n",
    "#                                          learning_rate = 0.00001,\n",
    "#                                           weight_decay =  .01,\n",
    "#                                           eta_min = 1e-9,\n",
    "#                                         sw_batch_size = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe35e9c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-02 12:50:55,119 - GPU available: True (mps), used: False\n",
      "2023-05-02 12:50:55,120 - TPU available: False, using: 0 TPU cores\n",
      "2023-05-02 12:50:55,120 - IPU available: False, using: 0 IPUs\n",
      "2023-05-02 12:50:55,120 - HPU available: False, using: 0 HPUs\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "2023-05-02 12:50:55,242 - \n",
      "   | Name                 | Type                  | Params\n",
      "----------------------------------------------------------------\n",
      "0  | metrics              | ModuleDict            | 0     \n",
      "1  | model                | UNet                  | 6.5 M \n",
      "2  | loss                 | MaskedLoss            | 0     \n",
      "3  | mine_focal           | FocalLoss             | 0     \n",
      "4  | weighted_bce_loss    | BCEWithLogitsLoss     | 0     \n",
      "5  | loss_dice            | DiceLoss              | 0     \n",
      "6  | loss_tversky         | TverskyLoss           | 0     \n",
      "7  | loss_bce             | SoftBCEWithLogitsLoss | 0     \n",
      "8  | loss_focal           | FocalLoss             | 0     \n",
      "9  | diceloss             | DiceLoss              | 0     \n",
      "10 | monai_tverskyLoss    | TverskyLoss           | 0     \n",
      "11 | focalloss            | FocalLoss             | 0     \n",
      "12 | monai_masked_tversky | MaskedLoss            | 0     \n",
      "13 | masked_dice          | MaskedLoss            | 0     \n",
      "14 | masked_focal         | MaskedLoss            | 0     \n",
      "----------------------------------------------------------------\n",
      "6.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.5 M     Total params\n",
      "26.033    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gregory/anaconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "/Users/gregory/anaconda3/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /Users/gregory/PROJECT_ML/VESUVIUS_Challenge/logs/Full_16_768to1024_batch exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "/Users/gregory/anaconda3/lib/python3.10/site-packages/lightning_fabric/loggers/csv_logs.py:188: UserWarning: Experiment logs directory logs/Full_16_768to1024_batch/lightning_logs/version_0 exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gregory/anaconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/gregory/anaconda3/lib/python3.10/site-packages/monai/losses/spatial_mask.py:51: UserWarning: No mask value specified for the MaskedLoss.\n",
      "  warnings.warn(\"No mask value specified for the MaskedLoss.\")\n",
      "/Users/gregory/anaconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c2053b1c6bd4fb0ae923e86babaea77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    save_top_k=5,\n",
    "    monitor=\"FBETA\",\n",
    "    mode=\"max\",\n",
    "    dirpath=\"logs/Full_16_768to1024_batch/\",\n",
    "    filename=\"FocalDice_768{epoch:02d}{FBETA:.2f}{recall:.2f}{precision:.2f}\",\n",
    "    save_last =True,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "        accelerator='cpu',\n",
    "        #benchmark=True,\n",
    "        max_epochs=100,\n",
    "        check_val_every_n_epoch= 1,\n",
    "        devices=1,\n",
    "        #fast_dev_run=fast_dev_run,\n",
    "        logger=pl.loggers.CSVLogger(save_dir='logs/Full_16_768to1024_batch/'),\n",
    "        log_every_n_steps=1,\n",
    "        default_root_dir = 'logs/Full_16_768to1024_batch/',\n",
    "        #overfit_batches=1,\n",
    "        #precision=16,\n",
    "        accumulate_grad_batches=2, \n",
    "        callbacks=[checkpoint_callback],\n",
    "        #resume_from_checkpoint ='logs/smp_unet_32_.5/lightning_logs/version_3/checkpoints/FocalDice_768epoch=123FBETA=0.30recall=0.77precision=0.26.ckpt'\n",
    "        \n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trainer.fit(lit_model, datamodule=dataset,\n",
    "            #ckpt_path='logs/Full_16_768to1024_batch/FocalDice_768epoch=151FBETA=0.41recall=0.56precision=0.39.ckpt'\n",
    "           )\n",
    "# resume_from_checkpoint = \n",
    "#ckpt_path='logs/unet_smp-epoch=102-val_loss=0.00.ckpt'\n",
    "# ckpt_path='logs/Eff_monai_32z/lightning_logs/version_0/checkpoints/epoch=19-step=40.ckp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28963218",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59386b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a48b4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
