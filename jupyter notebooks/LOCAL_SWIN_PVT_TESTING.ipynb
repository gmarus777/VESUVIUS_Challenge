{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aed10554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting path:/Users/gregory/PROJECT_ML/VESUVIUS_Challenge/jupyter notebooks\n",
      "Current path:/Users/gregory/PROJECT_ML/VESUVIUS_Challenge\n"
     ]
    }
   ],
   "source": [
    "# Here we take care of paths.\n",
    "# Make sure root project directory is named 'VESUVIUS_Challenge' for this to work\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "print('Starting path:' + os.getcwd())\n",
    "if os.getcwd()[-18:] == 'VESUVIUS_Challenge':\n",
    "    pass\n",
    "else:\n",
    "    PATH = Path().resolve().parents[0]\n",
    "    os.chdir(PATH)\n",
    "\n",
    "# make sure you are in the root folder of the project\n",
    "print('Current path:' + os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "196e3f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-16 16:56:05,087 - Created a temporary directory at /var/folders/wc/60y8v25x3ns_jgsx6clbdb180000gn/T/tmpv0l_x52i\n",
      "2023-05-16 16:56:05,088 - Writing /var/folders/wc/60y8v25x3ns_jgsx6clbdb180000gn/T/tmpv0l_x52i/_remote_module_non_scriptable.py\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import monai\n",
    "from monai.visualize import matshow3d\n",
    "import segmentation_models_pytorch as smp\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Tuple, List\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from Data_Modules.Vesuvius_Dataset import Vesuvius_Tile_Datamodule\n",
    "from lit_models.Vesuvius_Lit_Model import Lit_Model\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "from Models.PVT_model import PyramidVisionTransformerV2\n",
    "from Models.Swin import SwinTransformer, SwinTransformerBlockV2, PatchMergingV2\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "import timm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4aa84f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need a tensor torch.Size([8, 16, 224, 224])\n",
    "\n",
    "x = torch.randn(8,16,256,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa311bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pvt = PyramidVisionTransformerV2(img_size=256,\n",
    "                                  #patch_size=4,\n",
    "                                  in_chans=16,\n",
    "                                  num_classes=1,\n",
    "                                  embed_dims=[64, 128, 256, 512],\n",
    "                                num_heads=[16, 2, 4, 8],\n",
    "                                  mlp_ratios=[4, 4, 4, 4],\n",
    "                                  qkv_bias=True,\n",
    "                                  qk_scale=None,\n",
    "                                  drop_rate=0.,\n",
    "                                attn_drop_rate=0.,\n",
    "                                  drop_path_rate=0.1,\n",
    "                                  norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
    "                                  depths=[3, 4, 6, 3],\n",
    "                                  sr_ratios=[8, 4, 2, 1]\n",
    "                                 )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74d374c",
   "metadata": {},
   "source": [
    "# PVT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb36a865",
   "metadata": {},
   "outputs": [],
   "source": [
    "pvt_output = pvt(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88951f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pvt outputs 5 tensors\n",
      "torch.Size([8, 16, 256, 256])\n",
      "torch.Size([8, 64, 64, 64])\n",
      "torch.Size([8, 128, 32, 32])\n",
      "torch.Size([8, 256, 16, 16])\n",
      "torch.Size([8, 512, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "print('pvt outputs', len(pvt_output), 'tensors')\n",
    "for t in pvt_output:\n",
    "    print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "854d4973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pvt_output[1].shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dc0e44",
   "metadata": {},
   "source": [
    "## SWIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20de644e",
   "metadata": {},
   "outputs": [],
   "source": [
    "swin = SwinTransformer(\n",
    "         #patch_size=[4, 4],\n",
    "        embed_dim=96,\n",
    "        depths=[2, 2, 4, ],\n",
    "        num_heads=[2, 4, 8, ],\n",
    "        window_size=[8, 8],\n",
    "        #in_channels = 128,\n",
    "        stochastic_depth_prob=0.2,\n",
    "        #weights=weights,\n",
    "        #progress=progress,\n",
    "        block=SwinTransformerBlockV2,\n",
    "        downsample_layer=PatchMergingV2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7def2e1a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "outputs = []\n",
    "for out in pvt_output:\n",
    "    swin_out = swin(out)\n",
    "    outputs.append(swin_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78205632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 384, 64, 64])\n",
      "torch.Size([8, 384, 16, 16])\n",
      "torch.Size([8, 384, 8, 8])\n",
      "torch.Size([8, 384, 4, 4])\n",
      "torch.Size([8, 384, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "for out in outputs:\n",
    "    print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0eb2cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PVT_w_SWIM(nn.Module):\n",
    "    def __init__(self, img_size = 256,in_channels =16,  ):\n",
    "        \n",
    "        \n",
    "        super().__init__()\n",
    "        self.pvt = pvt = PyramidVisionTransformerV2(img_size=img_size,\n",
    "                                  patch_size=4,\n",
    "                                  in_chans=in_channels,\n",
    "                                  num_classes=1,\n",
    "                                  embed_dims=[64, 128, 256, 512],\n",
    "                                num_heads=[1, 2, 4, 8],\n",
    "                                  mlp_ratios=[4, 4, 4, 4],\n",
    "                                  qkv_bias=True,\n",
    "                                  qk_scale=None,\n",
    "                                  drop_rate=0.,\n",
    "                                attn_drop_rate=0.,\n",
    "                                  drop_path_rate=0.1,\n",
    "                                  norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
    "                                  depths=[2, 2, 2, 2],\n",
    "                                  sr_ratios=[8, 4, 2, 1]\n",
    "                                 )\n",
    "        \n",
    "        self.swin = SwinTransformer(#patch_size=[4, 4],\n",
    "                                    embed_dim=96,\n",
    "                                    depths=[2, 2, 4, ],\n",
    "                                    num_heads=[2, 4, 8, ],\n",
    "                                    window_size=[8, 8],\n",
    "                                    #in_channels = 128,\n",
    "                                    stochastic_depth_prob=0.2,\n",
    "                                    #weights=weights,\n",
    "                                    #progress=progress,\n",
    "                                    block=SwinTransformerBlockV2,\n",
    "                                    downsample_layer=PatchMergingV2,)\n",
    "        \n",
    "        self.head = SegmentationHead()\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # pass through PVT\n",
    "        pvt_outs = self.pvt(x) # outputs 5 tensors\n",
    "        \n",
    "        \n",
    "        # we run each pvt output thru SWIM\n",
    "        # SWIM will outputs 5 tensors \n",
    "        swim_outs = []\n",
    "        swim_outs.append(x)\n",
    "        for pvt_out in pvt_outs:\n",
    "            swim_out = self.swin(pvt_out)\n",
    "            swim_outs.append(swim_out)\n",
    "            \n",
    "        # UPsample and use Segmentation head\n",
    "        final_outs = self.head(swim_outs)\n",
    "        \n",
    "        \n",
    "        \n",
    "        return final_outs\n",
    "        \n",
    "        \n",
    "\n",
    "class SegmentationHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SegmentationHead, self).__init__()\n",
    "\n",
    "        # Define deconvolution layers\n",
    "        self.deconv1 = nn.ConvTranspose2d(in_channels=384, out_channels=384, \n",
    "                                          kernel_size=4, stride=4, padding=0)\n",
    "        self.deconv2 = nn.ConvTranspose2d(in_channels=384, out_channels=384, \n",
    "                                          kernel_size=16, stride=16, padding=0)\n",
    "        self.deconv3 = nn.ConvTranspose2d(in_channels=384, out_channels=384, \n",
    "                                          kernel_size=32, stride=32, padding=0)\n",
    "        self.deconv4 = nn.ConvTranspose2d(in_channels=384, out_channels=384, \n",
    "                                          kernel_size=64, stride=64, padding=0)\n",
    "        self.deconv5 = nn.ConvTranspose2d(in_channels=384, out_channels=384, \n",
    "                                          kernel_size=128, stride=128, padding=0)\n",
    "\n",
    "        # 1x1 convolution to reduce the number of channels to 1\n",
    "        self.conv1x1 = nn.Conv2d(in_channels=16 + 384*5, out_channels=1, kernel_size=1)\n",
    "\n",
    "    def forward(self, swin_outputs):\n",
    "        # Upsample the outputs (except the first one which is already at the desired spatial size)\n",
    "        deconv_outputs = [self.deconv1(swin_outputs[1]), self.deconv2(swin_outputs[2]), \n",
    "                          self.deconv3(swin_outputs[3]), self.deconv4(swin_outputs[4]), \n",
    "                          self.deconv5(swin_outputs[5])]\n",
    "\n",
    "        # Now, we add the first output from swin_outputs (which is already at the desired spatial size)\n",
    "        deconv_outputs = [swin_outputs[0]] + deconv_outputs\n",
    "\n",
    "        # Concatenate\n",
    "        concatenated_output = torch.cat(deconv_outputs, dim=1)  # Shape: (8, 16 + 384*5, 256, 256)\n",
    "\n",
    "        # 1x1 convolution\n",
    "        final_output = self.conv1x1(concatenated_output)  # Shape: (8, 1, 256, 256)\n",
    "\n",
    "        return final_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfecf014",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(8,16,256,256)\n",
    "model = PVT_w_SWIM()\n",
    "final_outs = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6bf66fd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1, 256, 256])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_outs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2376559f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1222214",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7234b62e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4968038",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17dd9728",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SegmentationHead, self).__init__()\n",
    "\n",
    "        # Define deconvolution layers\n",
    "        self.deconv1 = nn.ConvTranspose2d(in_channels=384, out_channels=384, \n",
    "                                          kernel_size=4, stride=4, padding=0)\n",
    "        self.deconv2 = nn.ConvTranspose2d(in_channels=384, out_channels=384, \n",
    "                                          kernel_size=16, stride=16, padding=0)\n",
    "        self.deconv3 = nn.ConvTranspose2d(in_channels=384, out_channels=384, \n",
    "                                          kernel_size=32, stride=32, padding=0)\n",
    "        self.deconv4 = nn.ConvTranspose2d(in_channels=384, out_channels=384, \n",
    "                                          kernel_size=64, stride=64, padding=0)\n",
    "        self.deconv5 = nn.ConvTranspose2d(in_channels=384, out_channels=384, \n",
    "                                          kernel_size=128, stride=128, padding=0)\n",
    "\n",
    "        # 1x1 convolution to reduce the number of channels to 1\n",
    "        self.conv1x1 = nn.Conv2d(in_channels=16 + 384*5, out_channels=1, kernel_size=1)\n",
    "\n",
    "    def forward(self, swin_outputs):\n",
    "        # Upsample the outputs (except the first one which is already at the desired spatial size)\n",
    "        deconv_outputs = [self.deconv1(swin_outputs[1]), self.deconv2(swin_outputs[2]), \n",
    "                          self.deconv3(swin_outputs[3]), self.deconv4(swin_outputs[4]), \n",
    "                          self.deconv5(swin_outputs[5])]\n",
    "\n",
    "        # Now, we add the first output from swin_outputs (which is already at the desired spatial size)\n",
    "        deconv_outputs = [swin_outputs[0]] + deconv_outputs\n",
    "\n",
    "        # Concatenate\n",
    "        concatenated_output = torch.cat(deconv_outputs, dim=1)  # Shape: (8, 16 + 384*5, 256, 256)\n",
    "\n",
    "        # 1x1 convolution\n",
    "        final_output = self.conv1x1(concatenated_output)  # Shape: (8, 1, 256, 256)\n",
    "\n",
    "        return final_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e014fb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fcf11cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given transposed=1, weight of size [384, 384, 4, 4], expected input[1, 1, 256, 256] to have 384 channels, but got 1 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m head \u001b[38;5;241m=\u001b[39m SegmentationHead()\n\u001b[0;32m----> 2\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_outs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[14], line 22\u001b[0m, in \u001b[0;36mSegmentationHead.forward\u001b[0;34m(self, swin_outputs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, swin_outputs):\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# Upsample the outputs (except the first one which is already at the desired spatial size)\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     deconv_outputs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mswin_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeconv2(swin_outputs[\u001b[38;5;241m2\u001b[39m]), \n\u001b[1;32m     23\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeconv3(swin_outputs[\u001b[38;5;241m3\u001b[39m]), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeconv4(swin_outputs[\u001b[38;5;241m4\u001b[39m]), \n\u001b[1;32m     24\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeconv5(swin_outputs[\u001b[38;5;241m5\u001b[39m])]\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# Now, we add the first output from swin_outputs (which is already at the desired spatial size)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     deconv_outputs \u001b[38;5;241m=\u001b[39m [swin_outputs[\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m+\u001b[39m deconv_outputs\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/conv.py:956\u001b[0m, in \u001b[0;36mConvTranspose2d.forward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    951\u001b[0m num_spatial_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    952\u001b[0m output_padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_padding(\n\u001b[1;32m    953\u001b[0m     \u001b[38;5;28minput\u001b[39m, output_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    954\u001b[0m     num_spatial_dims, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m--> 956\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_transpose2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_padding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given transposed=1, weight of size [384, 384, 4, 4], expected input[1, 1, 256, 256] to have 384 channels, but got 1 channels instead"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69125f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "outs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9627170c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9c5759",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalSwinTransformer(nn.Module):\n",
    "    def __init__(self, num_levels, **kwargs):\n",
    "        super().__init__()\n",
    "        self.swins = nn.ModuleList([SwinTransformer(**kwargs) for _ in range(num_levels)])\n",
    "\n",
    "    def forward(self, xs):\n",
    "        outputs = [swin(x) for swin, x in zip(self.swins, xs)]\n",
    "        # Upsample and average outputs\n",
    "        max_h, max_w = outputs[0].shape[-2:]\n",
    "        outputs = [F.interpolate(out, size=(max_h, max_w)) for out in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6362200f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PVTSwinSegmenter(nn.Module):\n",
    "    def __init__(self, img_channels=64, mask_channels=1):\n",
    "        super().__init__()\n",
    "        self.pvt = PyramidVisionTransformer(img_channels=img_channels)\n",
    "        self.swin = SwinTransformer(in_channels=self.pvt.embed_dim)  # Ensure input channels match PVT output\n",
    "        self.segmentation_head = nn.Sequential(\n",
    "            nn.Conv2d(self.swin.embed_dim, mask_channels, kernel_size=1), \n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pvt(x)\n",
    "        x = self.swin(x)\n",
    "        x = self.segmentation_head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25240d76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9802f20b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540e033c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
