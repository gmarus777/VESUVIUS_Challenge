{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4f3ca7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting path:/home/gregory_maruss/VESUVIUS_Challenge/jupyter notebooks\n",
      "Current path:/home/gregory_maruss/VESUVIUS_Challenge\n"
     ]
    }
   ],
   "source": [
    "# Here we take care of paths.\n",
    "# Make sure root project directory is named 'VESUVIUS_Challenge' for this to work\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "print('Starting path:' + os.getcwd())\n",
    "if os.getcwd()[-18:] == 'VESUVIUS_Challenge':\n",
    "    pass\n",
    "else:\n",
    "    PATH = Path().resolve().parents[0]\n",
    "    os.chdir(PATH)\n",
    "\n",
    "# make sure you are in the root folder of the project\n",
    "print('Current path:' + os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1faeebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-17 18:43:34,773 - Created a temporary directory at /tmp/tmpq8offbcw\n",
      "2023-05-17 18:43:34,775 - Writing /tmp/tmpq8offbcw/_remote_module_non_scriptable.py\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import monai\n",
    "from monai.visualize import matshow3d\n",
    "import segmentation_models_pytorch as smp\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Tuple, List\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from Data_Modules.Vesuvius_Dataset import Vesuvius_Tile_Datamodule\n",
    "from lit_models.Vesuvius_Lit_Model import Lit_Model\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "#from Models.PVT import PyramidVisionTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88b0edb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n self.pre_model =monai.networks.nets.UNet(\\n            spatial_dims=3,\\n            in_channels=1,\\n            out_channels=1,\\n            channels=(8,16, 32, 64,128),\\n            strides=(2, 2, 2, 2),\\n            num_res_units=2,\\n            dropout=0,\\n            norm='batch',\\n            bias =False,\\n\\n        )\\n        \\n        \\n        monai.networks.nets.FlexibleUNet(in_channels =Z_DIM,\\n                              out_channels =1 ,\\n                              backbone = 'efficientnet-b3',\\n                              pretrained=True,\\n                              decoder_channels=( 512, 256, 128, 64, 32,  ),\\n                              spatial_dims=2,\\n                              norm=('batch', {'eps': 0.001, 'momentum': 0.1}),\\n                              act=('relu', {'inplace': True}),\\n                              #act = None,\\n                              dropout=0.0,\\n                              decoder_bias=False,\\n                              upsample='deconv',\\n                              interp_mode='nearest',\\n                              is_pad=False)\\n                              \\n                              \\n                              \\n                              \\n        torchvision.models.swin_b( \\n                                                  weights = Swin_B_Weights.IMAGENET1K_V1,\\n                                                  progress: bool = True,\\n                                                  )\\n                                                  \\n                                                  \\n                                                  \\n                                                  \\n                                                  \\n      self.pre_model =monai.networks.nets.UNet(\\n                                        spatial_dims=2,\\n                                        in_channels=16,\\n                                        out_channels=3,\\n                                        channels=( 32, 64,128,),\\n                                        strides=(2, 2, ),\\n                                        num_res_units=2,\\n                                        dropout=0,\\n                                        norm='instance',\\n                                        bias =False,\\n\\n                                    )                                            \\n                                                  \\n\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    " self.pre_model =monai.networks.nets.UNet(\n",
    "            spatial_dims=3,\n",
    "            in_channels=1,\n",
    "            out_channels=1,\n",
    "            channels=(8,16, 32, 64,128),\n",
    "            strides=(2, 2, 2, 2),\n",
    "            num_res_units=2,\n",
    "            dropout=0,\n",
    "            norm='batch',\n",
    "            bias =False,\n",
    "\n",
    "        )\n",
    "        \n",
    "        \n",
    "        monai.networks.nets.FlexibleUNet(in_channels =Z_DIM,\n",
    "                              out_channels =1 ,\n",
    "                              backbone = 'efficientnet-b3',\n",
    "                              pretrained=True,\n",
    "                              decoder_channels=( 512, 256, 128, 64, 32,  ),\n",
    "                              spatial_dims=2,\n",
    "                              norm=('batch', {'eps': 0.001, 'momentum': 0.1}),\n",
    "                              act=('relu', {'inplace': True}),\n",
    "                              #act = None,\n",
    "                              dropout=0.0,\n",
    "                              decoder_bias=False,\n",
    "                              upsample='deconv',\n",
    "                              interp_mode='nearest',\n",
    "                              is_pad=False)\n",
    "                              \n",
    "                              \n",
    "                              \n",
    "                              \n",
    "        torchvision.models.swin_b( \n",
    "                                                  weights = Swin_B_Weights.IMAGENET1K_V1,\n",
    "                                                  progress: bool = True,\n",
    "                                                  )\n",
    "                                                  \n",
    "                                                  \n",
    "                                                  \n",
    "                                                  \n",
    "                                                  \n",
    "      self.pre_model =monai.networks.nets.UNet(\n",
    "                                        spatial_dims=2,\n",
    "                                        in_channels=16,\n",
    "                                        out_channels=3,\n",
    "                                        channels=( 32, 64,128,),\n",
    "                                        strides=(2, 2, ),\n",
    "                                        num_res_units=2,\n",
    "                                        dropout=0,\n",
    "                                        norm='instance',\n",
    "                                        bias =False,\n",
    "\n",
    "                                    )                                            \n",
    "                                                  \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5156250d",
   "metadata": {},
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.pre_model = monai.networks.nets.UNet(\n",
    "                                        spatial_dims=3,\n",
    "                                        in_channels=1,\n",
    "                                        out_channels=1,\n",
    "                                        channels=(16, 32, 64,128, ),\n",
    "                                        strides=(2, 2, 2, ),\n",
    "                                        num_res_units=4,\n",
    "                                        dropout=0,\n",
    "                                        norm='instance',\n",
    "                                        bias =False,\n",
    "\n",
    "                                    )\n",
    "        \n",
    "        self.model =monai.networks.nets.FlexibleUNet(in_channels =Z_DIM,\n",
    "                              out_channels =1 ,\n",
    "                              backbone = 'efficientnet-b3',\n",
    "                              pretrained=False,\n",
    "                              decoder_channels=( 512, 256, 128, 64, 32,  ),\n",
    "                              spatial_dims=2,\n",
    "                              norm=('instance', {'eps': 0.001, 'momentum': 0.1}),\n",
    "                              act=('relu', {'inplace': True}),\n",
    "                              #act = None,\n",
    "                              dropout=0.0,\n",
    "                              decoder_bias=False,\n",
    "                              upsample='deconv',\n",
    "                              interp_mode='nearest',\n",
    "                              is_pad=False)\n",
    "\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pre_model(x)\n",
    "        x = x.squeeze(1)\n",
    "        out = self.model(x)\n",
    "        return out\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7014d496",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATCH_SIZE = 256\n",
    "Z_DIM = 24\n",
    "COMPETITION_DATA_DIR_str =  \"kaggle/input/vesuvius-challenge-ink-detection/\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# change to the line below if not using Apple's M1 or chips\n",
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#mit_b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cff90f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.model =monai.networks.nets.SwinUNETR(img_size = (PATCH_SIZE,PATCH_SIZE),\n",
    "                                                      in_channels = Z_DIM ,\n",
    "                                                      out_channels = 1 ,\n",
    "                                                      depths=(2, 2, 2, 2, 2),\n",
    "                                                      num_heads=(3, 6, 12, 24, 48),\n",
    "                                                      feature_size=48,\n",
    "                                                      norm_name='instance',\n",
    "                                                      drop_rate=0.0,\n",
    "                                                      attn_drop_rate=0.0,\n",
    "                                                      dropout_path_rate=0.1,\n",
    "                                                      normalize=True,\n",
    "                                                      use_checkpoint=False,\n",
    "                                                      spatial_dims=2,\n",
    "                                                      downsample='mergingv2')\n",
    "        \n",
    "    def forward(self, x):\n",
    "       \n",
    "        x = x.squeeze(1)\n",
    "        out = self.model(x)\n",
    "        return out\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15d04b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CFG:\n",
    "    \n",
    "    device = DEVICE\n",
    "    accelerator = 'gpu'\n",
    "    THRESHOLD = 0.4\n",
    "    use_wandb = True\n",
    "    \n",
    "    ######### Dataset #########\n",
    "    \n",
    "    # stage: 'train' or 'test'\n",
    "    stage = 'train' \n",
    "    \n",
    "    # location of competition Data\n",
    "    competition_data_dir = COMPETITION_DATA_DIR_str\n",
    "    \n",
    "    # Number of slices in z-dim: 1<z_dim<65\n",
    "    z_dim = Z_DIM\n",
    "    \n",
    "    # fragments to use for training avalaible [1,2,3]\n",
    "    train_fragment_id=[2,3]\n",
    "    \n",
    "    # fragments to use for validation\n",
    "    val_fragment_id=[1]\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    batch_size = 16\n",
    "    \n",
    "    # Size of the patch and stride for feeding the model\n",
    "    patch_size = PATCH_SIZE\n",
    "    stride = patch_size // 2\n",
    "    \n",
    "    \n",
    "    num_workers = 8\n",
    "    on_gpu = True\n",
    "    \n",
    "    \n",
    "    \n",
    "    ######## Model and Lightning Model paramters ############\n",
    "    \n",
    "    # MODEL\n",
    "    model = Model()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    checkpoint = None\n",
    "    save_directory = None\n",
    "    \n",
    "    \n",
    "    accumulate_grad_batches = 128 // batch_size # experiments showed batch_size * accumulate_grad = 192 is optimal\n",
    "    learning_rate = 0.00005\n",
    "    eta_min = 1e-8\n",
    "    t_max = 70\n",
    "    max_epochs = 120\n",
    "    weight_decay =  0.00001\n",
    "    precision =16\n",
    "    \n",
    "    # checkpointing\n",
    "    save_top_k=5\n",
    "    \n",
    "    monitor=\"FBETA\"\n",
    "    mode=\"max\"\n",
    "    \n",
    "    \n",
    "    ####### Augemtnations ###############\n",
    "    \n",
    "    # Training Aug\n",
    "    train_transforms = [\n",
    "        # A.RandomResizedCrop(\n",
    "        #     size, size, scale=(0.85, 1.0)),\n",
    "        A.Resize(patch_size, patch_size),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.75),\n",
    "        A.ShiftScaleRotate(p=0.75),\n",
    "        A.OneOf([\n",
    "                A.GaussNoise(var_limit=[10, 50]),\n",
    "                A.GaussianBlur(),\n",
    "                A.MotionBlur(),\n",
    "                ], p=0.4),\n",
    "        \n",
    "       \n",
    "        A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.5),\n",
    "        A.CoarseDropout(max_holes=1, max_width=int(patch_size * 0.3), max_height=int(patch_size * 0.3), \n",
    "                        mask_fill_value=0, p=0.5),\n",
    "        # A.Cutout(max_h_size=int(size * 0.6),\n",
    "        #          max_w_size=int(size * 0.6), num_holes=1, p=1.0),\n",
    "        A.Normalize(\n",
    "            mean= [0] * z_dim,\n",
    "            std= [1] * z_dim\n",
    "        ),\n",
    "        ToTensorV2(transpose_mask=True),\n",
    "    ]\n",
    "    \n",
    "\n",
    "    \n",
    "    # Validaiton Aug\n",
    "    val_transforms = [\n",
    "        A.Resize(patch_size, patch_size),\n",
    "        A.Normalize(\n",
    "            mean= [0] * z_dim,\n",
    "            std= [1] * z_dim\n",
    "        ),\n",
    "        ToTensorV2(transpose_mask=True),\n",
    "    ]\n",
    "    \n",
    "    # Test Aug\n",
    "    test_transforms = [\n",
    "        A.Resize(patch_size, patch_size),\n",
    "        A.Normalize(\n",
    "            mean=[0] * z_dim,\n",
    "            std=[1] * z_dim\n",
    "        ),\n",
    "\n",
    "        ToTensorV2(transpose_mask=True),\n",
    "    ]\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57caa6b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd79a0c1afb849b5bfc24ee64b2f8c36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "049ae727cc2c4247a35977adac6b505b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfdd684ad1ec40a59c92ef1698363a2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = Vesuvius_Tile_Datamodule(cfg=CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7c8a690",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgmarus\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/gregory_maruss/VESUVIUS_Challenge/wandb/run-20230517_184425-ii73ik41</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gmarus/VESUVIUS_Challenge/runs/ii73ik41' target=\"_blank\">lilac-sunset-590</a></strong> to <a href='https://wandb.ai/gmarus/VESUVIUS_Challenge' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gmarus/VESUVIUS_Challenge' target=\"_blank\">https://wandb.ai/gmarus/VESUVIUS_Challenge</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gmarus/VESUVIUS_Challenge/runs/ii73ik41' target=\"_blank\">https://wandb.ai/gmarus/VESUVIUS_Challenge/runs/ii73ik41</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lit_model = Lit_Model(cfg=CFG,)\n",
    "\n",
    "Checkpoint = True\n",
    "if Checkpoint:\n",
    "    lit_model = lit_model.load_from_checkpoint('logs/SWINtr_24_MoFoDi_256/last.ckpt',\n",
    "                                               #learning_rate =2e-5 ,\n",
    "                                                t_max = 70,\n",
    "                                               eta_min = 1e-8,\n",
    "                                               weight_decay =  0.00001,\n",
    "                                              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eda33894",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-17 18:44:31,422 - Using 16bit None Automatic Mixed Precision (AMP)\n",
      "2023-05-17 18:44:31,479 - GPU available: True (cuda), used: True\n",
      "2023-05-17 18:44:31,481 - TPU available: False, using: 0 TPU cores\n",
      "2023-05-17 18:44:31,482 - IPU available: False, using: 0 IPUs\n",
      "2023-05-17 18:44:31,482 - HPU available: False, using: 0 HPUs\n",
      "2023-05-17 18:44:32,682 - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "2023-05-17 18:44:32,690 - \n",
      "  | Name                  | Type                  | Params\n",
      "----------------------------------------------------------------\n",
      "0 | metrics               | ModuleDict            | 0     \n",
      "1 | model                 | Model                 | 25.2 M\n",
      "2 | loss_dice             | DiceLoss              | 0     \n",
      "3 | loss_tversky          | TverskyLoss           | 0     \n",
      "4 | loss_focal            | FocalLoss             | 0     \n",
      "5 | loss_bce              | SoftBCEWithLogitsLoss | 0     \n",
      "6 | loss_monai_focal_dice | DiceFocalLoss         | 0     \n",
      "----------------------------------------------------------------\n",
      "25.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "25.2 M    Total params\n",
      "50.307    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0c2c5014cb44af49615105b431a317f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 4.9975e-05.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 4.9899e-05.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 4.9774e-05.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 4.9598e-05.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 4.9373e-05.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gregory_maruss/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "SAVE_DIR = 'logs/SWINtr_24_MoFoDi_256'\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    save_top_k=5,\n",
    "    monitor=\"FBETA\",\n",
    "    mode=\"max\",\n",
    "    dirpath=SAVE_DIR,\n",
    "    filename=\"SWINtr_24_MoFoDi_256{epoch:02d}{FBETA:.2f}{val_loss:.2f}{fbeta_4:.2f}{recall:.2f}{precision:.2f}\",\n",
    "    save_last =True,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "        accelerator=CFG.accelerator,\n",
    "        #benchmark=True,\n",
    "        max_epochs=CFG.max_epochs,\n",
    "        check_val_every_n_epoch= 1,\n",
    "        devices=1,\n",
    "        #fast_dev_run=fast_dev_run,\n",
    "        logger=pl.loggers.CSVLogger(save_dir=SAVE_DIR),\n",
    "        log_every_n_steps=1,\n",
    "        default_root_dir = SAVE_DIR,\n",
    "        #overfit_batches=1,\n",
    "        precision=CFG.precision,\n",
    "        accumulate_grad_batches=CFG.accumulate_grad_batches, \n",
    "        callbacks=[checkpoint_callback],\n",
    "        #resume_from_checkpoint ='logs/SWINtr_24_bce75_tver_alpha50_gamma1/last.ckpt'\n",
    "    \n",
    "        \n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trainer.fit(lit_model, datamodule=dataset,\n",
    "            #ckpt_path='logs/Local_Originalgood/last.ckpt'\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284f248e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6feb5b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
