{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42b3030c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we take care of paths.\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "print('Starting path:' + os.getcwd())\n",
    "if os.getcwd()[-18:] == 'VESUVIUS_Challenge':\n",
    "    pass\n",
    "else:\n",
    "    PATH = Path().resolve().parents[0]\n",
    "    os.chdir(PATH)\n",
    "\n",
    "# make sure you are in Paragraph_to_Tex folder\n",
    "print('Current path:' + os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17d76f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Dataset, ConcatDataset\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import glob\n",
    "import PIL.Image as Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from tqdm import tqdm\n",
    "from ipywidgets import interact, fixed\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "transform = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a42a82",
   "metadata": {},
   "source": [
    "# Dataset Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972ad61e",
   "metadata": {},
   "source": [
    "### CONFIGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a37bc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PATH = 'kaggle/input/vesuvius-challenge/'\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# scroll_1 size = 8181, 6330\n",
    "# scroll_2 size = 14830, 9506\n",
    "# scroll_3 size = 7606, 5249"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d3feed",
   "metadata": {},
   "source": [
    "### Base_Dataset class \n",
    "- due to multiprocessing issues in Ipython we import it from from Data_Modules.Base_Dataset import Base_Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad808c95",
   "metadata": {},
   "source": [
    "class Base_Dataset(Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                image_stack,\n",
    "                label,\n",
    "                pixels,\n",
    "                 buffer,\n",
    "                 z_dim,\n",
    "\n",
    "                 ):\n",
    "\n",
    "        self.image_stack = image_stack\n",
    "        self.label = label\n",
    "        self.pixels = pixels\n",
    "        self.buffer = buffer\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pixels)\n",
    "\n",
    "\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        y,x = self.pixels[index]\n",
    "        subvolume = self.image_stack[:, y - self.buffer:y + self.buffer + 1, x - self.buffer:x + self.buffer + 1].view(1, self.z_dim, self.buffer * 2 + 1,self.buffer * 2 + 1)\n",
    "        inklabel = self.label[y, x].view(1)\n",
    "        return subvolume, inklabel\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743d4f4c",
   "metadata": {},
   "source": [
    "### Scrolls_Dataset wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bec15ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Data_Modules.Base_Dataset import Base_Dataset\n",
    "class Scrolls_Dataset(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self,\n",
    "                 buffer = 30,\n",
    "                 z_start = 27,\n",
    "                 z_dim = 10,\n",
    "                 validation_rect = (1100, 3500, 700, 950),\n",
    "                shared_height = 8000,\n",
    "                 downsampling =None,\n",
    "                 scroll_fragments = [1,2,3],\n",
    "                 stage = 'train',\n",
    "                 shuffle=True,\n",
    "                 batch_size=8,\n",
    "                 num_workers =4 ,\n",
    "                 on_gpu= False,\n",
    "\n",
    "\n",
    "                 ):\n",
    "\n",
    "        self.buffer = buffer\n",
    "        self.z_start = z_start\n",
    "        self.z_dim = z_dim\n",
    "        self.validation_rect = validation_rect\n",
    "        self.shared_height = shared_height\n",
    "        self.downsampling = downsampling\n",
    "        self.scroll_fragments = scroll_fragments\n",
    "        self.stage = stage\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.on_gpu = on_gpu\n",
    "\n",
    "\n",
    "    def prepare_data(self, *args, **kwargs):\n",
    "        if self.stage.lower() == 'train':\n",
    "\n",
    "\n",
    "            z_slices = [[] for _ in range(len(self.scroll_fragments))]\n",
    "            labels =  [[] for _ in range(len(self.scroll_fragments))]\n",
    "            masks = [[] for _ in range(len(self.scroll_fragments))]\n",
    "\n",
    "            for i in self.scroll_fragments:\n",
    "                # get z_slices .tiffs paths\n",
    "                z_slices[i-1] += sorted(glob.glob(f\"{PATH}/{'train'}/{i}/surface_volume/*.tif\"))[self.z_start:self.z_start + self.z_dim]\n",
    "                # get labels\n",
    "                labels[i-1] = self.load_labels('train', i)\n",
    "                # get masks\n",
    "                masks[i-1] = self.load_mask('train', i)\n",
    "\n",
    "            # get images of z-slices and convert them to tensors\n",
    "            images = [[] for _ in range(len(self.scroll_fragments))]\n",
    "            for i in range(len(self.scroll_fragments)):\n",
    "                images[i] = self.load_slices(z_slices[i])\n",
    "\n",
    "            # concat images, labels and masks of different scrolls\n",
    "            images_tensors = torch.cat([image for image in images], axis=-1)\n",
    "            label_tensors =  torch.cat([label for label in labels], axis=-1)\n",
    "            mask_tensors =  np.concatenate([mask for mask in masks], axis=-1)\n",
    "            del images\n",
    "            del z_slices\n",
    "            del labels\n",
    "            del masks\n",
    "\n",
    "            # obtain train_pixesl and val_pixels\n",
    "            train_pixels , val_pixels = self.split_train_val(mask_tensors)\n",
    "            self.mask = mask_tensors\n",
    "            #del mask_tensors\n",
    "            \n",
    "            self.data_train = Base_Dataset(image_stack=images_tensors, label=label_tensors,  pixels=train_pixels, buffer=self.buffer, z_dim=self.z_dim )\n",
    "            self.data_val = Base_Dataset(image_stack=images_tensors, label=label_tensors,  pixels=val_pixels,  buffer=self.buffer, z_dim=self.z_dim)\n",
    "\n",
    "            del images_tensors\n",
    "            del label_tensors\n",
    "            del train_pixels\n",
    "            del val_pixels\n",
    "\n",
    "\n",
    "\n",
    "        # TODO: finish the same for test, note paths are different\n",
    "        elif self.stage.lower() == 'test':\n",
    "\n",
    "            # get z_slices paths\n",
    "            z_slices = [[], []]\n",
    "            for i, l in enumerate(['a','b']):\n",
    "                z_slices[i] = sorted(glob.glob(f\"{PATH}/{'test'}/{l}/surface_volume/*.tif\"))[self.z_star:self.z_star + self.z_dim]\n",
    "\n",
    "\n",
    "\n",
    "    def train_dataloader(self, *args, **kwargs) -> DataLoader:\n",
    "        \"\"\"\n",
    "        construct a dataloader for training data\n",
    "        data is shuffled !\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return DataLoader(\n",
    "            self.data_train,\n",
    "            shuffle=True,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.on_gpu,\n",
    "            #collate_fn=self.collate_function,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return DataLoader(\n",
    "            self.data_val,\n",
    "            shuffle=False,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.on_gpu,\n",
    "            #collate_fn=self.collate_function\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return DataLoader(\n",
    "            self.data_test,\n",
    "            shuffle=False,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.on_gpu,\n",
    "            collate_fn=self.collate_function,\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # image_stack = torch.stack([torch.from_numpy(image) for image in images], dim=0).to(DEVICE)\n",
    "    def load_slices(self, z_slices_fnames):\n",
    "        images = []\n",
    "        for z, filename in tqdm(enumerate(z_slices_fnames)):\n",
    "            img = Image.open(filename)\n",
    "            img = self.resize(img)\n",
    "            z_slice = np.array(img, dtype=\"float32\")/65535.0\n",
    "            images.append(z_slice)\n",
    "        return torch.stack([torch.from_numpy(image) for image in images], dim=0).to(DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "    def load_mask(self, split, index):\n",
    "        img = Image.open(f\"{PATH}/{split}/{index}/mask.png\").convert('1')\n",
    "        img = self.resize(img)\n",
    "        return np.array(img)\n",
    "\n",
    "\n",
    "\n",
    "    def load_labels(self, split, index):\n",
    "        img = Image.open(f\"{PATH}/{split}/{index}/inklabels.png\")\n",
    "        img = self.resize(img)\n",
    "        return torch.from_numpy(np.array(img)).gt(0).float().to(DEVICE)\n",
    "\n",
    "\n",
    "    def resize(self, img):\n",
    "        current_width, current_height = img.size\n",
    "        aspect_ratio = current_width / current_height\n",
    "        new_width = int(self.shared_height * aspect_ratio)\n",
    "        new_size = (new_width, self.shared_height)\n",
    "        img = img.resize(new_size)\n",
    "        return img\n",
    "\n",
    "\n",
    "\n",
    "    def split_train_val(self,mask):\n",
    "        rect = self.validation_rect\n",
    "        not_border = np.zeros(mask.shape, dtype=bool)\n",
    "        not_border[self.buffer:mask.shape[0] - self.buffer, self.buffer:mask.shape[1] - self.buffer] = True\n",
    "        arr_mask = np.array(mask) * not_border\n",
    "        inside_rect = np.zeros(mask.shape, dtype=bool) * arr_mask\n",
    "        inside_rect[rect[1]:rect[1] + rect[3] + 1, rect[0]:rect[0] + rect[2] + 1] = True\n",
    "        outside_rect = np.ones(mask.shape, dtype=bool) * arr_mask\n",
    "        outside_rect[rect[1]:rect[1] + rect[3] + 1, rect[0]:rect[0] + rect[2] + 1] = False\n",
    "        pixels_inside_rect = np.argwhere(inside_rect)\n",
    "        pixels_outside_rect = np.argwhere(outside_rect)\n",
    "        return pixels_outside_rect, pixels_inside_rect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e42e36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiating Dataset with parameters\n",
    "\n",
    "# buffer =   -- x,y patchsize for training\n",
    "# z_start =  --  Offset of slices in the z direction\n",
    "# z_dim =    -- Number of slices in the z direction. Max value is (64 - z_start)\n",
    "# validation_rect =  -- rectangle removed for validation set\n",
    "# shared_height = -- Height to resize all scrolls\n",
    "# scroll_fragments = -- scrolls to be used \n",
    "\n",
    "dataset = Scrolls_Dataset(\n",
    "                buffer = 31,\n",
    "                 z_start = 20,\n",
    "                 z_dim = 24,\n",
    "                 validation_rect = (1100, 3500, 700, 950),\n",
    "                shared_height = 5000,\n",
    "                 downsampling =None,\n",
    "                 scroll_fragments = [1,2,3],\n",
    "                 stage = 'train',\n",
    "                 shuffle=True,\n",
    "                 batch_size=8,\n",
    "                 num_workers =4 ,\n",
    "                 on_gpu= False,\n",
    "                          \n",
    "                         )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "906614ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gregory/opt/anaconda3/lib/python3.9/site-packages/PIL/Image.py:3035: DecompressionBombWarning: Image size (140973980 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "24it [00:06,  3.82it/s]\n",
      "24it [00:13,  1.74it/s]\n",
      "24it [00:04,  4.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# prepeare data, by processng images and loading dataloader\n",
    "\n",
    "#dataset.prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f580877",
   "metadata": {},
   "source": [
    "### Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d494b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subvolume shape: torch.Size([8, 1, 24, 63, 63])\n",
      "inklabel shape: torch.Size([8, 1])\n"
     ]
    }
   ],
   "source": [
    "dataloader = iter(dataset.train_dataloader())\n",
    "train_tensor = None\n",
    "train_label = None\n",
    "for i in range(1):\n",
    "    # Get image and label from train data -- change number for different ones\n",
    "    #print(next(dataloader))\n",
    "    subvolume, inklabel = next(dataloader)\n",
    "    print('subvolume shape:',subvolume.shape)\n",
    "    print('inklabel shape:',inklabel.shape)\n",
    "    train_tensor = subvolume\n",
    "    train_label = inklabel\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dcb6d9",
   "metadata": {},
   "source": [
    "### MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26f7cca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gregory/opt/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/gregory/opt/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# three backbone options (also try U-NET and V-Net)\n",
    "r3d_18 = models.video.r3d_18(pretrained=False)\n",
    "\n",
    "#r2plus1d_18 = models.video.r2plus1d_18(pretrained=False)\n",
    "#mc3_18 = models.video.mc3_18(pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ebf9cf3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Models.positional_encoding'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mModels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpositional_encoding\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PositionalEncoding1D, PositionalEncoding2D\n\u001b[1;32m     12\u001b[0m TF_DIM \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m    \u001b[38;5;66;03m# embedding_dim 128\u001b[39;00m\n\u001b[1;32m     13\u001b[0m TF_FC_DIM \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m \u001b[38;5;66;03m# decoder fully connected dim 256\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Models.positional_encoding'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from typing import Union\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models\n",
    "from torch import Tensor\n",
    "from Models.positional_encoding import PositionalEncoding1D, PositionalEncoding2D\n",
    "\n",
    "\n",
    "\n",
    "TF_DIM = 256    # embedding_dim 128\n",
    "TF_FC_DIM = 512 # decoder fully connected dim 256\n",
    "TF_DROPOUT = 0.4 # decoder_dropout 0.4\n",
    "TF_LAYERS = 6   # decoder_layers\n",
    "TF_NHEAD = 8    # decoder_heads\n",
    "RESNET_DIM = 512  # hard-coded\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# TODO: Pass parameters from the dataset\n",
    "\n",
    "# self.d_model = TF_DIM\n",
    "# self.max_output_len = dataset.max_label_length\n",
    "# dim_feedforward = TF_FC_DIM\n",
    "\n",
    "\n",
    "'''\n",
    "Image to Tex OCR:\n",
    "Resnet18 encoder with the first three layers and Transformer Decoder.\n",
    "\n",
    "'''\n",
    "class ResNetTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes = 2,\n",
    "        max_label_length = 1,\n",
    "        embedding_dim = TF_DIM,\n",
    "        decoder_heads = TF_NHEAD,\n",
    "        decoder_layers = TF_LAYERS,\n",
    "        decoder_dropout = TF_DROPOUT,\n",
    "        decoder_fc = TF_FC_DIM,\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_output_len = max_label_length\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "\n",
    "        ### Encoder ###\n",
    "        r3d_18 = torchvision.models.video.r3d_18(pretrained=False)\n",
    "        self.backbone = nn.Sequential(\n",
    "            r3d_18.stem,\n",
    "            r3d_18.layer1,\n",
    "            r3d_18.layer2,\n",
    "            r3d_18.layer3,\n",
    "            r3d_18.layer4,\n",
    "        )\n",
    "        self.bottleneck = nn.Conv3d(RESNET_DIM, self.embedding_dim, 1) # in channels, out channels, stride\n",
    "        \n",
    "        ### Decoder ###\n",
    "        \n",
    "        self.y_mask = generate_square_subsequent_mask(self.max_output_len)\n",
    "       \n",
    "        transformer_decoder_layer = nn.TransformerDecoderLayer(self.embedding_dim, decoder_heads, decoder_fc, decoder_dropout)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(transformer_decoder_layer, decoder_layers)\n",
    "        self.fc = nn.Linear(self.embedding_dim, self.num_classes)\n",
    "\n",
    "\n",
    "        # It is empirically important to initialize weights properly\n",
    "        #if self.training:\n",
    "            #self._init_weights()\n",
    "        self._init_weights()\n",
    "\n",
    "\n",
    "    def _init_weights(self) -> None:\n",
    "        \"\"\"Initialize weights.\"\"\"\n",
    "        init_range = 0.1\n",
    "        self.embedding.weight.data.uniform_(-init_range, init_range)\n",
    "        self.fc.bias.data.zero_()\n",
    "        self.fc.weight.data.uniform_(-init_range, init_range)\n",
    "\n",
    "        nn.init.kaiming_normal_(\n",
    "            self.bottleneck.weight.data,\n",
    "            a=0,\n",
    "            mode=\"fan_out\",\n",
    "            nonlinearity=\"relu\",\n",
    "        )\n",
    "        if self.bottleneck.bias is not None:\n",
    "            _, fan_out = nn.init._calculate_fan_in_and_fan_out(self.bottleneck.weight.data)\n",
    "            bound = 1 / math.sqrt(fan_out)\n",
    "            nn.init.normal_(self.bottleneck.bias, -bound, bound)\n",
    "\n",
    "\n",
    "    def forward(self, x: Tensor, y: Tensor) -> Tensor:\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x: (B, _E, _H, _W)\n",
    "            y: (B, Sy) with elements in (0, num_classes - 1)\n",
    "\n",
    "        Returns:\n",
    "            (B, num_classes, Sy) logits\n",
    "        \"\"\"\n",
    "        encoded_x = self.encode(x)  # (Sx, B, E)\n",
    "        output = self.decode(y, encoded_x)  # (Sy, B, num_classes)\n",
    "        output = output.permute(1, 2, 0)  # (B, num_classes, Sy)\n",
    "        return output\n",
    "\n",
    "    def encode(self, x: Tensor) -> Tensor:\n",
    "       \n",
    "        # Resnet expects 3 channels but training images are in gray scale\n",
    "        if x.shape[1] == 1:   # (B, 1, z_dim, r) where r = Buffer*2+1\n",
    "            x = x.repeat(1, 3, 1, 1, 1)\n",
    "        x = self.backbone(x.float())  # (B, RESNET_DIM, Z_DIM, R); Z_DIM = z_dim // 8, R = r // 16\n",
    "        \n",
    "        x = self.bottleneck(x)  # (B, E, H, W); E:= embedding dim\n",
    "\n",
    "        # x = x * math.sqrt(self.embedding_dim)   # This prevented any learning\n",
    "       \n",
    "        x = x.flatten(start_dim=2)  # (B, E, H * W)\n",
    "        x = x.permute(2, 0, 1)  # (Sx, B, E); Sx = H * W\n",
    "        return x\n",
    "\n",
    "    def decode(self, y: Tensor, encoded_x: Tensor) -> Tensor:\n",
    "        \"\"\"Decode encoded inputs with teacher-forcing.\n",
    "\n",
    "        Args:\n",
    "            encoded_x: (Sx, B, E)\n",
    "            y: (B, Sy) with elements in (0, num_classes - 1)\n",
    "\n",
    "        Returns:\n",
    "            (Sy, B, num_classes) logits\n",
    "        \"\"\"\n",
    "        y = y.permute(1, 0)  # (Sy, B)\n",
    "        y = self.embedding(y) * math.sqrt(self.embedding_dim)  # (Sy, B, E)\n",
    "        y = self.word_positional_encoder(y)  # (Sy, B, E)\n",
    "        Sy = y.shape[0]\n",
    "        y_mask = self.y_mask[:Sy, :Sy].type_as(encoded_x)  # (Sy, Sy)\n",
    "        output = self.transformer_decoder(y, encoded_x, y_mask)  # (Sy, B, E)\n",
    "        output = self.fc(output)  # (Sy, B, num_classes)\n",
    "        return output\n",
    "\n",
    "    def predict(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Make predctions at inference time.\n",
    "\n",
    "        Args:\n",
    "            x: (B, C, H, W). Input images.\n",
    "\n",
    "        Returns:\n",
    "            (B, max_output_len) with elements in (0, num_classes - 1).\n",
    "        \"\"\"\n",
    "        B = x.shape[0]\n",
    "        S = self.max_output_len\n",
    "\n",
    "        encoded_x = self.encode(x)  # (Sx, B, E)\n",
    "\n",
    "        output_indices = torch.full(size=(B, S), fill_value=self.pad_index).type_as(x).long()\n",
    "        output_indices[:, 0] = self.sos_index\n",
    "        has_ended = torch.full((B,), 0)\n",
    "        has_ended.to(torch.bool)\n",
    "\n",
    "        for Sy in range(1, S):\n",
    "            y = output_indices[:, :Sy]  # (B, Sy)\n",
    "            logits = self.decode(y, encoded_x)  # (Sy, B, num_classes)\n",
    "            # Select the token with the highest conditional probability\n",
    "            output = torch.argmax(logits, dim=-1)  # (Sy, B)\n",
    "            if B ==1:\n",
    "                output_indices[:, Sy:Sy+1] = output[-1:]\n",
    "            else:\n",
    "                output_indices[:, Sy] = output[-1:]  # Set the last output token\n",
    "\n",
    "            # Early stopping of prediction loop to speed up prediction\n",
    "            has_ended |= (output_indices[:, Sy] == self.eos_index).type_as(has_ended)\n",
    "            if torch.all(has_ended):\n",
    "                break\n",
    "\n",
    "        # Set all tokens after end token to be padding\n",
    "        eos_positions = find_first(output_indices, self.eos_index)\n",
    "        for i in range(B):\n",
    "            j = int(eos_positions[i].item()) + 1\n",
    "            output_indices[i, j:] = self.pad_index\n",
    "\n",
    "        return output_indices\n",
    "\n",
    "\n",
    "\n",
    "def generate_square_subsequent_mask(size: int) -> Tensor:\n",
    "    \"\"\"Generate a triangular (size, size) mask.\"\"\"\n",
    "    mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float(\"-inf\")).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def find_first(x: Tensor, element: Union[int, float], dim: int = 1) -> Tensor:\n",
    "    \"\"\"Find the first occurence of element in x along a given dimension.\n",
    "\n",
    "    Args:\n",
    "        x: The input tensor to be searched.\n",
    "        element: The number to look for.\n",
    "        dim: The dimension to reduce.\n",
    "\n",
    "    Returns:\n",
    "        Indices of the first occurence of the element in x. If not found, return the\n",
    "        length of x along dim.\n",
    "\n",
    "    Usage:\n",
    "        >>> first_element(Tensor([[1, 2, 3], [2, 3, 3], [1, 1, 1]]), 3)\n",
    "        tensor([2, 1, 3])\n",
    "\n",
    "    Reference:\n",
    "        https://discuss.pytorch.org/t/first-nonzero-index/24769/9\n",
    "\n",
    "        I fixed an edge case where the element we are looking for is at index 0. The\n",
    "        original algorithm will return the length of x instead of 0.\n",
    "    \"\"\"\n",
    "    mask = x == element\n",
    "    found, indices = ((mask.cumsum(dim) == 1) & mask).max(dim)\n",
    "    indices[(~found) & (indices == 0)] = x.shape[dim]\n",
    "    return indices\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee8a7b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbdfdd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77ed8174",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = nn.Sequential(\n",
    "            r3d_18.stem,\n",
    "            r3d_18.layer1,\n",
    "            r3d_18.layer2,\n",
    "            r3d_18.layer3,\n",
    "            r3d_18.layer4,\n",
    "        )\n",
    "bottleneck = nn.Conv3d(512, 128, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "973dbf40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 24, 63, 63])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3a3e80dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1, 24, 63, 63])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "63e09233",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if train_tensor.shape[1] == 1:\n",
    "    train_tensor = train_tensor.repeat(1, 3, 1, 1, 1)\n",
    "output = backbone(train_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2052ee87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512, 3, 4, 4])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [8, 1, 10, H, W] goes to [8, RESNTETDIM of 512, 2, H/16, W/16]\n",
    "# [8, 1, 10, 65, 65] goes to [8, 512, 2, 5, 5]\n",
    "# [8, 1, 10, 63, 63] goes to [8, 512, 2, 5, 5]\n",
    "# [8, 1, 10, 61, 61]  goes to [8, 512, 2, 4, 4]\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "be09a059",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = bottleneck(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "049991a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 128, 3, 4, 4])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f1c179f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "out =out.flatten(start_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d89166cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 6144])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "98a62ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = nn.Linear(6144, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f4150ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b2eb6ff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "74c5a014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label.squeeze(1).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b11b695a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label.squeeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6dbb8099",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ee00d9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "los = loss_fn(res, train_label.squeeze(1).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "14e88ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6174, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "los"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "92408be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n",
      "input:  tensor([[0.1593, 0.3356, 0.4385, 0.7986, 0.1470],\n",
      "        [0.5411, 0.6084, 0.8018, 0.6210, 0.1916],\n",
      "        [0.5296, 0.5167, 0.5870, 0.0325, 0.8659]])\n",
      "target:  tensor([2, 3, 1])\n",
      "Cross Entropy Loss:  tensor(1.5901)\n"
     ]
    }
   ],
   "source": [
    "# Example of target with class indices\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input = torch.rand(3, 5)\n",
    "target = torch.empty(3, dtype = torch.long).random_(5)\n",
    "print(target.shape)\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "output = loss(input, target)\n",
    "\n",
    "print('input: ', input)\n",
    "print('target: ', target)\n",
    "print('Cross Entropy Loss: ', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "afb7a988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d8830b7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da232971",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
