{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5354bff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting path:/Users/gregory/PROJECT_ML/VESUVIUS_Challenge/jupyter notebooks\n",
      "Current path:/Users/gregory/PROJECT_ML/VESUVIUS_Challenge\n"
     ]
    }
   ],
   "source": [
    "# Here we take care of paths.\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "print('Starting path:' + os.getcwd())\n",
    "if os.getcwd()[-18:] == 'VESUVIUS_Challenge':\n",
    "    pass\n",
    "else:\n",
    "    PATH = Path().resolve().parents[0]\n",
    "    os.chdir(PATH)\n",
    "\n",
    "# make sure you are in Paragraph_to_Tex folder\n",
    "print('Current path:' + os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac301599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Dataset, ConcatDataset\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import glob\n",
    "import PIL.Image as Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from tqdm import tqdm\n",
    "from ipywidgets import interact, fixed\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "transform = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcafd048",
   "metadata": {},
   "source": [
    "# Dataset Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a75eb49",
   "metadata": {},
   "source": [
    "### CONFIGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "148fb743",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PATH = 'kaggle/input/vesuvius-challenge/'\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# scroll_1 size = 8181, 6330\n",
    "# scroll_2 size = 14830, 9506\n",
    "# scroll_3 size = 7606, 5249"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f967ca8",
   "metadata": {},
   "source": [
    "### Base_Dataset class \n",
    "- due to multiprocessing issues in Ipython we import it from from Data_Modules.Base_Dataset import Base_Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9050c8ff",
   "metadata": {},
   "source": [
    "### Scrolls_Dataset wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "313629da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Data_Modules.Base_Dataset import Base_Dataset\n",
    "class Scrolls_Dataset(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self,\n",
    "                 buffer = 30,\n",
    "                 z_start = 27,\n",
    "                 z_dim = 10,\n",
    "                 validation_rect = (1100, 3500, 700, 950),\n",
    "                shared_height = 8000,\n",
    "                 downsampling =None,\n",
    "                 scroll_fragments = [1,2,3],\n",
    "                 stage = 'train',\n",
    "                 shuffle=True,\n",
    "                 batch_size=8,\n",
    "                 num_workers =4 ,\n",
    "                 on_gpu= False,\n",
    "\n",
    "\n",
    "                 ):\n",
    "\n",
    "        self.buffer = buffer\n",
    "        self.z_start = z_start\n",
    "        self.z_dim = z_dim\n",
    "        self.validation_rect = validation_rect\n",
    "        self.shared_height = shared_height\n",
    "        self.downsampling = downsampling\n",
    "        self.scroll_fragments = scroll_fragments\n",
    "        self.stage = stage\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.on_gpu = on_gpu\n",
    "\n",
    "\n",
    "    def prepare_data(self, *args, **kwargs):\n",
    "        if self.stage.lower() == 'train':\n",
    "\n",
    "\n",
    "            z_slices = [[] for _ in range(len(self.scroll_fragments))]\n",
    "            labels =  [[] for _ in range(len(self.scroll_fragments))]\n",
    "            masks = [[] for _ in range(len(self.scroll_fragments))]\n",
    "\n",
    "            for i in self.scroll_fragments:\n",
    "                # get z_slices .tiffs paths\n",
    "                z_slices[i-1] += sorted(glob.glob(f\"{PATH}/{'train'}/{i}/surface_volume/*.tif\"))[self.z_start:self.z_start + self.z_dim]\n",
    "                # get labels\n",
    "                labels[i-1] = self.load_labels('train', i)\n",
    "                # get masks\n",
    "                masks[i-1] = self.load_mask('train', i)\n",
    "\n",
    "            # get images of z-slices and convert them to tensors\n",
    "            images = [[] for _ in range(len(self.scroll_fragments))]\n",
    "            for i in range(len(self.scroll_fragments)):\n",
    "                images[i] = self.load_slices(z_slices[i])\n",
    "\n",
    "            # concat images, labels and masks of different scrolls\n",
    "            images_tensors = torch.cat([image for image in images], axis=-1)\n",
    "            label_tensors =  torch.cat([label for label in labels], axis=-1)\n",
    "            mask_tensors =  np.concatenate([mask for mask in masks], axis=-1)\n",
    "            del images\n",
    "            del z_slices\n",
    "            del labels\n",
    "            del masks\n",
    "\n",
    "            # obtain train_pixesl and val_pixels\n",
    "            train_pixels , val_pixels = self.split_train_val(mask_tensors)\n",
    "            self.mask = mask_tensors\n",
    "            #del mask_tensors\n",
    "            \n",
    "            self.data_train = Base_Dataset(image_stack=images_tensors, label=label_tensors,  pixels=train_pixels, buffer=self.buffer, z_dim=self.z_dim )\n",
    "            self.data_val = Base_Dataset(image_stack=images_tensors, label=label_tensors,  pixels=val_pixels,  buffer=self.buffer, z_dim=self.z_dim)\n",
    "\n",
    "            del images_tensors\n",
    "            del label_tensors\n",
    "            del train_pixels\n",
    "            del val_pixels\n",
    "\n",
    "\n",
    "\n",
    "        # TODO: finish the same for test, note paths are different\n",
    "        elif self.stage.lower() == 'test':\n",
    "\n",
    "            # get z_slices paths\n",
    "            z_slices = [[], []]\n",
    "            for i, l in enumerate(['a','b']):\n",
    "                z_slices[i] = sorted(glob.glob(f\"{PATH}/{'test'}/{l}/surface_volume/*.tif\"))[self.z_star:self.z_star + self.z_dim]\n",
    "\n",
    "\n",
    "\n",
    "    def train_dataloader(self, *args, **kwargs) -> DataLoader:\n",
    "        \"\"\"\n",
    "        construct a dataloader for training data\n",
    "        data is shuffled !\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return DataLoader(\n",
    "            self.data_train,\n",
    "            shuffle=True,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.on_gpu,\n",
    "            #collate_fn=self.collate_function,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return DataLoader(\n",
    "            self.data_val,\n",
    "            shuffle=False,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.on_gpu,\n",
    "            #collate_fn=self.collate_function\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return DataLoader(\n",
    "            self.data_test,\n",
    "            shuffle=False,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.on_gpu,\n",
    "            collate_fn=self.collate_function,\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # image_stack = torch.stack([torch.from_numpy(image) for image in images], dim=0).to(DEVICE)\n",
    "    def load_slices(self, z_slices_fnames):\n",
    "        images = []\n",
    "        for z, filename in tqdm(enumerate(z_slices_fnames)):\n",
    "            img = Image.open(filename)\n",
    "            img = self.resize(img)\n",
    "            z_slice = np.array(img, dtype=\"float32\")/65535.0\n",
    "            images.append(z_slice)\n",
    "        return torch.stack([torch.from_numpy(image) for image in images], dim=0).to(DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "    def load_mask(self, split, index):\n",
    "        img = Image.open(f\"{PATH}/{split}/{index}/mask.png\").convert('1')\n",
    "        img = self.resize(img)\n",
    "        return np.array(img)\n",
    "\n",
    "\n",
    "\n",
    "    def load_labels(self, split, index):\n",
    "        img = Image.open(f\"{PATH}/{split}/{index}/inklabels.png\")\n",
    "        img = self.resize(img)\n",
    "        return torch.from_numpy(np.array(img)).gt(0).float().to(DEVICE)\n",
    "\n",
    "\n",
    "    def resize(self, img):\n",
    "        current_width, current_height = img.size\n",
    "        aspect_ratio = current_width / current_height\n",
    "        new_width = int(self.shared_height * aspect_ratio)\n",
    "        new_size = (new_width, self.shared_height)\n",
    "        img = img.resize(new_size)\n",
    "        return img\n",
    "\n",
    "\n",
    "\n",
    "    def split_train_val(self,mask):\n",
    "        rect = self.validation_rect\n",
    "        not_border = np.zeros(mask.shape, dtype=bool)\n",
    "        not_border[self.buffer:mask.shape[0] - self.buffer, self.buffer:mask.shape[1] - self.buffer] = True\n",
    "        arr_mask = np.array(mask) * not_border\n",
    "        inside_rect = np.zeros(mask.shape, dtype=bool) * arr_mask\n",
    "        inside_rect[rect[1]:rect[1] + rect[3] + 1, rect[0]:rect[0] + rect[2] + 1] = True\n",
    "        outside_rect = np.ones(mask.shape, dtype=bool) * arr_mask\n",
    "        outside_rect[rect[1]:rect[1] + rect[3] + 1, rect[0]:rect[0] + rect[2] + 1] = False\n",
    "        pixels_inside_rect = np.argwhere(inside_rect)\n",
    "        pixels_outside_rect = np.argwhere(outside_rect)\n",
    "        return pixels_outside_rect, pixels_inside_rect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d271b86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiating Dataset with parameters\n",
    "\n",
    "# buffer =   -- x,y patchsize for training\n",
    "# z_start =  --  Offset of slices in the z direction\n",
    "# z_dim =    -- Number of slices in the z direction. Max value is (64 - z_start)\n",
    "# validation_rect =  -- rectangle removed for validation set\n",
    "# shared_height = -- Height to resize all scrolls\n",
    "# scroll_fragments = -- scrolls to be used \n",
    "\n",
    "dataset = Scrolls_Dataset(\n",
    "                buffer = 31,\n",
    "                 z_start = 27,\n",
    "                 z_dim = 8,\n",
    "                 validation_rect = (1100, 3500, 700, 950),\n",
    "                shared_height = 8000,\n",
    "                 downsampling =None,\n",
    "                 scroll_fragments = [1],#[1,2,3],\n",
    "                 stage = 'train',\n",
    "                 shuffle=True,\n",
    "                 batch_size=8,\n",
    "                 num_workers =4 ,\n",
    "                 on_gpu= False,\n",
    "                          \n",
    "                         )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18fd0459",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:02,  2.89it/s]\n"
     ]
    }
   ],
   "source": [
    "# prepeare data, by processng images and loading dataloader\n",
    "\n",
    "dataset.prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec28157",
   "metadata": {},
   "source": [
    "### Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "498d6e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subvolume shape: torch.Size([8, 1, 8, 63, 63])\n",
      "inklabel shape: torch.Size([8, 1])\n"
     ]
    }
   ],
   "source": [
    "dataloader = iter(dataset.train_dataloader())\n",
    "train_tensor = None\n",
    "train_label = None\n",
    "for i in range(1):\n",
    "    # Get image and label from train data -- change number for different ones\n",
    "    #print(next(dataloader))\n",
    "    subvolume, inklabel = next(dataloader)\n",
    "    print('subvolume shape:',subvolume.shape)\n",
    "    print('inklabel shape:',inklabel.shape)\n",
    "    train_tensor = subvolume\n",
    "    train_label = inklabel\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2bb043",
   "metadata": {},
   "source": [
    "### MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e55b1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gregory/anaconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/gregory/anaconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# three backbone options (also try U-NET and V-Net)\n",
    "r3d_18 = models.video.r3d_18(pretrained=False)\n",
    "\n",
    "#r2plus1d_18 = models.video.r2plus1d_18(pretrained=False)\n",
    "#mc3_18 = models.video.mc3_18(pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afa6d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1439db07",
   "metadata": {},
   "outputs": [],
   "source": [
    "r3d_18 = torchvision.models.video.r3d_18(pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cf657d",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = nn.Sequential(\n",
    "            r3d_18.stem,\n",
    "            r3d_18.layer1,\n",
    "            r3d_18.layer2,\n",
    "            r3d_18.layer3,\n",
    "            #r3d_18.layer4,\n",
    "        )\n",
    "bottleneck = nn.Conv3d(512, 128, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5a41b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7fd928",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1b93b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if train_tensor.shape[1] == 1:\n",
    "    train_tensor = train_tensor.repeat(1, 3, 1, 1, 1)\n",
    "output = backbone(train_tensor.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e429ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [8, 1, 10, H, W] goes to [8, RESNTETDIM of 512, 2, H/16, W/16]\n",
    "# [8, 1, 10, 65, 65] goes to [8, 512, 2, 5, 5]\n",
    "# [8, 1, 10, 63, 63] goes to [8, 512, 2, 5, 5]\n",
    "# [8, 1, 10, 61, 61]  goes to [8, 512, 2, 4, 4]\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bd06ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = bottleneck(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9756219c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd7f74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "out =out.flatten(start_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd9ca80",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e73d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = nn.Linear(6144, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a402fd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb943f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd58584",
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bc9de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label.squeeze(1).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfab6b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label.squeeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59062520",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef52422",
   "metadata": {},
   "outputs": [],
   "source": [
    "los = loss_fn(res, train_label.squeeze(1).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626fe1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "los"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af02de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of target with class indices\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input = torch.rand(3, 5)\n",
    "target = torch.empty(3, dtype = torch.long).random_(5)\n",
    "print(target.shape)\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "output = loss(input, target)\n",
    "\n",
    "print('input: ', input)\n",
    "print('target: ', target)\n",
    "print('Cross Entropy Loss: ', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddfc1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e29301",
   "metadata": {},
   "outputs": [],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ab3f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset.data_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205bd2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset.data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3af529d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Scrolls_Dataset' object has no attribute 'pixels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpixels\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Scrolls_Dataset' object has no attribute 'pixels'"
     ]
    }
   ],
   "source": [
    "dataset.pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b885ad94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f4288d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
