{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98e43a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting path:/home/gregorymar577/VESUVIUS_Challenge/jupyter notebooks\n",
      "Current path:/home/gregorymar577/VESUVIUS_Challenge\n"
     ]
    }
   ],
   "source": [
    "# Here we take care of paths.\n",
    "# Make sure root project directory is named 'VESUVIUS_Challenge' for this to work\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "print('Starting path:' + os.getcwd())\n",
    "if os.getcwd()[-18:] == 'VESUVIUS_Challenge':\n",
    "    pass\n",
    "else:\n",
    "    PATH = Path().resolve().parents[0]\n",
    "    os.chdir(PATH)\n",
    "\n",
    "# make sure you are in the root folder of the project\n",
    "print('Current path:' + os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73cc2ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-29 23:23:27,055 - Created a temporary directory at /tmp/tmpgv94ekud\n",
      "2023-05-29 23:23:27,057 - Writing /tmp/tmpgv94ekud/_remote_module_non_scriptable.py\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import monai\n",
    "import segmentation_models_pytorch as smp\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Tuple, List\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from Data_Modules.Vesuvius_Dataset import Vesuvius_Tile_Datamodule\n",
    "from lit_models.Vesuvius_Lit_Model import Lit_Model\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from lit_models.scratch_models import FPNDecoder\n",
    "import gc\n",
    "from Models.PreBackbone_3d_Zdim import PreBackbone_3D_ZDIM\n",
    "from Models.PreBackbone_3D import PreBackbone_3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2258478b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT = 'logs/24_bnorm_BMasked_Att3D_SmpU_mitb3_bce3_05tver70/last.ckpt'\n",
    "SAMPLE_SUBMISSION = 'kaggle/input/vesuvius-challenge-ink-detection/sample_submission.csv'\n",
    "\n",
    "\n",
    "PATCH_SIZE = 256\n",
    "Z_DIM = 24\n",
    "COMPETITION_DATA_DIR_str =  \"kaggle/input/vesuvius-challenge-ink-detection/\"\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# change to the line below if not using Apple's M1 or chips\n",
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f08d5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_3dAtt_w_Segformer(nn.Module):\n",
    "    def __init__(self ):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.model_3d =PreBackbone_3D_ZDIM(out_channels = 3,\n",
    "                                           z_dim= Z_DIM, \n",
    "                                           emdedding_dims=[8],\n",
    "                                           filter_sizes=[32, 64, 128]).to(DEVICE) \n",
    "        \n",
    "       \n",
    "        self.model_2d = smp.Unet(encoder_name='mit_b3',\n",
    "                                 encoder_depth=5,\n",
    "                                 encoder_weights='imagenet', \n",
    "                                 decoder_use_batchnorm=True, \n",
    "                                 decoder_channels=(512, 256, 128, 64, 32,),\n",
    "                                 decoder_attention_type=None,\n",
    "                                 in_channels=3,\n",
    "                                 classes=1, activation=None, aux_params=None).to(DEVICE) \n",
    "\n",
    "       \n",
    "\n",
    "    def forward(self, x):\n",
    "      \n",
    "        outs_3d = self.model_3d(x)\n",
    "        logits = self.model_2d(outs_3d)\n",
    "        \n",
    "       \n",
    "       \n",
    "            \n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a82c1b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CFG:\n",
    "    \n",
    "    device = DEVICE\n",
    "    \n",
    "    THRESHOLD = 0.5\n",
    "    use_wandb = False\n",
    "    \n",
    "    ######### Dataset #########\n",
    "    \n",
    "    # stage: 'train' or 'test'\n",
    "    stage = 'test' \n",
    "    \n",
    "    # location of competition Data\n",
    "    competition_data_dir = COMPETITION_DATA_DIR_str\n",
    "    \n",
    "    # Number of slices in z-dim: 1<z_dim<65\n",
    "    z_dim = Z_DIM\n",
    "    \n",
    "    # fragments to use for training avalaible [1,2,3]\n",
    "    train_fragment_id=[2,3]\n",
    "    \n",
    "    # fragments to use for validation\n",
    "    val_fragment_id=[1]\n",
    "    \n",
    "    test_fragment_ids = ['a','b']\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    batch_size = 4\n",
    "    \n",
    "    # Size of the patch and stride for feeding the model\n",
    "    patch_size = PATCH_SIZE\n",
    "    stride = patch_size // 2\n",
    "    \n",
    "    \n",
    "    num_workers = 0\n",
    "    on_gpu = True\n",
    "    \n",
    "    \n",
    "    ######## Model and Lightning Model paramters ############\n",
    "    \n",
    "    # MODEL\n",
    "    model = Model_3dAtt_w_Segformer().to(DEVICE) \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    checkpoint = None\n",
    "    save_directory = None\n",
    "    \n",
    "    \n",
    "    accumulate_grad_batches = 192// batch_size  # experiments showed batch_size * accumulate_grad = 192 is optimal\n",
    "    learning_rate = 0.0001\n",
    "    eta_min = 1e-7\n",
    "    t_max = 80\n",
    "    max_epochs = 120\n",
    "    weight_decay =  0.0001\n",
    "    precision =16\n",
    "    \n",
    "    # checkpointing\n",
    "    save_top_k=5\n",
    "    \n",
    "    monitor=\"FBETA\"\n",
    "    mode=\"max\"\n",
    "    \n",
    "    \n",
    "    ####### Augemtnations ###############\n",
    "    \n",
    "    # Training Aug\n",
    "    train_transforms = [\n",
    "        # A.RandomResizedCrop(\n",
    "        #     size, size, scale=(0.85, 1.0)),\n",
    "        A.Resize(patch_size, patch_size),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.75),\n",
    "        A.ShiftScaleRotate(p=0.75),\n",
    "        A.OneOf([\n",
    "                A.GaussNoise(var_limit=[10, 50]),\n",
    "                A.GaussianBlur(),\n",
    "                A.MotionBlur(),\n",
    "                ], p=0.4),\n",
    "        A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.5),\n",
    "        A.CoarseDropout(max_holes=1, max_width=int(patch_size * 0.3), max_height=int(patch_size * 0.3), \n",
    "                        mask_fill_value=0, p=0.5),\n",
    "        # A.Cutout(max_h_size=int(size * 0.6),\n",
    "        #          max_w_size=int(size * 0.6), num_holes=1, p=1.0),\n",
    "        A.Normalize(\n",
    "            mean= [0] * z_dim,\n",
    "            std= [1] * z_dim\n",
    "        ),\n",
    "        ToTensorV2(transpose_mask=True),\n",
    "    ]\n",
    "    \n",
    "\n",
    "    \n",
    "    # Validaiton Aug\n",
    "    val_transforms = [\n",
    "        A.Resize(patch_size, patch_size),\n",
    "        A.Normalize(\n",
    "            mean= [0] * z_dim,\n",
    "            std= [1] * z_dim\n",
    "        ),\n",
    "        ToTensorV2(transpose_mask=True),\n",
    "    ]\n",
    "    \n",
    "    # Test Aug\n",
    "    test_transforms = [\n",
    "        A.Resize(patch_size, patch_size),\n",
    "        A.Normalize(\n",
    "            mean=[0] * z_dim,\n",
    "            std=[1] * z_dim\n",
    "        ),\n",
    "\n",
    "        ToTensorV2(transpose_mask=True),\n",
    "    ]\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5ec1424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lit_Model(\n",
       "  (metrics): ModuleDict(\n",
       "    (train_metrics): MetricCollection(\n",
       "      (dice): Dice(),\n",
       "      prefix=train_\n",
       "    )\n",
       "    (val_metrics): MetricCollection(\n",
       "      (dice): Dice(),\n",
       "      prefix=val_\n",
       "    )\n",
       "  )\n",
       "  (model): Model_3dAtt_w_Segformer(\n",
       "    (model_3d): PreBackbone_3D_ZDIM(\n",
       "      (embed_layer): Embed(\n",
       "        (conv_3d): Conv3d(1, 8, kernel_size=(3, 1, 1), stride=(2, 1, 1), padding=(1, 0, 0))\n",
       "        (conv_3d_embed): Conv3d(8, 1, kernel_size=(1, 4, 4), stride=(1, 4, 4), padding=(0, 1, 1))\n",
       "        (norm): LayerNorm2d((8,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm_embed): LayerNorm2d((1,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (attention): EfficientMultiHeadAttention(\n",
       "        (reducer): Sequential(\n",
       "          (0): Conv2d(12, 12, kernel_size=(4, 4), stride=(4, 4))\n",
       "          (1): LayerNorm_att((12,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (att): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (pool): AvgPool3d(kernel_size=(2, 1, 1), stride=(2, 1, 1), padding=0)\n",
       "      (global_pool): AdaptiveAvgPool3d(output_size=(1, None, None))\n",
       "      (global_pool_final): AdaptiveAvgPool3d(output_size=(3, None, None))\n",
       "      (batch_norm): BatchNorm3d(1, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_relu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (conv1): Conv3d(9, 32, kernel_size=(3, 3, 3), stride=(2, 1, 1), padding=(1, 1, 1))\n",
       "      (batch_norm1): BatchNorm3d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv3d(16, 64, kernel_size=(3, 3, 3), stride=(2, 1, 1), padding=(1, 1, 1))\n",
       "      (batch_norm2): BatchNorm3d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv3d(32, 128, kernel_size=(3, 3, 3), stride=(2, 1, 1), padding=(1, 1, 1))\n",
       "      (batch_norm3): BatchNorm3d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv4): Conv3d(1, 1, kernel_size=(3, 3, 3), stride=(2, 1, 1), padding=(1, 1, 1))\n",
       "    )\n",
       "    (model_2d): Unet(\n",
       "      (encoder): MixVisionTransformerEncoder(\n",
       "        (patch_embed1): OverlapPatchEmbed(\n",
       "          (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n",
       "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (patch_embed2): OverlapPatchEmbed(\n",
       "          (proj): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (patch_embed3): OverlapPatchEmbed(\n",
       "          (proj): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (patch_embed4): OverlapPatchEmbed(\n",
       "          (proj): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (block1): ModuleList(\n",
       "          (0): Block(\n",
       "            (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (q): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (kv): Linear(in_features=64, out_features=128, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))\n",
       "              (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "              (dwconv): DWConv(\n",
       "                (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Block(\n",
       "            (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (q): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (kv): Linear(in_features=64, out_features=128, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))\n",
       "              (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.004)\n",
       "            (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "              (dwconv): DWConv(\n",
       "                (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): Block(\n",
       "            (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (q): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (kv): Linear(in_features=64, out_features=128, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))\n",
       "              (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.007)\n",
       "            (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "              (dwconv): DWConv(\n",
       "                (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "        (block2): ModuleList(\n",
       "          (0): Block(\n",
       "            (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (q): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (kv): Linear(in_features=128, out_features=256, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "              (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.011)\n",
       "            (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (dwconv): DWConv(\n",
       "                (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Block(\n",
       "            (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (q): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (kv): Linear(in_features=128, out_features=256, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "              (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.015)\n",
       "            (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (dwconv): DWConv(\n",
       "                (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): Block(\n",
       "            (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (q): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (kv): Linear(in_features=128, out_features=256, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "              (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.019)\n",
       "            (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (dwconv): DWConv(\n",
       "                (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): Block(\n",
       "            (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (q): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (kv): Linear(in_features=128, out_features=256, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "              (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.022)\n",
       "            (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (dwconv): DWConv(\n",
       "                (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "        (block3): ModuleList(\n",
       "          (0): Block(\n",
       "            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "              (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.026)\n",
       "            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "              (dwconv): DWConv(\n",
       "                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Block(\n",
       "            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "              (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.030)\n",
       "            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "              (dwconv): DWConv(\n",
       "                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): Block(\n",
       "            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "              (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.033)\n",
       "            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "              (dwconv): DWConv(\n",
       "                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): Block(\n",
       "            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "              (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.037)\n",
       "            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "              (dwconv): DWConv(\n",
       "                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): Block(\n",
       "            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "              (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.041)\n",
       "            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "              (dwconv): DWConv(\n",
       "                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): Block(\n",
       "            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "              (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.044)\n",
       "            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "              (dwconv): DWConv(\n",
       "                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): Block(\n",
       "            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "              (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.048)\n",
       "            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "              (dwconv): DWConv(\n",
       "                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): Block(\n",
       "            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "              (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.052)\n",
       "            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "              (dwconv): DWConv(\n",
       "                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): Block(\n",
       "            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "              (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.056)\n",
       "            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "              (dwconv): DWConv(\n",
       "                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): Block(\n",
       "            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "              (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.059)\n",
       "            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "              (dwconv): DWConv(\n",
       "                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): Block(\n",
       "            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "              (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.063)\n",
       "            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "              (dwconv): DWConv(\n",
       "                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): Block(\n",
       "            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "              (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.067)\n",
       "            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "              (dwconv): DWConv(\n",
       "                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (12): Block(\n",
       "            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "              (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.070)\n",
       "            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "              (dwconv): DWConv(\n",
       "                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (13): Block(\n",
       "            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "              (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.074)\n",
       "            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "              (dwconv): DWConv(\n",
       "                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (14): Block(\n",
       "            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "              (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.078)\n",
       "            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "              (dwconv): DWConv(\n",
       "                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (15): Block(\n",
       "            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "              (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.081)\n",
       "            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "              (dwconv): DWConv(\n",
       "                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (16): Block(\n",
       "            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "              (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.085)\n",
       "            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "              (dwconv): DWConv(\n",
       "                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (17): Block(\n",
       "            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "              (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.089)\n",
       "            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "              (dwconv): DWConv(\n",
       "                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (norm3): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "        (block4): ModuleList(\n",
       "          (0): Block(\n",
       "            (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (kv): Linear(in_features=512, out_features=1024, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.093)\n",
       "            (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (dwconv): DWConv(\n",
       "                (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Block(\n",
       "            (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (kv): Linear(in_features=512, out_features=1024, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.096)\n",
       "            (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (dwconv): DWConv(\n",
       "                (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): Block(\n",
       "            (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (kv): Linear(in_features=512, out_features=1024, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.100)\n",
       "            (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (dwconv): DWConv(\n",
       "                (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (norm4): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): UnetDecoder(\n",
       "        (center): Identity()\n",
       "        (blocks): ModuleList(\n",
       "          (0): DecoderBlock(\n",
       "            (conv1): Conv2dReLU(\n",
       "              (0): Conv2d(832, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (attention1): Attention(\n",
       "              (attention): Identity()\n",
       "            )\n",
       "            (conv2): Conv2dReLU(\n",
       "              (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (attention2): Attention(\n",
       "              (attention): Identity()\n",
       "            )\n",
       "          )\n",
       "          (1): DecoderBlock(\n",
       "            (conv1): Conv2dReLU(\n",
       "              (0): Conv2d(640, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (attention1): Attention(\n",
       "              (attention): Identity()\n",
       "            )\n",
       "            (conv2): Conv2dReLU(\n",
       "              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (attention2): Attention(\n",
       "              (attention): Identity()\n",
       "            )\n",
       "          )\n",
       "          (2): DecoderBlock(\n",
       "            (conv1): Conv2dReLU(\n",
       "              (0): Conv2d(320, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (attention1): Attention(\n",
       "              (attention): Identity()\n",
       "            )\n",
       "            (conv2): Conv2dReLU(\n",
       "              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (attention2): Attention(\n",
       "              (attention): Identity()\n",
       "            )\n",
       "          )\n",
       "          (3): DecoderBlock(\n",
       "            (conv1): Conv2dReLU(\n",
       "              (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (attention1): Attention(\n",
       "              (attention): Identity()\n",
       "            )\n",
       "            (conv2): Conv2dReLU(\n",
       "              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (attention2): Attention(\n",
       "              (attention): Identity()\n",
       "            )\n",
       "          )\n",
       "          (4): DecoderBlock(\n",
       "            (conv1): Conv2dReLU(\n",
       "              (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (attention1): Attention(\n",
       "              (attention): Identity()\n",
       "            )\n",
       "            (conv2): Conv2dReLU(\n",
       "              (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (attention2): Attention(\n",
       "              (attention): Identity()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (segmentation_head): SegmentationHead(\n",
       "        (0): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): Identity()\n",
       "        (2): Activation(\n",
       "          (activation): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (loss_tversky): TverskyLoss()\n",
       "  (loss_bce): SoftBCEWithLogitsLoss()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lit_model = Lit_Model(cfg=CFG)\n",
    "\n",
    "lit_model = lit_model.load_from_checkpoint(CHECKPOINT, cfg=CFG,).to(CFG.device)\n",
    "\n",
    "lit_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1b7a762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77a15d7ea1ca477bae2cb4a8a1797233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "506ec3a91fc74012bb3e8f6c4c171c9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/504 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask_count_min: 0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWIAAAGiCAYAAADdtMzOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAq8ElEQVR4nO3df1RU54E+8GfCwARYuCvgzDgrWnJ2jsGCqcHsMCRbaEE0zYTktCfaYGfNieuPVSGz6mrY7G5pzmaIdqvZlq1V16OJmqV/NDTZrp2A28qG5WewsxWCJj1hI0QGSDrcgYSdQXy/f+SbuzuMsQxa38E8n3PuH3Pvc+/c9+p5fHN9gzohhAAREUlzh+wbICL6vGMRExFJxiImIpKMRUxEJBmLmIhIMhYxEZFkLGIiIslYxEREkrGIiYgkYxETEUkW80X8wx/+EFlZWbjzzjuRl5eHN954Q/YtERHdVDFdxD/+8Y/hcrnwzDPP4Fe/+hX+9E//FA8++CAuXbok+9aIiG4aXSz/0B+bzYZ7770XBw8e1PZlZ2fj0UcfRU1NjcQ7IyK6efSyb+CzhEIhdHV14emnnw7bX1paipaWloh8MBhEMBjUPl+9ehW//e1vkZ6eDp1O93u/XyIiIQTGxsZgsVhwxx0zf+EQs0X8wQcfYGpqCiaTKWy/yWSCz+eLyNfU1OA73/nOrbo9IqLP1N/fj4ULF844H7NF/Knps1khxDVnuFVVVdixY4f2WVVVLFq0CA/ga9Aj/vd+n0REVzCJZpxGSkpKVOfFbBFnZGQgLi4uYvY7PDwcMUsGAIPBAIPBELFfj3jodSxiIroF/v/fuEX7OjRmV00kJCQgLy8PjY2NYfsbGxtRUFAg6a6IiG6+mJ0RA8COHTvgdDqxYsUK2O12HD58GJcuXcKWLVtk3xoR0U0T00W8du1afPjhh3j22WcxODiInJwcnD59GosXL5Z9a0REN01MryO+EYFAAIqioAiP8B0xEd0SV8QkzuJVqKqK1NTUGZ8Xs++IiYg+L1jERESSsYiJiCRjERMRScYiJiKSjEVMRCQZi5iISDIWMRGRZCxiIiLJWMRERJKxiImIJGMRExFJxiImIpKMRUxEJBmLmIhIMhYxEZFkLGIiIslYxEREkrGIiYgkYxETEUnGIiYikoxFTEQkGYuYiEgyFjERkWQsYiIiyVjERESSsYiJiCRjERMRScYiJiKSjEVMRCQZi5iISDIWMRGRZCxiIiLJWMRERJJFXcT/8R//gYcffhgWiwU6nQ4//elPw44LIVBdXQ2LxYLExEQUFRWhp6cnLBMMBlFRUYGMjAwkJyejrKwMAwMDYRm/3w+n0wlFUaAoCpxOJ0ZHR6MeIBFRrIu6iD/66CPcc889qK2tvebxffv2Yf/+/aitrUVnZyfMZjNWrlyJsbExLeNyuVBfX4+6ujo0NzdjfHwcDocDU1NTWqa8vBxerxcejwcejwderxdOp3MWQyQiim06IYSY9ck6Herr6/Hoo48C+GQ2bLFY4HK5sGfPHgCfzH5NJhP27t2LzZs3Q1VVzJ8/HydOnMDatWsBAJcvX0ZmZiZOnz6NVatWobe3F0uXLkVbWxtsNhsAoK2tDXa7HRcuXMCSJUt+570FAgEoioIiPAK9Ln62QyQimrErYhJn8SpUVUVqauqMz7up74j7+vrg8/lQWlqq7TMYDCgsLERLSwsAoKurC5OTk2EZi8WCnJwcLdPa2gpFUbQSBoD8/HwoiqJlpgsGgwgEAmEbEdFccFOL2OfzAQBMJlPYfpPJpB3z+XxISEjAvHnzrpsxGo0R1zcajVpmupqaGu19sqIoyMzMvOHxEBHdCr+XVRM6nS7ssxAiYt900zPXyl/vOlVVVVBVVdv6+/tncedERLfeTS1is9kMABGz1uHhYW2WbDabEQqF4Pf7r5sZGhqKuP7IyEjEbPtTBoMBqampYRsR0VxwU4s4KysLZrMZjY2N2r5QKISmpiYUFBQAAPLy8hAfHx+WGRwcRHd3t5ax2+1QVRUdHR1apr29HaqqahkiotuFPtoTxsfH8Zvf/Eb73NfXB6/Xi7S0NCxatAgulwtutxtWqxVWqxVutxtJSUkoLy8HACiKgg0bNmDnzp1IT09HWloadu3ahdzcXJSUlAAAsrOzsXr1amzcuBGHDh0CAGzatAkOh2NGKyaIiOaSqIv4zTffxFe+8hXt844dOwAA69evx/Hjx7F7925MTExg69at8Pv9sNlsaGhoQEpKinbOgQMHoNfrsWbNGkxMTKC4uBjHjx9HXFycljl16hQqKyu11RVlZWWfuXaZiGguu6F1xLGM64iJ6FaLiXXEREQUPRYxEZFkLGIiIslYxEREkrGIiYgkYxETEUnGIiYikoxFTEQkGYuYiEgyFjERkWQsYiIiyVjERESSsYiJiCRjERMRScYiJiKSjEVMRCQZi5iISDIWMRGRZCxiIiLJWMRERJKxiImIJGMRExFJxiImIpKMRUxEJBmLmIhIMhYxEZFkLGIiIslYxEREkrGIiYgkYxETEUnGIiYikoxFTEQkGYuYiEgyFjERkWRRFXFNTQ3uu+8+pKSkwGg04tFHH8XFixfDMkIIVFdXw2KxIDExEUVFRejp6QnLBINBVFRUICMjA8nJySgrK8PAwEBYxu/3w+l0QlEUKIoCp9OJ0dHR2Y2SiCiGRVXETU1N2LZtG9ra2tDY2IgrV66gtLQUH330kZbZt28f9u/fj9raWnR2dsJsNmPlypUYGxvTMi6XC/X19airq0NzczPGx8fhcDgwNTWlZcrLy+H1euHxeODxeOD1euF0Om/CkImIYotOCCFme/LIyAiMRiOamprw5S9/GUIIWCwWuFwu7NmzB8Ans1+TyYS9e/di8+bNUFUV8+fPx4kTJ7B27VoAwOXLl5GZmYnTp09j1apV6O3txdKlS9HW1gabzQYAaGtrg91ux4ULF7BkyZLfeW+BQACKoqAIj0Cvi5/tEImIZuyKmMRZvApVVZGamjrj827oHbGqqgCAtLQ0AEBfXx98Ph9KS0u1jMFgQGFhIVpaWgAAXV1dmJycDMtYLBbk5ORomdbWViiKopUwAOTn50NRFC0zXTAYRCAQCNuIiOaCWRexEAI7duzAAw88gJycHACAz+cDAJhMprCsyWTSjvl8PiQkJGDevHnXzRiNxojvNBqNWma6mpoa7X2yoijIzMyc7dCIiG6pWRfx9u3b8etf/xr/8i//EnFMp9OFfRZCROybbnrmWvnrXaeqqgqqqmpbf3//TIZBRCTdrIq4oqICr732Gn75y19i4cKF2n6z2QwAEbPW4eFhbZZsNpsRCoXg9/uvmxkaGor43pGRkYjZ9qcMBgNSU1PDNiKiuSCqIhZCYPv27XjllVfwi1/8AllZWWHHs7KyYDab0djYqO0LhUJoampCQUEBACAvLw/x8fFhmcHBQXR3d2sZu90OVVXR0dGhZdrb26GqqpYhIrpd6KMJb9u2DS+//DJeffVVpKSkaDNfRVGQmJgInU4Hl8sFt9sNq9UKq9UKt9uNpKQklJeXa9kNGzZg586dSE9PR1paGnbt2oXc3FyUlJQAALKzs7F69Wps3LgRhw4dAgBs2rQJDodjRismiIjmkqiK+ODBgwCAoqKisP3Hjh3DE088AQDYvXs3JiYmsHXrVvj9fthsNjQ0NCAlJUXLHzhwAHq9HmvWrMHExASKi4tx/PhxxMXFaZlTp06hsrJSW11RVlaG2tra2YyRiCim3dA64ljGdcREdKtJWUdMREQ3jkVMRCQZi5iISDIWMRGRZCxiIiLJWMRERJKxiImIJGMRExFJxiImIpKMRUxEJBmLmIhIMhYxEZFkLGIiIslYxEREkrGIiYgkYxETEUnGIiYikoxFTEQkGYuYiEgyFjERkWQsYiIiyVjERESSsYiJiCRjERMRScYiJiKSjEVMRCQZi5iISDIWMRGRZCxiIiLJWMRERJKxiImIJGMRExFJxiImIpKMRUxEJBmLmIhIsqiK+ODBg1i2bBlSU1ORmpoKu92On//859pxIQSqq6thsViQmJiIoqIi9PT0hF0jGAyioqICGRkZSE5ORllZGQYGBsIyfr8fTqcTiqJAURQ4nU6Mjo7OfpRERDEsqiJeuHAhnn/+ebz55pt488038dWvfhWPPPKIVrb79u3D/v37UVtbi87OTpjNZqxcuRJjY2PaNVwuF+rr61FXV4fm5maMj4/D4XBgampKy5SXl8Pr9cLj8cDj8cDr9cLpdN6kIRMRxRadEELcyAXS0tLw3e9+F08++SQsFgtcLhf27NkD4JPZr8lkwt69e7F582aoqor58+fjxIkTWLt2LQDg8uXLyMzMxOnTp7Fq1Sr09vZi6dKlaGtrg81mAwC0tbXBbrfjwoULWLJkyTXvIxgMIhgMap8DgQAyMzNRhEeg18XfyBCJiGbkipjEWbwKVVWRmpo64/Nm/Y54amoKdXV1+Oijj2C329HX1wefz4fS0lItYzAYUFhYiJaWFgBAV1cXJicnwzIWiwU5OTlaprW1FYqiaCUMAPn5+VAURctcS01NjfYqQ1EUZGZmznZoRES3VNRFfP78efzBH/wBDAYDtmzZgvr6eixduhQ+nw8AYDKZwvImk0k75vP5kJCQgHnz5l03YzQaI77XaDRqmWupqqqCqqra1t/fH+3QiIik0Ed7wpIlS+D1ejE6Ooqf/OQnWL9+PZqamrTjOp0uLC+EiNg33fTMtfK/6zoGgwEGg2GmwyAiihlRz4gTEhLwx3/8x1ixYgVqampwzz334B//8R9hNpsBIGLWOjw8rM2SzWYzQqEQ/H7/dTNDQ0MR3zsyMhIx2yYiuh3c8DpiIQSCwSCysrJgNpvR2NioHQuFQmhqakJBQQEAIC8vD/Hx8WGZwcFBdHd3axm73Q5VVdHR0aFl2tvboaqqliEiup1E9Wrir//6r/Hggw8iMzMTY2NjqKurw9mzZ+HxeKDT6eByueB2u2G1WmG1WuF2u5GUlITy8nIAgKIo2LBhA3bu3In09HSkpaVh165dyM3NRUlJCQAgOzsbq1evxsaNG3Ho0CEAwKZNm+BwOD5zxQQR0VwWVREPDQ3B6XRicHAQiqJg2bJl8Hg8WLlyJQBg9+7dmJiYwNatW+H3+2Gz2dDQ0ICUlBTtGgcOHIBer8eaNWswMTGB4uJiHD9+HHFxcVrm1KlTqKys1FZXlJWVoba29maMl4go5tzwOuJYFQgEoCgK1xET0S1zy9cRExHRzcEiJiKSjEVMRCQZi5iISDIWMRGRZCxiIiLJWMRERJKxiImIJGMRExFJxiImIpKMRUxEJBmLmIhIMhYxEZFkLGIiIslYxEREkrGIiYgkYxETEUnGIiYikoxFTEQkGYuYiEgyFjERkWQsYiIiyVjERESSsYiJiCRjERMRScYiJiKSjEVMRCQZi5iISDIWMRGRZCxiIiLJWMRERJKxiImIJGMRExFJxiImIpLshoq4pqYGOp0OLpdL2yeEQHV1NSwWCxITE1FUVISenp6w84LBICoqKpCRkYHk5GSUlZVhYGAgLOP3++F0OqEoChRFgdPpxOjo6I3cLhFRTJp1EXd2duLw4cNYtmxZ2P59+/Zh//79qK2tRWdnJ8xmM1auXImxsTEt43K5UF9fj7q6OjQ3N2N8fBwOhwNTU1Napry8HF6vFx6PBx6PB16vF06nc7a3S0QUs2ZVxOPj41i3bh2OHDmCefPmafuFEHjhhRfwzDPP4Otf/zpycnLw4osv4uOPP8bLL78MAFBVFUePHsX3vvc9lJSUYPny5Th58iTOnz+PM2fOAAB6e3vh8Xjwz//8z7Db7bDb7Thy5Ah+9rOf4eLFi9e8p2AwiEAgELYREc0Fsyribdu24aGHHkJJSUnY/r6+Pvh8PpSWlmr7DAYDCgsL0dLSAgDo6urC5ORkWMZisSAnJ0fLtLa2QlEU2Gw2LZOfnw9FUbTMdDU1NdprDEVRkJmZOZuhERHdclEXcV1dHc6dO4eampqIYz6fDwBgMpnC9ptMJu2Yz+dDQkJC2Ez6Whmj0RhxfaPRqGWmq6qqgqqq2tbf3x/t0IiIpNBHE+7v78dTTz2FhoYG3HnnnZ+Z0+l0YZ+FEBH7ppueuVb+etcxGAwwGAzX/Q4iolgU1Yy4q6sLw8PDyMvLg16vh16vR1NTE77//e9Dr9drM+Hps9bh4WHtmNlsRigUgt/vv25maGgo4vtHRkYiZttERHNdVEVcXFyM8+fPw+v1atuKFSuwbt06eL1e3HXXXTCbzWhsbNTOCYVCaGpqQkFBAQAgLy8P8fHxYZnBwUF0d3drGbvdDlVV0dHRoWXa29uhqqqWISK6XUT1aiIlJQU5OTlh+5KTk5Genq7td7lccLvdsFqtsFqtcLvdSEpKQnl5OQBAURRs2LABO3fuRHp6OtLS0rBr1y7k5uZqf/mXnZ2N1atXY+PGjTh06BAAYNOmTXA4HFiyZMkND5qIKJZEVcQzsXv3bkxMTGDr1q3w+/2w2WxoaGhASkqKljlw4AD0ej3WrFmDiYkJFBcX4/jx44iLi9Myp06dQmVlpba6oqysDLW1tTf7domIpNMJIYTsm/h9CAQCUBQFRXgEel287Nshos+BK2ISZ/EqVFVFamrqjM/jz5ogIpKMRUxEJBmLmIhIMhYxEZFkLGIiIslYxEREkrGIiYgkYxETEUnGIiYikoxFTEQkGYuYiEgyFjERkWQsYiIiyVjERESSsYiJiCRjERMRScYiJiKSjEVMRCQZi5iISDIWMRGRZCxiIiLJWMRERJKxiImIJGMRExFJxiImIpKMRUxEJBmLmIhIMhYxEZFkLGIiIslYxEREkrGIiYgkYxETEUnGIiYikoxFTEQkWVRFXF1dDZ1OF7aZzWbtuBAC1dXVsFgsSExMRFFREXp6esKuEQwGUVFRgYyMDCQnJ6OsrAwDAwNhGb/fD6fTCUVRoCgKnE4nRkdHZz9KIqIYFvWM+Itf/CIGBwe17fz589qxffv2Yf/+/aitrUVnZyfMZjNWrlyJsbExLeNyuVBfX4+6ujo0NzdjfHwcDocDU1NTWqa8vBxerxcejwcejwderxdOp/MGh0pEFJv0UZ+g14fNgj8lhMALL7yAZ555Bl//+tcBAC+++CJMJhNefvllbN68Gaqq4ujRozhx4gRKSkoAACdPnkRmZibOnDmDVatWobe3Fx6PB21tbbDZbACAI0eOwG634+LFi1iyZMmNjJeIKOZEPSN+5513YLFYkJWVhW9+85t49913AQB9fX3w+XwoLS3VsgaDAYWFhWhpaQEAdHV1YXJyMixjsViQk5OjZVpbW6EoilbCAJCfnw9FUbTMtQSDQQQCgbCNiGguiKqIbTYbXnrpJbz++us4cuQIfD4fCgoK8OGHH8Ln8wEATCZT2Dkmk0k75vP5kJCQgHnz5l03YzQaI77baDRqmWupqanR3ikrioLMzMxohkZEJE1URfzggw/iG9/4BnJzc1FSUoJ/+7d/A/DJK4hP6XS6sHOEEBH7ppueuVb+d12nqqoKqqpqW39//4zGREQk2w0tX0tOTkZubi7eeecd7b3x9Fnr8PCwNks2m80IhULw+/3XzQwNDUV818jISMRs+/8yGAxITU0N24iI5oIbKuJgMIje3l4sWLAAWVlZMJvNaGxs1I6HQiE0NTWhoKAAAJCXl4f4+PiwzODgILq7u7WM3W6Hqqro6OjQMu3t7VBVVcsQEd1Oolo1sWvXLjz88MNYtGgRhoeH8fd///cIBAJYv349dDodXC4X3G43rFYrrFYr3G43kpKSUF5eDgBQFAUbNmzAzp07kZ6ejrS0NOzatUt71QEA2dnZWL16NTZu3IhDhw4BADZt2gSHw8EVE0R0W4qqiAcGBvD444/jgw8+wPz585Gfn4+2tjYsXrwYALB7925MTExg69at8Pv9sNlsaGhoQEpKinaNAwcOQK/XY82aNZiYmEBxcTGOHz+OuLg4LXPq1ClUVlZqqyvKyspQW1t7M8ZLRBRzdEIIIfsmfh8CgQAURUERHoFeFy/7dojoc+CKmMRZvApVVaP6eyr+rAkiIslYxEREkrGIiYgkYxETEUnGIiYikoxFTEQkGYuYiEgyFjERkWQsYiIiyVjERESSsYiJiCRjERMRScYiJiKSjEVMRCQZi5iISDIWMRGRZCxiIiLJWMRERJKxiImIJGMRExFJxiImIpKMRUxEJBmLmIhIMhYxEZFkLGIiIslYxEREkrGIiYgkYxETEUnGIiYikoxFTEQkGYuYiEgyFjERkWQsYiIiyVjERESSRV3E77//Pr71rW8hPT0dSUlJ+NKXvoSuri7tuBAC1dXVsFgsSExMRFFREXp6esKuEQwGUVFRgYyMDCQnJ6OsrAwDAwNhGb/fD6fTCUVRoCgKnE4nRkdHZzdKIqIYFlUR+/1+3H///YiPj8fPf/5zvPXWW/je976HP/zDP9Qy+/btw/79+1FbW4vOzk6YzWasXLkSY2NjWsblcqG+vh51dXVobm7G+Pg4HA4HpqamtEx5eTm8Xi88Hg88Hg+8Xi+cTueNj5iIKMbohBBipuGnn34a//mf/4k33njjmseFELBYLHC5XNizZw+AT2a/JpMJe/fuxebNm6GqKubPn48TJ05g7dq1AIDLly8jMzMTp0+fxqpVq9Db24ulS5eira0NNpsNANDW1ga73Y4LFy5gyZIlv/NeA4EAFEVBER6BXhc/0yESEc3aFTGJs3gVqqoiNTV1xudFNSN+7bXXsGLFCjz22GMwGo1Yvnw5jhw5oh3v6+uDz+dDaWmpts9gMKCwsBAtLS0AgK6uLkxOToZlLBYLcnJytExraysURdFKGADy8/OhKIqWmS4YDCIQCIRtRERzQVRF/O677+LgwYOwWq14/fXXsWXLFlRWVuKll14CAPh8PgCAyWQKO89kMmnHfD4fEhISMG/evOtmjEZjxPcbjUYtM11NTY32PllRFGRmZkYzNCIiaaIq4qtXr+Lee++F2+3G8uXLsXnzZmzcuBEHDx4My+l0urDPQoiIfdNNz1wrf73rVFVVQVVVbevv75/psIiIpIqqiBcsWIClS5eG7cvOzsalS5cAAGazGQAiZq3Dw8PaLNlsNiMUCsHv9183MzQ0FPH9IyMjEbPtTxkMBqSmpoZtRERzQVRFfP/99+PixYth+95++20sXrwYAJCVlQWz2YzGxkbteCgUQlNTEwoKCgAAeXl5iI+PD8sMDg6iu7tby9jtdqiqio6ODi3T3t4OVVW1DBHR7UIfTfgv//IvUVBQALfbjTVr1qCjowOHDx/G4cOHAXzyOsHlcsHtdsNqtcJqtcLtdiMpKQnl5eUAAEVRsGHDBuzcuRPp6elIS0vDrl27kJubi5KSEgCfzLJXr16NjRs34tChQwCATZs2weFwzGjFBBHRXBJVEd93332or69HVVUVnn32WWRlZeGFF17AunXrtMzu3bsxMTGBrVu3wu/3w2azoaGhASkpKVrmwIED0Ov1WLNmDSYmJlBcXIzjx48jLi5Oy5w6dQqVlZXa6oqysjLU1tbe6HiJiGJOVOuI5xKuIyaiW+2WrCMmIqKbj0VMRCQZi5iISDIWMRGRZCxiIiLJWMRERJKxiImIJGMRExFJxiImIpKMRUxEJBmLmIhIMhYxEZFkLGIiIslYxEREkrGIiYgkYxETEUnGIiYikoxFTEQkGYuYiEgyFjERkWQsYiIiyVjERESSsYiJiCRjERMRScYiJiKSjEVMRCQZi5iISDIWMRGRZCxiIiLJWMRERJKxiImIJGMRExFJxiImIpKMRUxEJFlURfyFL3wBOp0uYtu2bRsAQAiB6upqWCwWJCYmoqioCD09PWHXCAaDqKioQEZGBpKTk1FWVoaBgYGwjN/vh9PphKIoUBQFTqcTo6OjNzZSIqIYFVURd3Z2YnBwUNsaGxsBAI899hgAYN++fdi/fz9qa2vR2dkJs9mMlStXYmxsTLuGy+VCfX096urq0NzcjPHxcTgcDkxNTWmZ8vJyeL1eeDweeDweeL1eOJ3OmzFeIqKYoxNCiNme7HK58LOf/QzvvPMOAMBiscDlcmHPnj0APpn9mkwm7N27F5s3b4aqqpg/fz5OnDiBtWvXAgAuX76MzMxMnD59GqtWrUJvby+WLl2KtrY22Gw2AEBbWxvsdjsuXLiAJUuWzOjeAoEAFEVBER6BXhc/2yESEc3YFTGJs3gVqqoiNTV1xufN+h1xKBTCyZMn8eSTT0Kn06Gvrw8+nw+lpaVaxmAwoLCwEC0tLQCArq4uTE5OhmUsFgtycnK0TGtrKxRF0UoYAPLz86Eoipa5lmAwiEAgELYREc0Fsy7in/70pxgdHcUTTzwBAPD5fAAAk8kUljOZTNoxn8+HhIQEzJs377oZo9EY8X1Go1HLXEtNTY32TllRFGRmZs52aEREt9Ssi/jo0aN48MEHYbFYwvbrdLqwz0KIiH3TTc9cK/+7rlNVVQVVVbWtv79/JsMgIpJuVkX83nvv4cyZM/jzP/9zbZ/ZbAaAiFnr8PCwNks2m80IhULw+/3XzQwNDUV858jISMRs+/8yGAxITU0N24iI5oJZFfGxY8dgNBrx0EMPafuysrJgNpu1lRTAJ++Rm5qaUFBQAADIy8tDfHx8WGZwcBDd3d1axm63Q1VVdHR0aJn29naoqqpliIhuJ/poT7h69SqOHTuG9evXQ6//39N1Oh1cLhfcbjesViusVivcbjeSkpJQXl4OAFAUBRs2bMDOnTuRnp6OtLQ07Nq1C7m5uSgpKQEAZGdnY/Xq1di4cSMOHToEANi0aRMcDseMV0wQEc0lURfxmTNncOnSJTz55JMRx3bv3o2JiQls3boVfr8fNpsNDQ0NSElJ0TIHDhyAXq/HmjVrMDExgeLiYhw/fhxxcXFa5tSpU6isrNRWV5SVlaG2tnY24yMiink3tI44lnEdMRHdard8HTEREd0cLGIiIslYxEREkrGIiYgkYxETEUnGIiYikoxFTEQkGYuYiEgyFjERkWQsYiIiyVjERESSsYiJiCRjERMRScYiJiKSjEVMRCQZi5iISDIWMRGRZCxiIiLJWMRERJKxiImIJGMRExFJxiImIpKMRUxEJBmLmIhIMhYxEZFkLGIiIslYxEREkrGIiYgkYxETEUnGIiYikoxFTEQkGYuYiEgyFjERkWQsYiIiyaIq4itXruBv/uZvkJWVhcTERNx111149tlncfXqVS0jhEB1dTUsFgsSExNRVFSEnp6esOsEg0FUVFQgIyMDycnJKCsrw8DAQFjG7/fD6XRCURQoigKn04nR0dHZj5SIKEZFVcR79+7Fj370I9TW1qK3txf79u3Dd7/7XfzgBz/QMvv27cP+/ftRW1uLzs5OmM1mrFy5EmNjY1rG5XKhvr4edXV1aG5uxvj4OBwOB6amprRMeXk5vF4vPB4PPB4PvF4vnE7nTRgyEVFs0QkhxEzDDocDJpMJR48e1fZ94xvfQFJSEk6cOAEhBCwWC1wuF/bs2QPgk9mvyWTC3r17sXnzZqiqivnz5+PEiRNYu3YtAODy5cvIzMzE6dOnsWrVKvT29mLp0qVoa2uDzWYDALS1tcFut+PChQtYsmTJ77zXQCAARVFQhEeg18VH9VCIiGbjipjEWbwKVVWRmpo64/OimhE/8MAD+Pd//3e8/fbbAID/+q//QnNzM772ta8BAPr6+uDz+VBaWqqdYzAYUFhYiJaWFgBAV1cXJicnwzIWiwU5OTlaprW1FYqiaCUMAPn5+VAURctMFwwGEQgEwjYiorlAH014z549UFUVd999N+Li4jA1NYXnnnsOjz/+OADA5/MBAEwmU9h5JpMJ7733npZJSEjAvHnzIjKfnu/z+WA0GiO+32g0apnpampq8J3vfCea4RARxYSoZsQ//vGPcfLkSbz88ss4d+4cXnzxRfzDP/wDXnzxxbCcTqcL+yyEiNg33fTMtfLXu05VVRVUVdW2/v7+mQ6LiEiqqGbEf/VXf4Wnn34a3/zmNwEAubm5eO+991BTU4P169fDbDYD+GRGu2DBAu284eFhbZZsNpsRCoXg9/vDZsXDw8MoKCjQMkNDQxHfPzIyEjHb/pTBYIDBYIhmOEREMSGqGfHHH3+MO+4IPyUuLk5bvpaVlQWz2YzGxkbteCgUQlNTk1ayeXl5iI+PD8sMDg6iu7tby9jtdqiqio6ODi3T3t4OVVW1DBHR7SKqGfHDDz+M5557DosWLcIXv/hF/OpXv8L+/fvx5JNPAvjkdYLL5YLb7YbVaoXVaoXb7UZSUhLKy8sBAIqiYMOGDdi5cyfS09ORlpaGXbt2ITc3FyUlJQCA7OxsrF69Ghs3bsShQ4cAAJs2bYLD4ZjRigkiorkkqiL+wQ9+gL/927/F1q1bMTw8DIvFgs2bN+Pv/u7vtMzu3bsxMTGBrVu3wu/3w2azoaGhASkpKVrmwIED0Ov1WLNmDSYmJlBcXIzjx48jLi5Oy5w6dQqVlZXa6oqysjLU1tbe6HiJiGJOVOuI5xKuIyaiW22264ijmhHPJZ/++XIFk8Bt+UcNEcWaK5gE8L/9M1O3bRF/+OGHAIBmnJZ8J0T0eTM2NgZFUWacv22LOC0tDQBw6dKlqB4IRS8QCCAzMxP9/f1R/ecYRYfP+da4kecshMDY2BgsFktU5922RfzpMjtFUfib9hZJTU3ls74F+Jxvjdk+59lM/PjziImIJGMRExFJdtsWscFgwLe//W3+b8+3AJ/1rcHnfGvIeM637TpiIqK54radERMRzRUsYiIiyVjERESSsYiJiCRjERMRSXbbFvEPf/hDZGVl4c4770ReXh7eeOMN2bcUs2pqanDfffchJSUFRqMRjz76KC5evBiWEUKguroaFosFiYmJKCoqQk9PT1gmGAyioqICGRkZSE5ORllZGQYGBsIyfr8fTqcTiqJAURQ4nU6Mjo7+vocYk2pqarSf4f0pPueb5/3338e3vvUtpKenIykpCV/60pfQ1dWlHY+pZy1uQ3V1dSI+Pl4cOXJEvPXWW+Kpp54SycnJ4r333pN9azFp1apV4tixY6K7u1t4vV7x0EMPiUWLFonx8XEt8/zzz4uUlBTxk5/8RJw/f16sXbtWLFiwQAQCAS2zZcsW8Ud/9EeisbFRnDt3TnzlK18R99xzj7hy5YqWWb16tcjJyREtLS2ipaVF5OTkCIfDcUvHGws6OjrEF77wBbFs2TLx1FNPafv5nG+O3/72t2Lx4sXiiSeeEO3t7aKvr0+cOXNG/OY3v9EysfSsb8si/pM/+ROxZcuWsH133323ePrppyXd0dwyPDwsAIimpiYhhBBXr14VZrNZPP/881rmf/7nf4SiKOJHP/qREEKI0dFRER8fL+rq6rTM+++/L+644w7h8XiEEEK89dZbAoBoa2vTMq2trQKAuHDhwq0YWkwYGxsTVqtVNDY2isLCQq2I+Zxvnj179ogHHnjgM4/H2rO+7V5NhEIhdHV1af+yx6dKS0vR0tIi6a7mFlVVAfzvT7Dr6+uDz+cLe6YGgwGFhYXaM+3q6sLk5GRYxmKxICcnR8u0trZCURTYbDYtk5+fD0VRPle/Ntu2bcNDDz2k/dNgn+Jzvnlee+01rFixAo899hiMRiOWL1+OI0eOaMdj7VnfdkX8wQcfYGpqKuJfezaZTPD5fJLuau4QQmDHjh144IEHkJOTAwDac7veM/X5fEhISAj7l7mvlTEajRHfaTQaPze/NnV1dTh37hxqamoijvE53zzvvvsuDh48CKvVitdffx1btmxBZWUlXnrpJQCx96xv2x+DqdPpwj4LISL2UaTt27fj17/+NZqbmyOOzeaZTs9cK/95+bXp7+/HU089hYaGBtx5552fmeNzvnFXr17FihUr4Ha7AQDLly9HT08PDh48iD/7sz/TcrHyrG+7GXFGRgbi4uIi/jQaHh6O+NOPwlVUVOC1117DL3/5SyxcuFDbbzabAeC6z9RsNiMUCsHv9183MzQ0FPG9IyMjn4tfm66uLgwPDyMvLw96vR56vR5NTU34/ve/D71erz0DPucbt2DBAixdujRsX3Z2Ni5dugQg9n5P33ZFnJCQgLy8PDQ2Nobtb2xsREFBgaS7im1CCGzfvh2vvPIKfvGLXyArKyvseFZWFsxmc9gzDYVCaGpq0p5pXl4e4uPjwzKDg4Po7u7WMna7HaqqoqOjQ8u0t7dDVdXPxa9NcXExzp8/D6/Xq20rVqzAunXr4PV6cdddd/E53yT3339/xBLMt99+G4sXLwYQg7+nZ/zXenPIp8vXjh49Kt566y3hcrlEcnKy+O///m/ZtxaT/uIv/kIoiiLOnj0rBgcHte3jjz/WMs8//7xQFEW88sor4vz58+Lxxx+/5lKfhQsXijNnzohz586Jr371q9dc6rNs2TLR2toqWltbRW5u7udqWdV0/3fVhBB8zjdLR0eH0Ov14rnnnhPvvPOOOHXqlEhKShInT57UMrH0rG/LIhZCiH/6p38SixcvFgkJCeLee+/VlmJRJHzy71xHbMeOHdMyV69eFd/+9reF2WwWBoNBfPnLXxbnz58Pu87ExITYvn27SEtLE4mJicLhcIhLly6FZT788EOxbt06kZKSIlJSUsS6deuE3++/BaOMTdOLmM/55vnXf/1XkZOTIwwGg7j77rvF4cOHw47H0rPmzyMmIpLstntHTEQ017CIiYgkYxETEUnGIiYikoxFTEQkGYuYiEgyFjERkWQsYiIiyVjERESSsYiJiCRjERMRSfb/AM6rjjIfwOhKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24701/1845676345.py:236: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  score = beta * beta / (1 + beta * beta) * 1 / recall + 1 / (1 + beta * beta) * 1 / precision\n",
      "/tmp/ipykernel_24701/1845676345.py:236: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  score = beta * beta / (1 + beta * beta) * 1 / recall + 1 / (1 + beta * beta) * 1 / precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bce=33.22940\n",
      "th   prec   recall   fpr   dice   score\n",
      "---------------------------------------\n",
      "0.1, 0.153, 1.000, 0.634,  0.266,  0.185\n",
      "0.2, 0.185, 1.000, 0.507,  0.312,  0.221\n",
      "0.3, 0.250, 0.086, 0.029,  0.128,  0.181\n",
      "0.4, 0.028, 0.000, 0.000,  0.000,  0.000\n",
      "0.5, 0.000, 0.000, 0.000,  0.000,  0.000\n",
      "0.6, 0.000, 0.000, 0.000,  0.000,  0.000\n",
      "0.7, 0.000, 0.000, 0.000,  0.000,  0.000\n",
      "0.8, 0.000, 0.000, 0.000,  0.000,  0.000\n",
      "0.9, 0.000, 0.000, 0.000,  0.000,  0.000\n"
     ]
    }
   ],
   "source": [
    "def TTA(x:torch.Tensor,model:nn.Module):\n",
    "    #x.shape=(batch,c,h,w)\n",
    "    if 1:\n",
    "        shape=x.shape\n",
    "        x=[x,*[torch.rot90(x,k=i,dims=(-2,-1)) for i in range(1,4)]]\n",
    "        x=torch.cat(x,dim=0)\n",
    "        x=model(x)\n",
    "        x=torch.sigmoid(x)\n",
    "        x=x.reshape(4,shape[0],*shape[2:])\n",
    "        x=[torch.rot90(x[i],k=-i,dims=(-2,-1)) for i in range(4)]\n",
    "        x=torch.stack(x,dim=0)\n",
    "        return x.mean(0)\n",
    "    else :\n",
    "        x=model(x)\n",
    "        x=torch.sigmoid(x)\n",
    "        return x\n",
    "def make_val_dataset(fragment_id):\n",
    "    test_images = read_image_val(fragment_id)\n",
    "    \n",
    "    x1_list = list(range(0, test_images.shape[1]-CFG.patch_size+1, CFG.stride))\n",
    "    y1_list = list(range(0, test_images.shape[0]-CFG.patch_size+1, CFG.stride))\n",
    "    \n",
    "    test_images_list = []\n",
    "    xyxys = []\n",
    "    for y1 in y1_list:\n",
    "        for x1 in x1_list:\n",
    "            y2 = y1 + CFG.patch_size\n",
    "            x2 = x1 + CFG.patch_size\n",
    "            if np.all(test_images[y1:y2, x1:x2]==0):\n",
    "                continue\n",
    "            test_images_list.append(test_images[y1:y2, x1:x2])\n",
    "            xyxys.append((x1, y1, x2, y2))\n",
    "    xyxys = np.stack(xyxys)\n",
    "            \n",
    "    test_dataset = CustomDataset(test_images_list, CFG, transform=get_transforms(data='valid', cfg=CFG))\n",
    "    \n",
    "    test_loader = DataLoader(test_dataset,\n",
    "                          batch_size=CFG.batch_size,\n",
    "                          shuffle=False,\n",
    "                          num_workers=CFG.num_workers, pin_memory=CFG.on_gpu, drop_last=False)\n",
    "    \n",
    "    return test_loader, xyxys\n",
    "\n",
    "\n",
    "def get_transforms(data, cfg):\n",
    "    return A.Compose(\n",
    "        [\n",
    "        A.Resize(CFG.patch_size, CFG.patch_size),\n",
    "        A.Normalize(\n",
    "            mean=[0] * CFG.z_dim,\n",
    "            std=[1] * CFG.z_dim\n",
    "        ),\n",
    "\n",
    "        ToTensorV2(transpose_mask=True),\n",
    "    ]\n",
    "    )\n",
    "\n",
    "def read_image_val(fragment_id):\n",
    "    images = []\n",
    "\n",
    "    # idxs = range(65)\n",
    "    mid = 65 // 2\n",
    "    start = mid - CFG.z_dim // 2\n",
    "    end = mid + CFG.z_dim // 2\n",
    "    idxs = range(start, end)\n",
    "\n",
    "    for i in tqdm(idxs):\n",
    "        \n",
    "        image = cv2.imread(COMPETITION_DATA_DIR_str + f\"train/{fragment_id}/surface_volume/{i:02}.tif\", 0)\n",
    "\n",
    "        pad0 = (CFG.patch_size - image.shape[0] % CFG.patch_size)\n",
    "        pad1 = (CFG.patch_size - image.shape[1] % CFG.patch_size)\n",
    "\n",
    "        image = np.pad(image, [(0, pad0), (0, pad1)], constant_values=0)\n",
    "\n",
    "        images.append(image)\n",
    "    images = np.stack(images, axis=2)\n",
    "    \n",
    "    return images\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, cfg, labels=None, transform=None):\n",
    "        self.images = images\n",
    "        self.cfg = cfg\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        # return len(self.xyxys)\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # x1, y1, x2, y2 = self.xyxys[idx]\n",
    "        image = self.images[idx]\n",
    "        data = self.transform(image=image)\n",
    "        image = data['image']\n",
    "        return image\n",
    "    \n",
    "    \n",
    "    \n",
    "def rle(img):\n",
    "    '''\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels = img.flatten()\n",
    "    # pixels = (pixels >= thr).astype(int)\n",
    "    \n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "\n",
    "THRESHOLD =0.6\n",
    "results = []\n",
    "saved_result_val = []\n",
    "for fragment_id in [1]:\n",
    "    \n",
    "    test_loader, xyxys = make_val_dataset(fragment_id)\n",
    "    \n",
    "    binary_mask = cv2.imread(COMPETITION_DATA_DIR_str + f\"train/{fragment_id}/mask.png\", 0)\n",
    "    binary_mask = (binary_mask / 255).astype(int)\n",
    "    binary_mask = torch.tensor(binary_mask)#.to(DEVICE)\n",
    "    \n",
    "    ori_h = binary_mask.shape[0]\n",
    "    ori_w = binary_mask.shape[1]\n",
    "    # mask = mask / 255\n",
    "\n",
    "    pad0 = (CFG.patch_size - binary_mask.shape[0] % CFG.patch_size)\n",
    "    pad1 = (CFG.patch_size - binary_mask.shape[1] % CFG.patch_size)\n",
    "\n",
    "    binary_mask = np.pad(binary_mask, [(0, pad0), (0, pad1)], constant_values=0)\n",
    "    \n",
    "    mask_pred = torch.zeros(binary_mask.shape)#.to(CFG.device)\n",
    "    mask_count = torch.zeros(binary_mask.shape)#.to(CFG.device)\n",
    "\n",
    "    for step, (images) in tqdm(enumerate(test_loader), total=len(test_loader)):\n",
    "        images = images.to(DEVICE)\n",
    "        batch_size = images.size(0)\n",
    "        #images = images.unsqueeze(1)\n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            #y_preds = lit_model(images)\n",
    "            #y_preds = torch.sigmoid(y_preds)\n",
    "            #y_preds = y_preds.sigmoid().squeeze()\n",
    "            #y_preds = y_preds.numpy()\n",
    "            #print(type(y_preds))\n",
    "            y_preds = TTA(images,lit_model)\n",
    "\n",
    "        y_preds = y_preds.to('cpu')\n",
    "        start_idx = step*CFG.batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        #xyxys = torch.from_numpy(xyxys)\n",
    "        for i, (x1, y1, x2, y2) in enumerate(xyxys[start_idx:end_idx]):\n",
    "            mask_pred[y1:y2, x1:x2] += y_preds[i].squeeze(0)\n",
    "            mask_count[y1:y2, x1:x2] += torch.ones((CFG.patch_size, CFG.patch_size))#.to(DEVICE)\n",
    "    \n",
    "    #plt.imshow(mask_count.to('cpu'))\n",
    "   # plt.show()\n",
    "    \n",
    "    print(f'mask_count_min: {mask_count.min()}')\n",
    "    #mask_pred /= mask_count\n",
    "    mask_pred /=(mask_count +1e-5)\n",
    "    \n",
    "    #fig, axes = plt.subplots(1, 4, figsize=(15, 8))\n",
    "   \n",
    "    \n",
    "    \n",
    "    mask_pred = mask_pred.to('cpu').numpy()\n",
    "    \n",
    "    # Denoising\n",
    "    \n",
    "    #mask_pred=xp.array(mask_pred)\n",
    "    #mask_pred=denoise_image(mask_pred, iter_num=250)\n",
    "    #mask_pred=mask_pred.get()\n",
    "    \n",
    "     #axes[2].imshow(mask_pred)\n",
    "    \n",
    "    \n",
    "    mask_pred = mask_pred[:ori_h, :ori_w]\n",
    "    binary_mask = binary_mask[:ori_h, :ori_w]\n",
    "    \n",
    "    #mask_pred = mask_pred.to('cpu').numpy()\n",
    "    saved_result_val.append(mask_pred)\n",
    "    mask_pred = (mask_pred >= THRESHOLD).astype(int)\n",
    "    mask_pred *= binary_mask\n",
    "    \n",
    "    #axes[3].imshow(mask_pred)\n",
    "    plt.imshow(mask_pred)\n",
    "    plt.show()\n",
    "    \n",
    "    inklabels_rle = rle(mask_pred)\n",
    "    \n",
    "    \n",
    "    results.append( inklabels_rle)\n",
    "    \n",
    "\n",
    "    del mask_pred, mask_count\n",
    "    del test_loader\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    \n",
    "def metric_to_text(ink, label):\n",
    "    text = []\n",
    "\n",
    "    p = ink.reshape(-1)\n",
    "    t = label.reshape(-1)\n",
    "    pos = np.log(np.clip(p,1e-7,1))\n",
    "    neg = np.log(np.clip(1-p,1e-7,1))\n",
    "    bce = -(t*pos +(1-t)*neg).mean()\n",
    "    text.append(f'bce={bce:0.5f}')\n",
    "\n",
    "\n",
    "    #print(f'{threshold:0.1f}, {precision:0.3f}, {recall:0.3f}, {fpr:0.3f},  {dice:0.3f},  {score:0.3f}')\n",
    "    text.append('th   prec   recall   fpr   dice   score')\n",
    "    text.append('---------------------------------------')\n",
    "    for threshold in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "        p = ink.reshape(-1)\n",
    "        t = label.reshape(-1)\n",
    "        p = (p > threshold).astype(np.float32)\n",
    "        t = (t > 0.5).astype(np.float32)\n",
    "\n",
    "        tp = p * t\n",
    "        precision = tp.sum() / (p.sum() + 0.0001)\n",
    "        recall = tp.sum() / t.sum()\n",
    "\n",
    "        fp = p * (1 - t)\n",
    "        fpr = fp.sum() / (1 - t).sum()\n",
    "\n",
    "        beta = 0.5\n",
    "        #  0.2*1/recall + 0.8*1/prec\n",
    "        score = beta * beta / (1 + beta * beta) * 1 / recall + 1 / (1 + beta * beta) * 1 / precision\n",
    "        score = 1 / score\n",
    "\n",
    "        dice = 2 * tp.sum() / (p.sum() + t.sum())\n",
    "\n",
    "        # print(fold, threshold, precision, recall, fpr,  score)\n",
    "        text.append( f'{threshold:0.1f}, {precision:0.3f}, {recall:0.3f}, {fpr:0.3f},  {dice:0.3f},  {score:0.3f}')\n",
    "    text = '\\n'.join(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "label =  cv2.imread(COMPETITION_DATA_DIR_str + f\"train/{fragment_id}/inklabels.png\", 0)\n",
    "text = metric_to_text(saved_result_val[0], label)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aed2bc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TTA(x:torch.Tensor,model:nn.Module):\n",
    "    #x.shape=(batch,c,h,w)\n",
    "    if 1:\n",
    "        shape=x.shape\n",
    "        x=[x,*[torch.rot90(x,k=i,dims=(-2,-1)) for i in range(1,4)]]\n",
    "        x=torch.cat(x,dim=0)\n",
    "        x=model(x)\n",
    "        x=torch.sigmoid(x)\n",
    "        x=x.reshape(4,shape[0],*shape[2:])\n",
    "        x=[torch.rot90(x[i],k=-i,dims=(-2,-1)) for i in range(4)]\n",
    "        x=torch.stack(x,dim=0)\n",
    "        return x.mean(0)\n",
    "    else :\n",
    "        x=model(x)\n",
    "        x=torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "def make_test_dataset(fragment_id):\n",
    "    test_images = read_image(fragment_id)\n",
    "    \n",
    "    x1_list = list(range(0, test_images.shape[1]-CFG.patch_size+1, CFG.stride))\n",
    "    y1_list = list(range(0, test_images.shape[0]-CFG.patch_size+1, CFG.stride))\n",
    "    \n",
    "    test_images_list = []\n",
    "    xyxys = []\n",
    "    for y1 in y1_list:\n",
    "        for x1 in x1_list:\n",
    "            y2 = y1 + CFG.patch_size\n",
    "            x2 = x1 + CFG.patch_size\n",
    "            \n",
    "            test_images_list.append(test_images[y1:y2, x1:x2])\n",
    "            xyxys.append((x1, y1, x2, y2))\n",
    "    xyxys = np.stack(xyxys)\n",
    "            \n",
    "    test_dataset = CustomDataset(test_images_list, CFG, transform=get_transforms(data='valid', cfg=CFG))\n",
    "    \n",
    "    test_loader = DataLoader(test_dataset,\n",
    "                          batch_size=CFG.batch_size,\n",
    "                          shuffle=False,\n",
    "                          num_workers=CFG.num_workers, pin_memory=CFG.on_gpu, drop_last=False)\n",
    "    \n",
    "    return test_loader, xyxys\n",
    "\n",
    "\n",
    "def get_transforms(data, cfg):\n",
    "    return A.Compose(\n",
    "        [\n",
    "        A.Resize(CFG.patch_size, CFG.patch_size),\n",
    "        A.Normalize(\n",
    "            mean=[0] * CFG.z_dim,\n",
    "            std=[1] * CFG.z_dim\n",
    "        ),\n",
    "\n",
    "        ToTensorV2(transpose_mask=True),\n",
    "    ]\n",
    "    )\n",
    "\n",
    "def read_image(fragment_id):\n",
    "    images = []\n",
    "\n",
    "    # idxs = range(65)\n",
    "    mid = 65 // 2\n",
    "    start = mid - CFG.z_dim // 2\n",
    "    end = mid + CFG.z_dim // 2\n",
    "    idxs = range(start, end)\n",
    "\n",
    "    for i in tqdm(idxs):\n",
    "        \n",
    "        image = cv2.imread(COMPETITION_DATA_DIR_str + f\"test/{fragment_id}/surface_volume/{i:02}.tif\", 0)\n",
    "\n",
    "        pad0 = (CFG.patch_size - image.shape[0] % CFG.patch_size)\n",
    "        pad1 = (CFG.patch_size - image.shape[1] % CFG.patch_size)\n",
    "\n",
    "        image = np.pad(image, [(0, pad0), (0, pad1)], constant_values=0)\n",
    "\n",
    "        images.append(image)\n",
    "    images = np.stack(images, axis=2)\n",
    "    \n",
    "    return images\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, cfg, labels=None, transform=None):\n",
    "        self.images = images\n",
    "        self.cfg = cfg\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        # return len(self.xyxys)\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # x1, y1, x2, y2 = self.xyxys[idx]\n",
    "        image = self.images[idx]\n",
    "        data = self.transform(image=image)\n",
    "        image = data['image']\n",
    "        return image\n",
    "    \n",
    "    \n",
    "    \n",
    "def rle(img):\n",
    "    '''\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels = img.flatten()\n",
    "    # pixels = (pixels >= thr).astype(int)\n",
    "    \n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2492c93e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3e85a58c18f4a45a4fdb1b270dbbc4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "592fb31851474030b2eb6e90b177b71f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/258 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24701/569122699.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mmask_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinary_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_24701/3018819036.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;31m# x1, y1, x2, y2 = self.xyxys[idx]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/albumentations/core/composition.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, force_apply, *args, **data)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcheck_each_transform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/albumentations/core/transforms_interface.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, force_apply, *args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m                     )\n\u001b[1;32m    117\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_with_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/albumentations/core/transforms_interface.py\u001b[0m in \u001b[0;36mapply_with_params\u001b[0;34m(self, params, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0mtarget_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_target_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0mtarget_dependencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_dependence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m                 \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtarget_dependencies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/albumentations/pytorch/transforms.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, img, **params)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_to_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# skipcq: PYL-W0613\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "THRESHOLD =0.3\n",
    "results = []\n",
    "saved_results = []\n",
    "for fragment_id in CFG.test_fragment_ids:\n",
    "    \n",
    "    test_loader, xyxys = make_test_dataset(fragment_id)\n",
    "    \n",
    "    binary_mask = cv2.imread(COMPETITION_DATA_DIR_str + f\"test/{fragment_id}/mask.png\", 0)\n",
    "    binary_mask = (binary_mask / 255).astype(int)\n",
    "    binary_mask = torch.tensor(binary_mask)#.to(DEVICE)\n",
    "    \n",
    "    ori_h = binary_mask.shape[0]\n",
    "    ori_w = binary_mask.shape[1]\n",
    "    # mask = mask / 255\n",
    "\n",
    "    pad0 = (CFG.patch_size - binary_mask.shape[0] % CFG.patch_size)\n",
    "    pad1 = (CFG.patch_size - binary_mask.shape[1] % CFG.patch_size)\n",
    "\n",
    "    binary_mask = np.pad(binary_mask, [(0, pad0), (0, pad1)], constant_values=0)\n",
    "    \n",
    "    mask_pred = torch.zeros(binary_mask.shape).to(CFG.device)\n",
    "    mask_count = torch.zeros(binary_mask.shape).to(CFG.device)\n",
    "\n",
    "    for step, (images) in tqdm(enumerate(test_loader), total=len(test_loader)):\n",
    "        images = images.to(DEVICE)\n",
    "        batch_size = images.size(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            #y_preds = lit_model(images)\n",
    "            #y_preds = torch.sigmoid(y_preds)\n",
    "            #y_preds = y_preds.numpy()\n",
    "            #print(type(y_preds))\n",
    "            y_preds = TTA(images,lit_model)\n",
    "\n",
    "        start_idx = step*CFG.batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        #xyxys = torch.from_numpy(xyxys)\n",
    "        for i, (x1, y1, x2, y2) in enumerate(xyxys[start_idx:end_idx]):\n",
    "            mask_pred[y1:y2, x1:x2] += y_preds[i].squeeze(0)\n",
    "            mask_count[y1:y2, x1:x2] += torch.ones((CFG.patch_size, CFG.patch_size)).to(DEVICE)\n",
    "    \n",
    "    plt.imshow(mask_count.to('cpu'))\n",
    "    plt.show()\n",
    "    \n",
    "    print(f'mask_count_min: {mask_count.min()}')\n",
    "    mask_pred /= mask_count\n",
    "    \n",
    "    mask_pred = mask_pred[:ori_h, :ori_w]\n",
    "    binary_mask = binary_mask[:ori_h, :ori_w]\n",
    "    \n",
    "    mask_pred = mask_pred.to('cpu').numpy()\n",
    "    saved_results.append(mask_pred)\n",
    "    mask_pred = (mask_pred >= THRESHOLD).astype(int)\n",
    "    mask_pred *= binary_mask\n",
    "    \n",
    "    plt.imshow(mask_pred)\n",
    "    plt.show()\n",
    "    \n",
    "    inklabels_rle = rle(mask_pred)\n",
    "    \n",
    "    \n",
    "    results.append( inklabels_rle)\n",
    "    \n",
    "\n",
    "    del mask_pred, mask_count\n",
    "    del test_loader\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6532d1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.5\n",
    "for image in saved_results:\n",
    "    mask_pred = image\n",
    "    mask_pred = (mask_pred >= THRESHOLD).astype(int)\n",
    "    plt.imshow(mask_pred)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9de99e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
