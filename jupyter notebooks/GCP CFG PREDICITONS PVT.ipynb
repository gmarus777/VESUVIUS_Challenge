{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98e43a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting path:/home/gregory_maruss/VESUVIUS_Challenge/jupyter notebooks\n",
      "Current path:/home/gregory_maruss/VESUVIUS_Challenge\n"
     ]
    }
   ],
   "source": [
    "# Here we take care of paths.\n",
    "# Make sure root project directory is named 'VESUVIUS_Challenge' for this to work\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "print('Starting path:' + os.getcwd())\n",
    "if os.getcwd()[-18:] == 'VESUVIUS_Challenge':\n",
    "    pass\n",
    "else:\n",
    "    PATH = Path().resolve().parents[0]\n",
    "    os.chdir(PATH)\n",
    "\n",
    "# make sure you are in the root folder of the project\n",
    "print('Current path:' + os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73cc2ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-18 01:29:17,596 - Created a temporary directory at /tmp/tmps8hrgaql\n",
      "2023-05-18 01:29:17,598 - Writing /tmp/tmps8hrgaql/_remote_module_non_scriptable.py\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import monai\n",
    "from monai.visualize import matshow3d\n",
    "import segmentation_models_pytorch as smp\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Tuple, List\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from Data_Modules.Vesuvius_Dataset import Vesuvius_Tile_Datamodule\n",
    "from lit_models.Vesuvius_Lit_Model import Lit_Model\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import torch.nn as nn\n",
    "from Models.PVT2 import PyramidVisionTransformerV2, Up, OutConv\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from Models.Swin import SwinTransformer, SwinTransformerBlockV2, PatchMergingV2\n",
    "import gc\n",
    "from torch.utils.data import DataLoader, Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2258478b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT = 'logs/PVT_Unet_bce50_tver50_gamma1/last.ckpt'\n",
    "SAMPLE_SUBMISSION = 'kaggle/input/vesuvius-challenge-ink-detection/sample_submission.csv'\n",
    "\n",
    "\n",
    "PATCH_SIZE = 256\n",
    "Z_DIM = 16\n",
    "COMPETITION_DATA_DIR_str =  \"kaggle/input/vesuvius-challenge-ink-detection/\"\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# change to the line below if not using Apple's M1 or chips\n",
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f08d5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PVT_w_UNet(nn.Module):\n",
    "    def __init__(self, in_channels,  embed_dims=[ 64, 128, 256, 512], n_classes=1, ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed_dims = embed_dims\n",
    "        \n",
    "        self.pvt = PyramidVisionTransformerV2(img_size = PATCH_SIZE,\n",
    "                                  patch_size = 4,\n",
    "                                  in_chans = Z_DIM,\n",
    "                                  num_classes = 1,\n",
    "                                  embed_dims = embed_dims,\n",
    "                                num_heads=[1, 2, 4, 8],\n",
    "                                  mlp_ratios=[8, 8, 4, 4],\n",
    "                                  qkv_bias=True,\n",
    "                                  qk_scale=None,\n",
    "                                  drop_rate=0.,\n",
    "                                attn_drop_rate=0.,\n",
    "                                  drop_path_rate=0.1,\n",
    "                                  norm_layer=partial(nn.LayerNorm, eps=1e-3),\n",
    "                                #norm_layer=nn.LayerNorm,          \n",
    "                                  depths=[3, 4, 6, 3],\n",
    "                                  sr_ratios=[8, 4, 2, 1]\n",
    "                                 ).to(DEVICE) \n",
    "        \n",
    "        self.up1 = Up(self.embed_dims[-1], self.embed_dims[-2])\n",
    "        self.up2 = Up(self.embed_dims[-2], self.embed_dims[-3])\n",
    "        self.up3 = Up(self.embed_dims[-3], self.embed_dims[-4])\n",
    "        self.up4 = Up(self.embed_dims[-4], in_channels, last_layer = True)\n",
    "        \n",
    "        self.out_conv = OutConv(in_channels,n_classes)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x1, x2, x3, x4, x5 = self.pvt(x)\n",
    "        \n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        \n",
    "        logits = self.out_conv(x)\n",
    "        \n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a82c1b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CFG:\n",
    "    \n",
    "    device = DEVICE\n",
    "    \n",
    "    THRESHOLD = 0.5\n",
    "    use_wandb = False\n",
    "    \n",
    "    ######### Dataset #########\n",
    "    \n",
    "    # stage: 'train' or 'test'\n",
    "    stage = 'test' \n",
    "    \n",
    "    # location of competition Data\n",
    "    competition_data_dir = COMPETITION_DATA_DIR_str\n",
    "    \n",
    "    # Number of slices in z-dim: 1<z_dim<65\n",
    "    z_dim = Z_DIM\n",
    "    \n",
    "    # fragments to use for training avalaible [1,2,3]\n",
    "    train_fragment_id=[2,3]\n",
    "    \n",
    "    # fragments to use for validation\n",
    "    val_fragment_id=[1]\n",
    "    \n",
    "    test_fragment_ids = ['a','b']\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    batch_size = 8\n",
    "    \n",
    "    # Size of the patch and stride for feeding the model\n",
    "    patch_size = PATCH_SIZE\n",
    "    stride = patch_size // 2\n",
    "    \n",
    "    \n",
    "    num_workers = 0\n",
    "    on_gpu = True\n",
    "    \n",
    "    \n",
    "    ######## Model and Lightning Model paramters ############\n",
    "    \n",
    "    # MODEL\n",
    "    model = PVT_w_UNet(in_channels = z_dim).to(DEVICE) \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    checkpoint = None\n",
    "    save_directory = None\n",
    "    \n",
    "    \n",
    "    accumulate_grad_batches = 192// batch_size  # experiments showed batch_size * accumulate_grad = 192 is optimal\n",
    "    learning_rate = 0.0001\n",
    "    eta_min = 1e-7\n",
    "    t_max = 80\n",
    "    max_epochs = 120\n",
    "    weight_decay =  0.0001\n",
    "    precision =16\n",
    "    \n",
    "    # checkpointing\n",
    "    save_top_k=5\n",
    "    \n",
    "    monitor=\"FBETA\"\n",
    "    mode=\"max\"\n",
    "    \n",
    "    \n",
    "    ####### Augemtnations ###############\n",
    "    \n",
    "    # Training Aug\n",
    "    train_transforms = [\n",
    "        # A.RandomResizedCrop(\n",
    "        #     size, size, scale=(0.85, 1.0)),\n",
    "        A.Resize(patch_size, patch_size),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.75),\n",
    "        A.ShiftScaleRotate(p=0.75),\n",
    "        A.OneOf([\n",
    "                A.GaussNoise(var_limit=[10, 50]),\n",
    "                A.GaussianBlur(),\n",
    "                A.MotionBlur(),\n",
    "                ], p=0.4),\n",
    "        A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.5),\n",
    "        A.CoarseDropout(max_holes=1, max_width=int(patch_size * 0.3), max_height=int(patch_size * 0.3), \n",
    "                        mask_fill_value=0, p=0.5),\n",
    "        # A.Cutout(max_h_size=int(size * 0.6),\n",
    "        #          max_w_size=int(size * 0.6), num_holes=1, p=1.0),\n",
    "        A.Normalize(\n",
    "            mean= [0] * z_dim,\n",
    "            std= [1] * z_dim\n",
    "        ),\n",
    "        ToTensorV2(transpose_mask=True),\n",
    "    ]\n",
    "    \n",
    "\n",
    "    \n",
    "    # Validaiton Aug\n",
    "    val_transforms = [\n",
    "        A.Resize(patch_size, patch_size),\n",
    "        A.Normalize(\n",
    "            mean= [0] * z_dim,\n",
    "            std= [1] * z_dim\n",
    "        ),\n",
    "        ToTensorV2(transpose_mask=True),\n",
    "    ]\n",
    "    \n",
    "    # Test Aug\n",
    "    test_transforms = [\n",
    "        A.Resize(patch_size, patch_size),\n",
    "        A.Normalize(\n",
    "            mean=[0] * z_dim,\n",
    "            std=[1] * z_dim\n",
    "        ),\n",
    "\n",
    "        ToTensorV2(transpose_mask=True),\n",
    "    ]\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5ec1424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lit_Model(\n",
       "  (metrics): ModuleDict(\n",
       "    (train_metrics): MetricCollection(\n",
       "      (dice): Dice(),\n",
       "      prefix=train_\n",
       "    )\n",
       "    (val_metrics): MetricCollection(\n",
       "      (dice): Dice(),\n",
       "      prefix=val_\n",
       "    )\n",
       "  )\n",
       "  (model): PVT_w_UNet(\n",
       "    (pvt): PyramidVisionTransformerV2(\n",
       "      (patch_embed1): OverlapPatchEmbed(\n",
       "        (proj): Conv2d(16, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n",
       "        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (block1): ModuleList(\n",
       "        (0): Block(\n",
       "          (norm1): LayerNorm((64,), eps=0.001, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (kv): Linear(in_features=64, out_features=128, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))\n",
       "            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((64,), eps=0.001, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=64, out_features=512, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=512, out_features=64, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Block(\n",
       "          (norm1): LayerNorm((64,), eps=0.001, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (kv): Linear(in_features=64, out_features=128, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))\n",
       "            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.007)\n",
       "          (norm2): LayerNorm((64,), eps=0.001, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=64, out_features=512, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=512, out_features=64, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): Block(\n",
       "          (norm1): LayerNorm((64,), eps=0.001, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (kv): Linear(in_features=64, out_features=128, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))\n",
       "            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.013)\n",
       "          (norm2): LayerNorm((64,), eps=0.001, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=64, out_features=512, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=512, out_features=64, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((64,), eps=0.001, elementwise_affine=True)\n",
       "      (patch_embed2): OverlapPatchEmbed(\n",
       "        (proj): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (block2): ModuleList(\n",
       "        (0): Block(\n",
       "          (norm1): LayerNorm((128,), eps=0.001, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (kv): Linear(in_features=128, out_features=256, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.020)\n",
       "          (norm2): LayerNorm((128,), eps=0.001, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=128, out_features=1024, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Block(\n",
       "          (norm1): LayerNorm((128,), eps=0.001, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (kv): Linear(in_features=128, out_features=256, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.027)\n",
       "          (norm2): LayerNorm((128,), eps=0.001, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=128, out_features=1024, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): Block(\n",
       "          (norm1): LayerNorm((128,), eps=0.001, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (kv): Linear(in_features=128, out_features=256, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.033)\n",
       "          (norm2): LayerNorm((128,), eps=0.001, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=128, out_features=1024, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): Block(\n",
       "          (norm1): LayerNorm((128,), eps=0.001, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (kv): Linear(in_features=128, out_features=256, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.040)\n",
       "          (norm2): LayerNorm((128,), eps=0.001, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=128, out_features=1024, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm2): LayerNorm((128,), eps=0.001, elementwise_affine=True)\n",
       "      (patch_embed3): OverlapPatchEmbed(\n",
       "        (proj): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (block3): ModuleList(\n",
       "        (0): Block(\n",
       "          (norm1): LayerNorm((256,), eps=0.001, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (kv): Linear(in_features=256, out_features=512, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (sr): Conv2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.047)\n",
       "          (norm2): LayerNorm((256,), eps=0.001, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Block(\n",
       "          (norm1): LayerNorm((256,), eps=0.001, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (kv): Linear(in_features=256, out_features=512, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (sr): Conv2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.053)\n",
       "          (norm2): LayerNorm((256,), eps=0.001, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): Block(\n",
       "          (norm1): LayerNorm((256,), eps=0.001, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (kv): Linear(in_features=256, out_features=512, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (sr): Conv2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.060)\n",
       "          (norm2): LayerNorm((256,), eps=0.001, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): Block(\n",
       "          (norm1): LayerNorm((256,), eps=0.001, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (kv): Linear(in_features=256, out_features=512, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (sr): Conv2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.067)\n",
       "          (norm2): LayerNorm((256,), eps=0.001, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): Block(\n",
       "          (norm1): LayerNorm((256,), eps=0.001, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (kv): Linear(in_features=256, out_features=512, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (sr): Conv2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.073)\n",
       "          (norm2): LayerNorm((256,), eps=0.001, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): Block(\n",
       "          (norm1): LayerNorm((256,), eps=0.001, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (kv): Linear(in_features=256, out_features=512, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (sr): Conv2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.080)\n",
       "          (norm2): LayerNorm((256,), eps=0.001, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm3): LayerNorm((256,), eps=0.001, elementwise_affine=True)\n",
       "      (patch_embed4): OverlapPatchEmbed(\n",
       "        (proj): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (block4): ModuleList(\n",
       "        (0): Block(\n",
       "          (norm1): LayerNorm((512,), eps=0.001, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (kv): Linear(in_features=512, out_features=1024, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.087)\n",
       "          (norm2): LayerNorm((512,), eps=0.001, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Block(\n",
       "          (norm1): LayerNorm((512,), eps=0.001, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (kv): Linear(in_features=512, out_features=1024, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.093)\n",
       "          (norm2): LayerNorm((512,), eps=0.001, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): Block(\n",
       "          (norm1): LayerNorm((512,), eps=0.001, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (kv): Linear(in_features=512, out_features=1024, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.100)\n",
       "          (norm2): LayerNorm((512,), eps=0.001, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm4): LayerNorm((512,), eps=0.001, elementwise_affine=True)\n",
       "    )\n",
       "    (up1): Up(\n",
       "      (up): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (conv): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (up2): Up(\n",
       "      (up): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (conv): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (up3): Up(\n",
       "      (up): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (conv): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (up4): Up(\n",
       "      (up): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (conv): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(48, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (out_conv): OutConv(\n",
       "      (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (loss_tversky): TverskyLoss()\n",
       "  (loss_focal): FocalLoss()\n",
       "  (loss_bce): SoftBCEWithLogitsLoss()\n",
       "  (loss_monai_focal_dice): DiceFocalLoss(\n",
       "    (dice): DiceLoss()\n",
       "    (focal): FocalLoss()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lit_model = Lit_Model(cfg=CFG)\n",
    "\n",
    "lit_model = lit_model.load_from_checkpoint(CHECKPOINT, cfg=CFG,).to(CFG.device)\n",
    "\n",
    "lit_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aed2bc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_test_dataset(fragment_id):\n",
    "    test_images = read_image(fragment_id)\n",
    "    \n",
    "    x1_list = list(range(0, test_images.shape[1]-CFG.patch_size+1, CFG.stride))\n",
    "    y1_list = list(range(0, test_images.shape[0]-CFG.patch_size+1, CFG.stride))\n",
    "    \n",
    "    test_images_list = []\n",
    "    xyxys = []\n",
    "    for y1 in y1_list:\n",
    "        for x1 in x1_list:\n",
    "            y2 = y1 + CFG.patch_size\n",
    "            x2 = x1 + CFG.patch_size\n",
    "            \n",
    "            test_images_list.append(test_images[y1:y2, x1:x2])\n",
    "            xyxys.append((x1, y1, x2, y2))\n",
    "    xyxys = np.stack(xyxys)\n",
    "            \n",
    "    test_dataset = CustomDataset(test_images_list, CFG, transform=get_transforms(data='valid', cfg=CFG))\n",
    "    \n",
    "    test_loader = DataLoader(test_dataset,\n",
    "                          batch_size=CFG.batch_size,\n",
    "                          shuffle=False,\n",
    "                          num_workers=CFG.num_workers, pin_memory=CFG.on_gpu, drop_last=False)\n",
    "    \n",
    "    return test_loader, xyxys\n",
    "\n",
    "\n",
    "def get_transforms(data, cfg):\n",
    "    return A.Compose(\n",
    "        [\n",
    "        A.Resize(CFG.patch_size, CFG.patch_size),\n",
    "        A.Normalize(\n",
    "            mean=[0] * CFG.z_dim,\n",
    "            std=[1] * CFG.z_dim\n",
    "        ),\n",
    "\n",
    "        ToTensorV2(transpose_mask=True),\n",
    "    ]\n",
    "    )\n",
    "\n",
    "def read_image(fragment_id):\n",
    "    images = []\n",
    "\n",
    "    # idxs = range(65)\n",
    "    mid = 65 // 2\n",
    "    start = mid - CFG.z_dim // 2\n",
    "    end = mid + CFG.z_dim // 2\n",
    "    idxs = range(start, end)\n",
    "\n",
    "    for i in tqdm(idxs):\n",
    "        \n",
    "        image = cv2.imread(COMPETITION_DATA_DIR_str + f\"test/{fragment_id}/surface_volume/{i:02}.tif\", 0)\n",
    "\n",
    "        pad0 = (CFG.patch_size - image.shape[0] % CFG.patch_size)\n",
    "        pad1 = (CFG.patch_size - image.shape[1] % CFG.patch_size)\n",
    "\n",
    "        image = np.pad(image, [(0, pad0), (0, pad1)], constant_values=0)\n",
    "\n",
    "        images.append(image)\n",
    "    images = np.stack(images, axis=2)\n",
    "    \n",
    "    return images\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, cfg, labels=None, transform=None):\n",
    "        self.images = images\n",
    "        self.cfg = cfg\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        # return len(self.xyxys)\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # x1, y1, x2, y2 = self.xyxys[idx]\n",
    "        image = self.images[idx]\n",
    "        data = self.transform(image=image)\n",
    "        image = data['image']\n",
    "        return image\n",
    "    \n",
    "    \n",
    "    \n",
    "def rle(img):\n",
    "    '''\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels = img.flatten()\n",
    "    # pixels = (pixels >= thr).astype(int)\n",
    "    \n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af4ea60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf9e5c4455ca40f2a00d104eef972bfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "055a28ca5825404d91b0ccf155318559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/129 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask_count_min: 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5636a12a7a334c4b8fe31b5e4f1e5cf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88e1dd8ab9f84ce9a8fc257e1598abbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/264 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "THRESHOLD =0.5\n",
    "results = []\n",
    "for fragment_id in CFG.test_fragment_ids:\n",
    "    \n",
    "    test_loader, xyxys = make_test_dataset(fragment_id)\n",
    "    \n",
    "    binary_mask = cv2.imread(COMPETITION_DATA_DIR_str + f\"test/{fragment_id}/mask.png\", 0)\n",
    "    binary_mask = (binary_mask / 255).astype(int)\n",
    "    binary_mask = torch.tensor(binary_mask)#.to(DEVICE)\n",
    "    \n",
    "    ori_h = binary_mask.shape[0]\n",
    "    ori_w = binary_mask.shape[1]\n",
    "    # mask = mask / 255\n",
    "\n",
    "    pad0 = (CFG.patch_size - binary_mask.shape[0] % CFG.patch_size)\n",
    "    pad1 = (CFG.patch_size - binary_mask.shape[1] % CFG.patch_size)\n",
    "\n",
    "    binary_mask = np.pad(binary_mask, [(0, pad0), (0, pad1)], constant_values=0)\n",
    "    \n",
    "    mask_pred = torch.zeros(binary_mask.shape).to(CFG.device)\n",
    "    mask_count = torch.zeros(binary_mask.shape).to(CFG.device)\n",
    "\n",
    "    for step, (images) in tqdm(enumerate(test_loader), total=len(test_loader)):\n",
    "        images = images.to(DEVICE)\n",
    "        batch_size = images.size(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            y_preds = lit_model(images)\n",
    "            y_preds = torch.sigmoid(y_preds)\n",
    "            #y_preds = y_preds.numpy()\n",
    "            #print(type(y_preds))\n",
    "\n",
    "        start_idx = step*CFG.batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        #xyxys = torch.from_numpy(xyxys)\n",
    "        for i, (x1, y1, x2, y2) in enumerate(xyxys[start_idx:end_idx]):\n",
    "            mask_pred[y1:y2, x1:x2] += y_preds[i].squeeze(0)\n",
    "            mask_count[y1:y2, x1:x2] += torch.ones((CFG.patch_size, CFG.patch_size)).to(DEVICE)\n",
    "    \n",
    "    plt.imshow(mask_count.to('cpu'))\n",
    "    plt.show()\n",
    "    \n",
    "    print(f'mask_count_min: {mask_count.min()}')\n",
    "    mask_pred /= mask_count\n",
    "    \n",
    "    mask_pred = mask_pred[:ori_h, :ori_w]\n",
    "    binary_mask = binary_mask[:ori_h, :ori_w]\n",
    "    \n",
    "    mask_pred = mask_pred.to('cpu').numpy()\n",
    "    mask_pred = (mask_pred >= THRESHOLD).astype(int)\n",
    "    mask_pred *= binary_mask\n",
    "    \n",
    "    plt.imshow(mask_pred)\n",
    "    plt.show()\n",
    "    \n",
    "    inklabels_rle = rle(mask_pred)\n",
    "    \n",
    "    \n",
    "    results.append( inklabels_rle)\n",
    "    \n",
    "\n",
    "    del mask_pred, mask_count\n",
    "    del test_loader\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6532d1f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9de99e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
